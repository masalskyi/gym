{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "from gym.envs.classic_control.mountain_car import MountainCarEnv\n",
    "\n",
    "class MountainCarModifiedReward(MountainCarEnv):\n",
    "    def step(self, action: int):\n",
    "        previous_state = self.state\n",
    "        new_state, reward, done, info = super().step(action)\n",
    "        modified_reward = reward + 300 * (0.95 * abs(new_state[1]) - abs(previous_state[1]))\n",
    "        if new_state[0] >= 0.5:\n",
    "            modified_reward += 100\n",
    "        return new_state, modified_reward, done, info\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "env =MountainCarModifiedReward()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "Discrete(3)"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    inputs = layers.Input(shape=(1, states))\n",
    "    x = layers.Dense(64, activation=\"relu\") (inputs)\n",
    "    x = layers.Dense(64, activation=\"relu\") (x)\n",
    "    x = layers.Flatten()(x)\n",
    "    outputs = layers.Dense(actions, activation=\"linear\")(x)\n",
    "    return Model(inputs, outputs, name=\"mountain_car_player\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "model = build_model(2,3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mountain_car_player\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 1, 2)]            0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 1, 64)             192       \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 1, 64)             4160      \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,547\n",
      "Trainable params: 4,547\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, policy=policy, memory=memory, nb_actions=actions, nb_steps_warmup=100, target_model_update=1e-2)\n",
    "    return dqn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "dqn = build_agent(model, 3)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=[\"mae\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "from callbacks import TrainEpisodeLogger, TrainIntervalLogger, TestLogger\n",
    "callbacks = [TrainEpisodeLogger(\"./model/\")]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leff0\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  998/10000 [=>............................] - ETA: 2:52 - reward: -1.1621  1000/100000: episode: 1, duration: 19.566s, episode steps: 1000, steps per second: 51, episode reward: -1160.563, mean reward: -1.161 [-2.204, -0.057], mean action: 1.009 [0.000, 2.000], loss: 0.071211, mae: 3.075225, mean_q: -4.418998\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 2000/10000 [=====>........................] - ETA: 2:36 - reward: -1.1946  2000/100000: episode: 2, duration: 19.960s, episode steps: 1000, steps per second: 50, episode reward: -1228.691, mean reward: -1.229 [-5.744, -0.032], mean action: 1.076 [0.000, 2.000], loss: 0.102267, mae: 9.075486, mean_q: -13.351703\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 2998/10000 [=======>......................] - ETA: 2:17 - reward: -1.2497  3000/100000: episode: 3, duration: 19.654s, episode steps: 1000, steps per second: 51, episode reward: -1358.686, mean reward: -1.359 [-14.360, -0.027], mean action: 0.993 [0.000, 2.000], loss: 0.219322, mae: 14.857383, mean_q: -21.917919\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4000/10000 [===========>..................] - ETA: 1:57 - reward: -1.2760  4000/100000: episode: 4, duration: 19.869s, episode steps: 1000, steps per second: 50, episode reward: -1355.996, mean reward: -1.356 [-12.224, -0.052], mean action: 1.025 [0.000, 2.000], loss: 0.406028, mae: 20.059750, mean_q: -29.680422\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4998/10000 [=============>................] - ETA: 1:38 - reward: -1.2566  5000/100000: episode: 5, duration: 19.310s, episode steps: 1000, steps per second: 52, episode reward: -1178.642, mean reward: -1.179 [-2.150, -0.081], mean action: 1.025 [0.000, 2.000], loss: 0.780557, mae: 25.150511, mean_q: -37.386616\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6000/10000 [=================>............] - ETA: 1:18 - reward: -1.2280  6000/100000: episode: 6, duration: 19.201s, episode steps: 1000, steps per second: 52, episode reward: -1085.394, mean reward: -1.085 [-1.978, -0.181], mean action: 0.975 [0.000, 2.000], loss: 0.756682, mae: 29.892097, mean_q: -44.532467\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6998/10000 [===================>..........] - ETA: 58s - reward: -1.2084  7000/100000: episode: 7, duration: 19.505s, episode steps: 1000, steps per second: 51, episode reward: -1089.986, mean reward: -1.090 [-1.904, -0.216], mean action: 0.950 [0.000, 2.000], loss: 1.062667, mae: 33.803322, mean_q: -50.350826\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8000/10000 [=======================>......] - ETA: 39s - reward: -1.2002  8000/100000: episode: 8, duration: 19.705s, episode steps: 1000, steps per second: 51, episode reward: -1143.947, mean reward: -1.144 [-1.907, -0.239], mean action: 0.956 [0.000, 2.000], loss: 1.515934, mae: 37.022583, mean_q: -55.150375\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8998/10000 [=========================>....] - ETA: 19s - reward: -1.1882  9000/100000: episode: 9, duration: 19.381s, episode steps: 1000, steps per second: 52, episode reward: -1092.817, mean reward: -1.093 [-1.981, -0.237], mean action: 0.983 [0.000, 2.000], loss: 2.261685, mae: 39.690956, mean_q: -59.111500\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "10000/10000 [==============================] - 196s 20ms/step - reward: -1.1841\n",
      " 10000/100000: episode: 10, duration: 19.596s, episode steps: 1000, steps per second: 51, episode reward: -1146.616, mean reward: -1.147 [-2.337, -0.059], mean action: 0.961 [0.000, 2.000], loss: 1.871768, mae: 42.058624, mean_q: -62.705040\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "10 episodes - episode_reward: -1184.134 [-1358.686, -1085.394] - loss: 0.913 - mae: 25.697 - mean_q: -38.202\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      " 1000/10000 [==>...........................] - ETA: 2:56 - reward: -1.1289 11000/100000: episode: 11, duration: 19.601s, episode steps: 1000, steps per second: 51, episode reward: -1128.865, mean reward: -1.129 [-1.992, -0.167], mean action: 0.993 [0.000, 2.000], loss: 2.252842, mae: 44.473946, mean_q: -66.339737\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 1998/10000 [====>.........................] - ETA: 2:36 - reward: -1.1141 12000/100000: episode: 12, duration: 19.565s, episode steps: 1000, steps per second: 51, episode reward: -1100.359, mean reward: -1.100 [-1.946, -0.264], mean action: 1.011 [0.000, 2.000], loss: 2.239898, mae: 46.719467, mean_q: -69.710800\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 3000/10000 [========>.....................] - ETA: 2:16 - reward: -1.1445 13000/100000: episode: 13, duration: 19.521s, episode steps: 1000, steps per second: 51, episode reward: -1204.410, mean reward: -1.204 [-2.228, -0.057], mean action: 1.003 [0.000, 2.000], loss: 1.972973, mae: 48.667171, mean_q: -72.663826\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4000/10000 [===========>..................] - ETA: 1:56 - reward: -1.1571 14000/100000: episode: 14, duration: 19.223s, episode steps: 1000, steps per second: 52, episode reward: -1194.823, mean reward: -1.195 [-2.211, -0.086], mean action: 1.023 [0.000, 2.000], loss: 3.215329, mae: 50.475433, mean_q: -75.302467\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 5000/10000 [==============>...............] - ETA: 1:37 - reward: -1.1478 15000/100000: episode: 15, duration: 19.636s, episode steps: 1000, steps per second: 51, episode reward: -1110.767, mean reward: -1.111 [-2.030, -0.186], mean action: 1.048 [0.000, 2.000], loss: 2.270942, mae: 52.162079, mean_q: -77.884575\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 5998/10000 [================>.............] - ETA: 1:18 - reward: -1.1428 16000/100000: episode: 16, duration: 19.528s, episode steps: 1000, steps per second: 51, episode reward: -1117.135, mean reward: -1.117 [-1.954, -0.250], mean action: 1.026 [0.000, 2.000], loss: 3.442007, mae: 53.723209, mean_q: -80.160065\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6999/10000 [===================>..........] - ETA: 58s - reward: -1.1364 17000/100000: episode: 17, duration: 19.715s, episode steps: 1000, steps per second: 51, episode reward: -1097.956, mean reward: -1.098 [-1.944, -0.238], mean action: 1.007 [0.000, 2.000], loss: 3.684308, mae: 54.810772, mean_q: -81.782555\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8000/10000 [=======================>......] - ETA: 38s - reward: -1.1361 18000/100000: episode: 18, duration: 18.987s, episode steps: 1000, steps per second: 53, episode reward: -1134.123, mean reward: -1.134 [-1.993, -0.188], mean action: 1.008 [0.000, 2.000], loss: 3.033198, mae: 55.987911, mean_q: -83.597382\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8999/10000 [=========================>....] - ETA: 19s - reward: -1.1332 19000/100000: episode: 19, duration: 19.233s, episode steps: 1000, steps per second: 52, episode reward: -1110.276, mean reward: -1.110 [-1.922, -0.224], mean action: 1.056 [0.000, 2.000], loss: 3.728192, mae: 56.950798, mean_q: -85.004753\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "10000/10000 [==============================] - 194s 19ms/step - reward: -1.1365\n",
      " 20000/100000: episode: 20, duration: 19.078s, episode steps: 1000, steps per second: 52, episode reward: -1166.652, mean reward: -1.167 [-2.232, -0.049], mean action: 1.020 [0.000, 2.000], loss: 4.401708, mae: 57.709232, mean_q: -86.116409\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "10 episodes - episode_reward: -1136.537 [-1204.410, -1097.956] - loss: 3.024 - mae: 52.168 - mean_q: -77.856\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      " 1000/10000 [==>...........................] - ETA: 2:53 - reward: -1.1483 21000/100000: episode: 21, duration: 19.276s, episode steps: 1000, steps per second: 52, episode reward: -1148.342, mean reward: -1.148 [-1.936, -0.190], mean action: 0.900 [0.000, 2.000], loss: 3.320743, mae: 58.279362, mean_q: -87.016418\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 2000/10000 [=====>........................] - ETA: 2:34 - reward: -1.1830 22000/100000: episode: 22, duration: 19.295s, episode steps: 1000, steps per second: 52, episode reward: -1217.691, mean reward: -1.218 [-2.192, -0.065], mean action: 0.980 [0.000, 2.000], loss: 3.573391, mae: 58.891342, mean_q: -87.915398\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 2998/10000 [=======>......................] - ETA: 2:14 - reward: -1.2255 23000/100000: episode: 23, duration: 19.266s, episode steps: 1000, steps per second: 52, episode reward: -1308.776, mean reward: -1.309 [-4.190, -0.033], mean action: 0.977 [0.000, 2.000], loss: 4.168642, mae: 59.512756, mean_q: -88.795372\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4000/10000 [===========>..................] - ETA: 1:55 - reward: -1.2260 24000/100000: episode: 24, duration: 19.451s, episode steps: 1000, steps per second: 51, episode reward: -1229.035, mean reward: -1.229 [-2.436, -0.035], mean action: 1.012 [0.000, 2.000], loss: 5.596130, mae: 60.166718, mean_q: -89.747643\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 5000/10000 [==============>...............] - ETA: 1:37 - reward: -1.2351 25000/100000: episode: 25, duration: 19.869s, episode steps: 1000, steps per second: 50, episode reward: -1271.901, mean reward: -1.272 [-6.317, -0.008], mean action: 0.989 [0.000, 2.000], loss: 3.740799, mae: 60.571205, mean_q: -90.454628\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 5998/10000 [================>.............] - ETA: 1:17 - reward: -1.2406 26000/100000: episode: 26, duration: 19.321s, episode steps: 1000, steps per second: 52, episode reward: -1269.050, mean reward: -1.269 [-2.343, -0.015], mean action: 1.039 [0.000, 2.000], loss: 4.237702, mae: 61.088287, mean_q: -91.199585\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 7000/10000 [====================>.........] - ETA: 58s - reward: -1.2504 27000/100000: episode: 27, duration: 19.564s, episode steps: 1000, steps per second: 51, episode reward: -1308.192, mean reward: -1.308 [-5.762, -0.059], mean action: 1.066 [0.000, 2.000], loss: 4.012483, mae: 61.675076, mean_q: -92.072060\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 7998/10000 [======================>.......] - ETA: 38s - reward: -1.2479 28000/100000: episode: 28, duration: 19.718s, episode steps: 1000, steps per second: 51, episode reward: -1230.774, mean reward: -1.231 [-2.251, -0.063], mean action: 1.032 [0.000, 2.000], loss: 5.640633, mae: 62.091946, mean_q: -92.595901\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8999/10000 [=========================>....] - ETA: 19s - reward: -1.2348 29000/100000: episode: 29, duration: 19.401s, episode steps: 1000, steps per second: 52, episode reward: -1128.435, mean reward: -1.128 [-2.049, -0.165], mean action: 1.034 [0.000, 2.000], loss: 4.769227, mae: 62.358212, mean_q: -93.058128\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "10000/10000 [==============================] - 195s 20ms/step - reward: -1.2248\n",
      " 30000/100000: episode: 30, duration: 19.946s, episode steps: 1000, steps per second: 50, episode reward: -1136.232, mean reward: -1.136 [-2.158, -0.084], mean action: 1.036 [0.000, 2.000], loss: 4.825825, mae: 62.725147, mean_q: -93.657097\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "10 episodes - episode_reward: -1224.843 [-1308.776, -1128.435] - loss: 4.389 - mae: 60.736 - mean_q: -90.651\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      " 1000/10000 [==>...........................] - ETA: 2:53 - reward: -1.2087 31000/100000: episode: 31, duration: 19.355s, episode steps: 1000, steps per second: 52, episode reward: -1208.712, mean reward: -1.209 [-2.450, -0.036], mean action: 1.017 [0.000, 2.000], loss: 3.914703, mae: 63.151867, mean_q: -94.299423\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 1998/10000 [====>.........................] - ETA: 2:37 - reward: -1.2459 32000/100000: episode: 32, duration: 19.918s, episode steps: 1000, steps per second: 50, episode reward: -1283.958, mean reward: -1.284 [-2.410, -0.006], mean action: 1.057 [0.000, 2.000], loss: 4.675259, mae: 63.554359, mean_q: -94.847206\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 2998/10000 [=======>......................] - ETA: 2:17 - reward: -1.2126 33000/100000: episode: 33, duration: 19.640s, episode steps: 1000, steps per second: 51, episode reward: -1143.733, mean reward: -1.144 [-2.200, -0.044], mean action: 1.058 [0.000, 2.000], loss: 4.608966, mae: 63.904461, mean_q: -95.375908\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 3998/10000 [==========>...................] - ETA: 1:57 - reward: -1.2084 34000/100000: episode: 34, duration: 19.510s, episode steps: 1000, steps per second: 51, episode reward: -1196.413, mean reward: -1.196 [-2.352, -0.094], mean action: 0.967 [0.000, 2.000], loss: 5.130610, mae: 64.195343, mean_q: -95.797417\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 5000/10000 [==============>...............] - ETA: 1:38 - reward: -1.2034 35000/100000: episode: 35, duration: 19.885s, episode steps: 1000, steps per second: 50, episode reward: -1184.255, mean reward: -1.184 [-2.052, -0.136], mean action: 0.965 [0.000, 2.000], loss: 5.991759, mae: 64.339172, mean_q: -95.969070\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6000/10000 [=================>............] - ETA: 1:18 - reward: -1.1957 36000/100000: episode: 36, duration: 19.803s, episode steps: 1000, steps per second: 50, episode reward: -1157.108, mean reward: -1.157 [-2.073, -0.093], mean action: 0.981 [0.000, 2.000], loss: 3.179735, mae: 64.750015, mean_q: -96.776329\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 7000/10000 [====================>.........] - ETA: 59s - reward: -1.2101 37000/100000: episode: 37, duration: 19.676s, episode steps: 1000, steps per second: 51, episode reward: -1296.479, mean reward: -1.296 [-2.491, -0.024], mean action: 1.054 [0.000, 2.000], loss: 5.281686, mae: 65.017593, mean_q: -97.064186\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 7998/10000 [======================>.......] - ETA: 39s - reward: -1.2152 38000/100000: episode: 38, duration: 19.439s, episode steps: 1000, steps per second: 51, episode reward: -1252.312, mean reward: -1.252 [-2.381, -0.030], mean action: 1.070 [0.000, 2.000], loss: 4.166096, mae: 65.041321, mean_q: -97.150642\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8999/10000 [=========================>....] - ETA: 19s - reward: -1.2124 39000/100000: episode: 39, duration: 19.843s, episode steps: 1000, steps per second: 50, episode reward: -1189.037, mean reward: -1.189 [-2.326, -0.022], mean action: 0.992 [0.000, 2.000], loss: 3.826454, mae: 65.432861, mean_q: -97.729507\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "10000/10000 [==============================] - 197s 20ms/step - reward: -1.2059\n",
      " 40000/100000: episode: 40, duration: 19.630s, episode steps: 1000, steps per second: 51, episode reward: -1146.965, mean reward: -1.147 [-2.065, -0.178], mean action: 0.987 [0.000, 2.000], loss: 4.402545, mae: 65.810791, mean_q: -98.260376\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "10 episodes - episode_reward: -1205.897 [-1296.479, -1143.733] - loss: 4.518 - mae: 64.520 - mean_q: -96.327\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "  999/10000 [=>............................] - ETA: 2:57 - reward: -1.1364 41000/100000: episode: 41, duration: 19.752s, episode steps: 1000, steps per second: 51, episode reward: -1135.600, mean reward: -1.136 [-2.064, -0.147], mean action: 1.042 [0.000, 2.000], loss: 5.794320, mae: 65.907768, mean_q: -98.308662\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 2000/10000 [=====>........................] - ETA: 2:37 - reward: -1.1447 42000/100000: episode: 42, duration: 19.720s, episode steps: 1000, steps per second: 51, episode reward: -1153.882, mean reward: -1.154 [-2.353, -0.090], mean action: 1.064 [0.000, 2.000], loss: 4.576586, mae: 65.972176, mean_q: -98.512932\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 2999/10000 [=======>......................] - ETA: 2:18 - reward: -1.1747 43000/100000: episode: 43, duration: 19.792s, episode steps: 1000, steps per second: 51, episode reward: -1233.556, mean reward: -1.234 [-2.332, -0.032], mean action: 0.974 [0.000, 2.000], loss: 4.535536, mae: 66.196671, mean_q: -98.830902\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 3998/10000 [==========>...................] - ETA: 1:58 - reward: -1.1720 44000/100000: episode: 44, duration: 19.787s, episode steps: 1000, steps per second: 51, episode reward: -1164.705, mean reward: -1.165 [-2.165, -0.065], mean action: 1.031 [0.000, 2.000], loss: 4.327883, mae: 66.421951, mean_q: -99.219147\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4840/10000 [=============>................] - ETA: 1:41 - reward: -1.1833 44842/100000: episode: 45, duration: 16.630s, episode steps: 842, steps per second: 51, episode reward: -941.518, mean reward: -1.118 [-2.706, 99.021], mean action: 1.034 [0.000, 2.000], loss: 5.888750, mae: 66.651955, mean_q: -99.406769\n",
      "The reward is higher than the best one, saving checkpoint weights\n",
      " 5842/10000 [================>.............] - ETA: 1:22 - reward: -1.1648 45842/100000: episode: 46, duration: 19.987s, episode steps: 1000, steps per second: 50, episode reward: -1175.492, mean reward: -1.175 [-2.446, -0.020], mean action: 1.030 [0.000, 2.000], loss: 5.412277, mae: 66.435608, mean_q: -99.147934\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6841/10000 [===================>..........] - ETA: 1:02 - reward: -1.1637 46842/100000: episode: 47, duration: 19.886s, episode steps: 1000, steps per second: 50, episode reward: -1157.319, mean reward: -1.157 [-2.165, -0.104], mean action: 1.053 [0.000, 2.000], loss: 5.984373, mae: 66.415565, mean_q: -99.121811\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 7840/10000 [======================>.......] - ETA: 42s - reward: -1.1670 47842/100000: episode: 48, duration: 19.937s, episode steps: 1000, steps per second: 50, episode reward: -1189.204, mean reward: -1.189 [-2.181, -0.074], mean action: 1.012 [0.000, 2.000], loss: 5.664579, mae: 66.306557, mean_q: -98.931709\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8841/10000 [=========================>....] - ETA: 23s - reward: -1.1600 48842/100000: episode: 49, duration: 19.930s, episode steps: 1000, steps per second: 50, episode reward: -1104.903, mean reward: -1.105 [-1.889, -0.324], mean action: 1.057 [0.000, 2.000], loss: 5.958875, mae: 66.294815, mean_q: -98.929298\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9841/10000 [============================>.] - ETA: 3s - reward: -1.1616 49842/100000: episode: 50, duration: 19.985s, episode steps: 1000, steps per second: 50, episode reward: -1177.013, mean reward: -1.177 [-2.282, -0.012], mean action: 0.983 [0.000, 2.000], loss: 5.632354, mae: 66.249481, mean_q: -98.919487\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "10000/10000 [==============================] - 199s 20ms/step - reward: -1.1600\n",
      "10 episodes - episode_reward: -1143.319 [-1233.556, -941.518] - loss: 5.378 - mae: 66.277 - mean_q: -98.922\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "  842/10000 [=>............................] - ETA: 3:03 - reward: -1.1773 50842/100000: episode: 51, duration: 20.076s, episode steps: 1000, steps per second: 50, episode reward: -1157.910, mean reward: -1.158 [-2.170, -0.088], mean action: 1.054 [0.000, 2.000], loss: 4.255938, mae: 66.221909, mean_q: -98.904205\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 1842/10000 [====>.........................] - ETA: 2:43 - reward: -1.1704 51842/100000: episode: 52, duration: 20.017s, episode steps: 1000, steps per second: 50, episode reward: -1164.664, mean reward: -1.165 [-2.060, -0.123], mean action: 1.075 [0.000, 2.000], loss: 5.492588, mae: 66.292160, mean_q: -98.925995\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 2841/10000 [=======>......................] - ETA: 2:23 - reward: -1.1479 52842/100000: episode: 53, duration: 19.987s, episode steps: 1000, steps per second: 50, episode reward: -1106.542, mean reward: -1.107 [-1.934, -0.288], mean action: 1.041 [0.000, 2.000], loss: 5.312396, mae: 66.241043, mean_q: -98.841248\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 3841/10000 [==========>...................] - ETA: 2:03 - reward: -1.1621 53842/100000: episode: 54, duration: 20.007s, episode steps: 1000, steps per second: 50, episode reward: -1203.277, mean reward: -1.203 [-2.312, -0.053], mean action: 0.948 [0.000, 2.000], loss: 5.135445, mae: 66.147682, mean_q: -98.794395\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4842/10000 [=============>................] - ETA: 1:43 - reward: -1.1709 54842/100000: episode: 55, duration: 20.004s, episode steps: 1000, steps per second: 50, episode reward: -1203.978, mean reward: -1.204 [-2.104, -0.128], mean action: 1.062 [0.000, 2.000], loss: 5.204044, mae: 66.182472, mean_q: -98.774857\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 5841/10000 [================>.............] - ETA: 1:23 - reward: -1.1778 55842/100000: episode: 56, duration: 20.025s, episode steps: 1000, steps per second: 50, episode reward: -1211.016, mean reward: -1.211 [-2.327, -0.092], mean action: 1.003 [0.000, 2.000], loss: 3.955307, mae: 66.271431, mean_q: -98.955620\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6842/10000 [===================>..........] - ETA: 1:03 - reward: -1.1791 56842/100000: episode: 57, duration: 20.050s, episode steps: 1000, steps per second: 50, episode reward: -1186.556, mean reward: -1.187 [-2.259, -0.067], mean action: 1.012 [0.000, 2.000], loss: 6.051017, mae: 66.354591, mean_q: -98.979248\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 7840/10000 [======================>.......] - ETA: 43s - reward: -1.1809 57842/100000: episode: 58, duration: 20.014s, episode steps: 1000, steps per second: 50, episode reward: -1193.974, mean reward: -1.194 [-2.965, -0.033], mean action: 1.018 [0.000, 2.000], loss: 4.513803, mae: 66.280327, mean_q: -98.947853\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8842/10000 [=========================>....] - ETA: 23s - reward: -1.1954 58842/100000: episode: 59, duration: 19.892s, episode steps: 1000, steps per second: 50, episode reward: -1308.037, mean reward: -1.308 [-14.086, -0.022], mean action: 1.019 [0.000, 2.000], loss: 5.323718, mae: 66.200241, mean_q: -98.770264\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9841/10000 [============================>.] - ETA: 3s - reward: -1.1947 59842/100000: episode: 60, duration: 20.114s, episode steps: 1000, steps per second: 50, episode reward: -1189.133, mean reward: -1.189 [-2.526, -0.039], mean action: 1.076 [0.000, 2.000], loss: 4.392341, mae: 66.180641, mean_q: -98.791245\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "10000/10000 [==============================] - 200s 20ms/step - reward: -1.1920\n",
      "10 episodes - episode_reward: -1192.509 [-1308.037, -1106.542] - loss: 4.935 - mae: 66.236 - mean_q: -98.867\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "  842/10000 [=>............................] - ETA: 3:03 - reward: -1.2397 60842/100000: episode: 61, duration: 20.020s, episode steps: 1000, steps per second: 50, episode reward: -1205.725, mean reward: -1.206 [-6.606, -0.008], mean action: 1.113 [0.000, 2.000], loss: 4.224873, mae: 66.070602, mean_q: -98.617172\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 1841/10000 [====>.........................] - ETA: 2:43 - reward: -1.1878 61842/100000: episode: 62, duration: 19.988s, episode steps: 1000, steps per second: 50, episode reward: -1144.233, mean reward: -1.144 [-2.188, -0.166], mean action: 1.131 [0.000, 2.000], loss: 5.292296, mae: 65.852394, mean_q: -98.239441\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 2600/10000 [======>.......................] - ETA: 2:28 - reward: -1.1851 62602/100000: episode: 63, duration: 15.270s, episode steps: 760, steps per second: 50, episode reward: -795.368, mean reward: -1.047 [-2.394, 98.946], mean action: 1.139 [0.000, 2.000], loss: 5.111847, mae: 65.664246, mean_q: -97.960159\n",
      "The reward is higher than the best one, saving checkpoint weights\n",
      " 3345/10000 [=========>....................] - ETA: 2:13 - reward: -1.1598 63347/100000: episode: 64, duration: 15.022s, episode steps: 745, steps per second: 50, episode reward: -798.514, mean reward: -1.072 [-6.481, 98.747], mean action: 1.169 [0.000, 2.000], loss: 4.249345, mae: 65.619034, mean_q: -97.949539\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4206/10000 [===========>..................] - ETA: 1:56 - reward: -1.1426 64206/100000: episode: 65, duration: 17.287s, episode steps: 859, steps per second: 50, episode reward: -1024.000, mean reward: -1.192 [-2.546, 98.842], mean action: 1.140 [0.000, 2.000], loss: 5.168611, mae: 65.377182, mean_q: -97.480080\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4602/10000 [============>.................] - ETA: 1:48 - reward: -1.1526 64603/100000: episode: 66, duration: 8.019s, episode steps: 397, steps per second: 50, episode reward: -399.849, mean reward: -1.007 [-4.588, 98.650], mean action: 1.101 [0.000, 2.000], loss: 4.138427, mae: 65.258446, mean_q: -97.413765\n",
      "The reward is higher than the best one, saving checkpoint weights\n",
      " 5430/10000 [===============>..............] - ETA: 1:31 - reward: -1.1469 65431/100000: episode: 67, duration: 16.624s, episode steps: 828, steps per second: 50, episode reward: -922.744, mean reward: -1.114 [-2.471, 99.078], mean action: 1.236 [0.000, 2.000], loss: 5.483568, mae: 65.155563, mean_q: -97.135155\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6342/10000 [==================>...........] - ETA: 1:13 - reward: -1.1453 66344/100000: episode: 68, duration: 18.304s, episode steps: 913, steps per second: 50, episode reward: -1036.883, mean reward: -1.136 [-2.476, 99.064], mean action: 1.196 [0.000, 2.000], loss: 6.790327, mae: 64.605408, mean_q: -96.213806\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6902/10000 [===================>..........] - ETA: 1:02 - reward: -1.1233 66902/100000: episode: 69, duration: 11.225s, episode steps: 558, steps per second: 50, episode reward: -587.713, mean reward: -1.053 [-2.449, 98.813], mean action: 1.220 [0.000, 2.000], loss: 5.833140, mae: 64.240562, mean_q: -95.693550\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 7716/10000 [======================>.......] - ETA: 45s - reward: -1.1321 67717/100000: episode: 70, duration: 16.325s, episode steps: 815, steps per second: 50, episode reward: -883.126, mean reward: -1.084 [-8.376, 98.670], mean action: 1.185 [0.000, 2.000], loss: 5.982914, mae: 64.044945, mean_q: -95.381729\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8252/10000 [=======================>......] - ETA: 35s - reward: -1.1120 68252/100000: episode: 71, duration: 10.646s, episode steps: 535, steps per second: 50, episode reward: -539.913, mean reward: -1.009 [-2.404, 99.045], mean action: 1.120 [0.000, 2.000], loss: 4.164726, mae: 63.975342, mean_q: -95.345108\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9252/10000 [==========================>...] - ETA: 15s - reward: -1.1140 69252/100000: episode: 72, duration: 20.180s, episode steps: 1000, steps per second: 50, episode reward: -1130.367, mean reward: -1.130 [-1.892, -0.257], mean action: 1.057 [0.000, 2.000], loss: 3.973739, mae: 63.767406, mean_q: -95.005547\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9728/10000 [============================>.] - ETA: 5s - reward: -1.1071 69728/100000: episode: 73, duration: 9.513s, episode steps: 476, steps per second: 50, episode reward: -463.306, mean reward: -0.973 [-2.252, 99.029], mean action: 1.122 [0.000, 2.000], loss: 6.479064, mae: 63.360607, mean_q: -94.175774\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "10000/10000 [==============================] - 201s 20ms/step - reward: -1.1086\n",
      "13 episodes - episode_reward: -840.903 [-1205.725, -399.849] - loss: 5.183 - mae: 64.863 - mean_q: -96.684\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "  127/10000 [..............................] - ETA: 3:16 - reward: -1.3946 70129/100000: episode: 74, duration: 8.182s, episode steps: 401, steps per second: 49, episode reward: -395.408, mean reward: -0.986 [-2.406, 98.742], mean action: 1.207 [0.000, 2.000], loss: 5.501732, mae: 63.025635, mean_q: -93.659958\n",
      "The reward is higher than the best one, saving checkpoint weights\n",
      "  730/10000 [=>............................] - ETA: 3:08 - reward: -1.0928 70732/100000: episode: 75, duration: 12.235s, episode steps: 603, steps per second: 49, episode reward: -620.770, mean reward: -1.029 [-2.353, 98.689], mean action: 1.149 [0.000, 2.000], loss: 4.472588, mae: 62.710075, mean_q: -93.195999\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 1300/10000 [==>...........................] - ETA: 2:55 - reward: -1.0476 71301/100000: episode: 76, duration: 11.325s, episode steps: 569, steps per second: 50, episode reward: -562.881, mean reward: -0.989 [-2.266, 98.533], mean action: 1.130 [0.000, 2.000], loss: 4.313099, mae: 62.112724, mean_q: -92.194138\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 1687/10000 [====>.........................] - ETA: 2:53 - reward: -1.0218 71688/100000: episode: 77, duration: 9.085s, episode steps: 387, steps per second: 43, episode reward: -361.452, mean reward: -0.934 [-2.233, 98.915], mean action: 1.145 [0.000, 2.000], loss: 6.279969, mae: 61.614452, mean_q: -91.312477\n",
      "The reward is higher than the best one, saving checkpoint weights\n",
      " 1888/10000 [====>.........................] - ETA: 2:51 - reward: -0.9876 71889/100000: episode: 78, duration: 4.355s, episode steps: 201, steps per second: 46, episode reward: -140.910, mean reward: -0.701 [-2.126, 98.951], mean action: 1.323 [0.000, 2.000], loss: 6.884934, mae: 60.991306, mean_q: -90.296349\n",
      "The reward is higher than the best one, saving checkpoint weights\n",
      " 2353/10000 [======>.......................] - ETA: 2:40 - reward: -0.9819 72354/100000: episode: 79, duration: 9.578s, episode steps: 465, steps per second: 49, episode reward: -445.842, mean reward: -0.959 [-2.273, 98.809], mean action: 1.189 [0.000, 2.000], loss: 3.912825, mae: 60.546864, mean_q: -89.660973\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 2736/10000 [=======>......................] - ETA: 2:31 - reward: -0.9729 72738/100000: episode: 80, duration: 7.716s, episode steps: 384, steps per second: 50, episode reward: -353.562, mean reward: -0.921 [-2.449, 98.490], mean action: 1.247 [0.000, 2.000], loss: 4.708533, mae: 59.954273, mean_q: -88.671059\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 3022/10000 [========>.....................] - ETA: 2:25 - reward: -0.9575 73024/100000: episode: 81, duration: 5.837s, episode steps: 286, steps per second: 49, episode reward: -230.539, mean reward: -0.806 [-2.187, 98.964], mean action: 1.210 [0.000, 2.000], loss: 3.063383, mae: 59.247837, mean_q: -87.557083\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 3322/10000 [========>.....................] - ETA: 2:18 - reward: -0.9177 73322/100000: episode: 82, duration: 5.975s, episode steps: 298, steps per second: 50, episode reward: -253.043, mean reward: -0.849 [-2.226, 98.794], mean action: 1.225 [0.000, 2.000], loss: 4.884822, mae: 58.765915, mean_q: -86.780014\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 3555/10000 [=========>....................] - ETA: 2:13 - reward: -0.9376 73556/100000: episode: 83, duration: 4.799s, episode steps: 234, steps per second: 49, episode reward: -185.798, mean reward: -0.794 [-2.248, 98.822], mean action: 1.265 [0.000, 2.000], loss: 2.169698, mae: 58.024616, mean_q: -85.675980\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 3890/10000 [==========>...................] - ETA: 2:06 - reward: -0.9060 73890/100000: episode: 84, duration: 6.748s, episode steps: 334, steps per second: 49, episode reward: -289.780, mean reward: -0.868 [-2.142, 99.060], mean action: 1.234 [0.000, 2.000], loss: 2.938762, mae: 57.496521, mean_q: -84.768471\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4162/10000 [===========>..................] - ETA: 2:00 - reward: -0.9289 74164/100000: episode: 85, duration: 5.459s, episode steps: 274, steps per second: 50, episode reward: -244.964, mean reward: -0.894 [-8.134, 98.468], mean action: 1.401 [0.000, 2.000], loss: 3.132004, mae: 57.153294, mean_q: -84.211693\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4464/10000 [============>.................] - ETA: 1:54 - reward: -0.9228 74465/100000: episode: 86, duration: 6.205s, episode steps: 301, steps per second: 49, episode reward: -251.427, mean reward: -0.835 [-2.250, 98.939], mean action: 1.136 [0.000, 2.000], loss: 4.469200, mae: 56.344997, mean_q: -82.840958\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4703/10000 [=============>................] - ETA: 1:49 - reward: -0.8921 74703/100000: episode: 87, duration: 4.771s, episode steps: 238, steps per second: 50, episode reward: -175.121, mean reward: -0.736 [-2.145, 98.904], mean action: 1.109 [0.000, 2.000], loss: 2.928651, mae: 55.836948, mean_q: -82.028954\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 5152/10000 [==============>...............] - ETA: 1:39 - reward: -0.9148 75154/100000: episode: 88, duration: 9.153s, episode steps: 451, steps per second: 49, episode reward: -420.194, mean reward: -0.932 [-6.002, 98.468], mean action: 1.053 [0.000, 2.000], loss: 2.780747, mae: 55.305367, mean_q: -81.226730\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 5431/10000 [===============>..............] - ETA: 1:34 - reward: -0.9128 75433/100000: episode: 89, duration: 5.600s, episode steps: 279, steps per second: 50, episode reward: -244.473, mean reward: -0.876 [-6.694, 98.468], mean action: 1.183 [0.000, 2.000], loss: 2.375656, mae: 54.347424, mean_q: -79.504211\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 5812/10000 [================>.............] - ETA: 1:26 - reward: -0.9118 75813/100000: episode: 90, duration: 7.755s, episode steps: 380, steps per second: 49, episode reward: -340.154, mean reward: -0.895 [-2.176, 98.914], mean action: 0.900 [0.000, 2.000], loss: 2.137233, mae: 53.758022, mean_q: -78.735229\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6042/10000 [=================>............] - ETA: 1:21 - reward: -0.9072 76044/100000: episode: 91, duration: 4.648s, episode steps: 231, steps per second: 50, episode reward: -183.228, mean reward: -0.793 [-2.214, 98.846], mean action: 0.991 [0.000, 2.000], loss: 4.271418, mae: 53.177185, mean_q: -77.582657\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6396/10000 [==================>...........] - ETA: 1:13 - reward: -0.8901 76396/100000: episode: 92, duration: 7.058s, episode steps: 352, steps per second: 50, episode reward: -309.319, mean reward: -0.879 [-2.121, 99.056], mean action: 0.912 [0.000, 2.000], loss: 2.448796, mae: 52.365170, mean_q: -76.377869\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6570/10000 [==================>...........] - ETA: 1:10 - reward: -0.8835 76570/100000: episode: 93, duration: 3.631s, episode steps: 174, steps per second: 48, episode reward: -111.284, mean reward: -0.640 [-2.063, 98.987], mean action: 1.092 [0.000, 2.000], loss: 3.953865, mae: 51.459579, mean_q: -74.522911\n",
      "The reward is higher than the best one, saving checkpoint weights\n",
      " 6867/10000 [===================>..........] - ETA: 1:04 - reward: -0.8965 76868/100000: episode: 94, duration: 6.000s, episode steps: 298, steps per second: 50, episode reward: -252.701, mean reward: -0.848 [-2.231, 98.835], mean action: 0.893 [0.000, 2.000], loss: 1.690278, mae: 51.130688, mean_q: -74.239143\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 7183/10000 [====================>.........] - ETA: 57s - reward: -0.8817 77183/100000: episode: 95, duration: 6.438s, episode steps: 315, steps per second: 49, episode reward: -276.016, mean reward: -0.876 [-2.128, 98.963], mean action: 0.832 [0.000, 2.000], loss: 1.315947, mae: 50.228619, mean_q: -72.541824\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 7334/10000 [=====================>........] - ETA: 54s - reward: -0.8900 77335/100000: episode: 96, duration: 3.523s, episode steps: 152, steps per second: 43, episode reward: -95.627, mean reward: -0.629 [-10.926, 98.468], mean action: 1.079 [0.000, 2.000], loss: 3.388211, mae: 49.218536, mean_q: -71.005898\n",
      "The reward is higher than the best one, saving checkpoint weights\n",
      " 7482/10000 [=====================>........] - ETA: 51s - reward: -0.8837 77483/100000: episode: 97, duration: 2.997s, episode steps: 148, steps per second: 49, episode reward: -84.141, mean reward: -0.569 [-2.223, 98.701], mean action: 0.959 [0.000, 2.000], loss: 1.769027, mae: 48.832790, mean_q: -70.205399\n",
      "The reward is higher than the best one, saving checkpoint weights\n",
      " 7823/10000 [======================>.......] - ETA: 44s - reward: -0.8839 77825/100000: episode: 98, duration: 6.849s, episode steps: 342, steps per second: 50, episode reward: -304.840, mean reward: -0.891 [-9.485, 98.468], mean action: 0.825 [0.000, 2.000], loss: 3.264715, mae: 47.642231, mean_q: -68.368576\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 7939/10000 [======================>.......] - ETA: 42s - reward: -0.8763 77941/100000: episode: 99, duration: 2.464s, episode steps: 116, steps per second: 47, episode reward: -40.850, mean reward: -0.352 [-2.001, 99.074], mean action: 1.414 [0.000, 2.000], loss: 1.620532, mae: 46.905724, mean_q: -67.299484\n",
      "The reward is higher than the best one, saving checkpoint weights\n",
      " 8362/10000 [========================>.....] - ETA: 33s - reward: -0.8765 78364/100000: episode: 100, duration: 8.508s, episode steps: 423, steps per second: 50, episode reward: -372.113, mean reward: -0.880 [-2.059, 99.088], mean action: 0.905 [0.000, 2.000], loss: 2.222898, mae: 46.022751, mean_q: -65.605629\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8630/10000 [========================>.....] - ETA: 28s - reward: -0.8749 78631/100000: episode: 101, duration: 5.477s, episode steps: 267, steps per second: 49, episode reward: -221.325, mean reward: -0.829 [-12.378, 98.468], mean action: 0.925 [0.000, 2.000], loss: 1.973588, mae: 44.373775, mean_q: -62.654083\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8860/10000 [=========================>....] - ETA: 23s - reward: -0.8718 78862/100000: episode: 102, duration: 4.606s, episode steps: 231, steps per second: 50, episode reward: -175.234, mean reward: -0.759 [-2.342, 98.527], mean action: 0.957 [0.000, 2.000], loss: 1.224246, mae: 43.709881, mean_q: -61.368465\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9022/10000 [==========================>...] - ETA: 20s - reward: -0.8559 79022/100000: episode: 103, duration: 3.211s, episode steps: 160, steps per second: 50, episode reward: -94.668, mean reward: -0.592 [-2.145, 98.864], mean action: 0.975 [0.000, 2.000], loss: 3.449294, mae: 43.226051, mean_q: -60.454506\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9255/10000 [==========================>...] - ETA: 15s - reward: -0.8658 79257/100000: episode: 104, duration: 4.743s, episode steps: 235, steps per second: 50, episode reward: -194.550, mean reward: -0.828 [-16.007, 98.468], mean action: 1.085 [0.000, 2.000], loss: 1.052361, mae: 42.307938, mean_q: -58.851738\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9402/10000 [===========================>..] - ETA: 12s - reward: -0.8618 79404/100000: episode: 105, duration: 3.033s, episode steps: 147, steps per second: 48, episode reward: -89.214, mean reward: -0.607 [-7.797, 98.468], mean action: 1.116 [0.000, 2.000], loss: 1.238315, mae: 41.430435, mean_q: -57.704960\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9570/10000 [===========================>..] - ETA: 8s - reward: -0.8469 79570/100000: episode: 106, duration: 3.333s, episode steps: 166, steps per second: 50, episode reward: -98.996, mean reward: -0.596 [-2.045, 98.951], mean action: 1.090 [0.000, 2.000], loss: 1.046855, mae: 41.282269, mean_q: -56.873684\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9716/10000 [============================>.] - ETA: 5s - reward: -0.8427 79716/100000: episode: 107, duration: 2.918s, episode steps: 146, steps per second: 50, episode reward: -83.298, mean reward: -0.571 [-2.390, 98.582], mean action: 1.185 [0.000, 2.000], loss: 1.567437, mae: 41.002594, mean_q: -56.088898\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9859/10000 [============================>.] - ETA: 2s - reward: -0.8494 79860/100000: episode: 108, duration: 2.879s, episode steps: 144, steps per second: 50, episode reward: -87.416, mean reward: -0.607 [-10.473, 98.468], mean action: 1.153 [0.000, 2.000], loss: 0.850274, mae: 40.135315, mean_q: -54.663593\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "10000/10000 [==============================] - 205s 21ms/step - reward: -0.8454\n",
      "35 episodes - episode_reward: -245.461 [-620.770, -40.850] - loss: 3.167 - mae: 53.800 - mean_q: -78.433\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "   10/10000 [..............................] - ETA: 3:13 - reward: -2.0795 80012/100000: episode: 109, duration: 3.185s, episode steps: 152, steps per second: 48, episode reward: -102.832, mean reward: -0.677 [-15.676, 98.468], mean action: 1.178 [0.000, 2.000], loss: 2.124347, mae: 40.057774, mean_q: -54.155609\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "  238/10000 [..............................] - ETA: 3:14 - reward: -0.8726 80240/100000: episode: 110, duration: 4.558s, episode steps: 228, steps per second: 50, episode reward: -186.890, mean reward: -0.820 [-14.077, 98.468], mean action: 1.132 [0.000, 2.000], loss: 1.241631, mae: 39.410957, mean_q: -53.108170\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "  463/10000 [>.............................] - ETA: 3:10 - reward: -0.8166 80464/100000: episode: 111, duration: 4.467s, episode steps: 224, steps per second: 50, episode reward: -168.691, mean reward: -0.753 [-2.414, 98.536], mean action: 1.000 [0.000, 2.000], loss: 2.267582, mae: 39.092220, mean_q: -51.932091\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "  618/10000 [>.............................] - ETA: 3:07 - reward: -0.7694 80620/100000: episode: 112, duration: 3.098s, episode steps: 156, steps per second: 50, episode reward: -99.018, mean reward: -0.635 [-2.505, 98.505], mean action: 1.186 [0.000, 2.000], loss: 0.900990, mae: 38.360458, mean_q: -50.722706\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "  771/10000 [=>............................] - ETA: 3:05 - reward: -0.7484 80773/100000: episode: 113, duration: 3.219s, episode steps: 153, steps per second: 48, episode reward: -101.669, mean reward: -0.665 [-12.700, 98.443], mean action: 1.222 [0.000, 2.000], loss: 2.139633, mae: 38.497952, mean_q: -50.567120\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 1014/10000 [==>...........................] - ETA: 3:00 - reward: -0.7684 81015/100000: episode: 114, duration: 4.811s, episode steps: 242, steps per second: 50, episode reward: -200.467, mean reward: -0.828 [-11.375, 98.464], mean action: 1.178 [0.000, 2.000], loss: 1.307213, mae: 38.332779, mean_q: -50.278248\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 1175/10000 [==>...........................] - ETA: 2:57 - reward: -0.6578 81175/100000: episode: 115, duration: 3.201s, episode steps: 160, steps per second: 50, episode reward: -92.177, mean reward: -0.576 [-2.243, 98.917], mean action: 1.212 [0.000, 2.000], loss: 2.508716, mae: 37.895420, mean_q: -49.849628\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 1430/10000 [===>..........................] - ETA: 2:52 - reward: -0.6742 81430/100000: episode: 116, duration: 5.219s, episode steps: 255, steps per second: 49, episode reward: -191.266, mean reward: -0.750 [-2.213, 99.005], mean action: 1.051 [0.000, 2.000], loss: 1.340593, mae: 37.821682, mean_q: -49.137569\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 1642/10000 [===>..........................] - ETA: 2:48 - reward: -0.7430 81643/100000: episode: 117, duration: 4.282s, episode steps: 213, steps per second: 50, episode reward: -157.172, mean reward: -0.738 [-2.391, 98.645], mean action: 1.136 [0.000, 2.000], loss: 2.748971, mae: 37.759274, mean_q: -49.188984\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 1806/10000 [====>.........................] - ETA: 2:44 - reward: -0.7299 81808/100000: episode: 118, duration: 3.300s, episode steps: 165, steps per second: 50, episode reward: -99.070, mean reward: -0.600 [-1.965, 98.940], mean action: 1.152 [0.000, 2.000], loss: 1.437773, mae: 37.410206, mean_q: -48.581970\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 1959/10000 [====>.........................] - ETA: 2:41 - reward: -0.7179 81960/100000: episode: 119, duration: 3.052s, episode steps: 152, steps per second: 50, episode reward: -87.358, mean reward: -0.575 [-2.179, 98.684], mean action: 1.145 [0.000, 2.000], loss: 1.963115, mae: 37.145176, mean_q: -47.699013\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 2187/10000 [=====>........................] - ETA: 2:37 - reward: -0.6763 82187/100000: episode: 120, duration: 4.692s, episode steps: 227, steps per second: 48, episode reward: -171.346, mean reward: -0.755 [-7.027, 98.423], mean action: 1.084 [0.000, 2.000], loss: 2.483761, mae: 37.071587, mean_q: -48.421314\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 2369/10000 [======>.......................] - ETA: 2:33 - reward: -0.7224 82370/100000: episode: 121, duration: 3.678s, episode steps: 183, steps per second: 50, episode reward: -133.693, mean reward: -0.731 [-12.069, 98.478], mean action: 1.180 [0.000, 2.000], loss: 1.124473, mae: 36.459309, mean_q: -47.516048\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 2605/10000 [======>.......................] - ETA: 2:29 - reward: -0.7334 82606/100000: episode: 122, duration: 4.695s, episode steps: 236, steps per second: 50, episode reward: -199.173, mean reward: -0.844 [-14.466, 98.516], mean action: 1.174 [0.000, 2.000], loss: 1.823417, mae: 36.346916, mean_q: -46.889740\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 2765/10000 [=======>......................] - ETA: 2:26 - reward: -0.6941 82765/100000: episode: 123, duration: 3.300s, episode steps: 159, steps per second: 48, episode reward: -107.359, mean reward: -0.675 [-9.989, 98.518], mean action: 1.189 [0.000, 2.000], loss: 2.684940, mae: 36.020046, mean_q: -46.448059\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 2905/10000 [=======>......................] - ETA: 2:23 - reward: -0.6856 82905/100000: episode: 124, duration: 2.796s, episode steps: 140, steps per second: 50, episode reward: -72.236, mean reward: -0.516 [-2.336, 98.612], mean action: 1.250 [0.000, 2.000], loss: 2.165747, mae: 35.614559, mean_q: -45.155846\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 3057/10000 [========>.....................] - ETA: 2:20 - reward: -0.7117 83059/100000: episode: 125, duration: 3.097s, episode steps: 154, steps per second: 50, episode reward: -86.696, mean reward: -0.563 [-2.068, 98.801], mean action: 1.253 [0.000, 2.000], loss: 2.522434, mae: 35.450081, mean_q: -45.685696\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 3216/10000 [========>.....................] - ETA: 2:16 - reward: -0.7072 83218/100000: episode: 126, duration: 3.176s, episode steps: 159, steps per second: 50, episode reward: -99.121, mean reward: -0.623 [-2.376, 98.509], mean action: 1.157 [0.000, 2.000], loss: 2.041181, mae: 35.129627, mean_q: -44.656822\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 3378/10000 [=========>....................] - ETA: 2:13 - reward: -0.6725 83378/100000: episode: 127, duration: 3.374s, episode steps: 160, steps per second: 47, episode reward: -94.461, mean reward: -0.590 [-2.264, 98.548], mean action: 1.156 [0.000, 2.000], loss: 1.245820, mae: 34.699211, mean_q: -43.575085\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 3527/10000 [=========>....................] - ETA: 2:10 - reward: -0.6966 83529/100000: episode: 128, duration: 3.026s, episode steps: 151, steps per second: 50, episode reward: -87.853, mean reward: -0.582 [-2.340, 98.577], mean action: 1.106 [0.000, 2.000], loss: 1.703540, mae: 34.345119, mean_q: -43.173817\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 3713/10000 [==========>...................] - ETA: 2:06 - reward: -0.6964 83714/100000: episode: 129, duration: 3.705s, episode steps: 185, steps per second: 50, episode reward: -127.513, mean reward: -0.689 [-5.982, 98.487], mean action: 1.162 [0.000, 2.000], loss: 1.791628, mae: 33.873520, mean_q: -42.828506\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 3937/10000 [==========>...................] - ETA: 2:02 - reward: -0.6786 83937/100000: episode: 130, duration: 4.458s, episode steps: 223, steps per second: 50, episode reward: -184.597, mean reward: -0.828 [-16.328, 98.408], mean action: 1.368 [0.000, 2.000], loss: 1.705500, mae: 33.683556, mean_q: -41.952564\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4096/10000 [===========>..................] - ETA: 1:59 - reward: -0.6989 84098/100000: episode: 131, duration: 3.354s, episode steps: 161, steps per second: 48, episode reward: -93.587, mean reward: -0.581 [-2.189, 98.660], mean action: 1.106 [0.000, 2.000], loss: 1.784323, mae: 33.080284, mean_q: -41.567730\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4288/10000 [===========>..................] - ETA: 1:55 - reward: -0.6995 84290/100000: episode: 132, duration: 3.859s, episode steps: 192, steps per second: 50, episode reward: -137.112, mean reward: -0.714 [-10.926, 98.468], mean action: 1.141 [0.000, 2.000], loss: 1.716606, mae: 32.501751, mean_q: -39.642570\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4402/10000 [============>.................] - ETA: 1:53 - reward: -0.6687 84402/100000: episode: 133, duration: 2.228s, episode steps: 112, steps per second: 50, episode reward: -41.172, mean reward: -0.368 [-2.306, 98.673], mean action: 1.304 [0.000, 2.000], loss: 0.972212, mae: 31.928522, mean_q: -38.336346\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4565/10000 [============>.................] - ETA: 1:49 - reward: -0.6701 84565/100000: episode: 134, duration: 3.299s, episode steps: 163, steps per second: 49, episode reward: -115.393, mean reward: -0.708 [-16.394, 98.410], mean action: 1.252 [0.000, 2.000], loss: 1.239688, mae: 31.732250, mean_q: -39.054958\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4733/10000 [=============>................] - ETA: 1:46 - reward: -0.6880 84734/100000: episode: 135, duration: 3.689s, episode steps: 169, steps per second: 46, episode reward: -98.386, mean reward: -0.582 [-2.041, 99.003], mean action: 1.260 [0.000, 2.000], loss: 1.333699, mae: 31.591578, mean_q: -38.580090\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4899/10000 [=============>................] - ETA: 1:43 - reward: -0.6844 84901/100000: episode: 136, duration: 3.450s, episode steps: 167, steps per second: 48, episode reward: -97.580, mean reward: -0.584 [-2.270, 98.886], mean action: 1.108 [0.000, 2.000], loss: 1.873018, mae: 30.928507, mean_q: -37.723465\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 5109/10000 [==============>...............] - ETA: 1:39 - reward: -0.6701 85109/100000: episode: 137, duration: 4.209s, episode steps: 208, steps per second: 49, episode reward: -168.286, mean reward: -0.809 [-16.963, 98.468], mean action: 1.236 [0.000, 2.000], loss: 1.144427, mae: 30.598948, mean_q: -36.208683\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 5204/10000 [==============>...............] - ETA: 1:37 - reward: -0.6612 85204/100000: episode: 138, duration: 1.958s, episode steps: 95, steps per second: 49, episode reward: -17.788, mean reward: -0.187 [-1.969, 98.961], mean action: 1.305 [0.000, 2.000], loss: 1.401271, mae: 30.779949, mean_q: -35.477196\n",
      "The reward is higher than the best one, saving checkpoint weights\n",
      " 5362/10000 [===============>..............] - ETA: 1:34 - reward: -0.6780 85363/100000: episode: 139, duration: 3.521s, episode steps: 159, steps per second: 45, episode reward: -95.946, mean reward: -0.603 [-2.390, 98.478], mean action: 1.138 [0.000, 2.000], loss: 0.919590, mae: 29.610558, mean_q: -34.920860\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 5511/10000 [===============>..............] - ETA: 1:31 - reward: -0.6746 85513/100000: episode: 140, duration: 3.122s, episode steps: 150, steps per second: 48, episode reward: -83.273, mean reward: -0.555 [-2.353, 98.624], mean action: 1.153 [0.000, 2.000], loss: 0.817324, mae: 29.484629, mean_q: -35.146030\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 5688/10000 [================>.............] - ETA: 1:27 - reward: -0.6744 85690/100000: episode: 141, duration: 3.650s, episode steps: 177, steps per second: 48, episode reward: -118.971, mean reward: -0.672 [-8.542, 98.484], mean action: 1.102 [0.000, 2.000], loss: 1.215820, mae: 29.277679, mean_q: -34.521496\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 5855/10000 [================>.............] - ETA: 1:24 - reward: -0.6717 85856/100000: episode: 142, duration: 3.485s, episode steps: 166, steps per second: 48, episode reward: -94.308, mean reward: -0.568 [-2.025, 98.929], mean action: 1.193 [0.000, 2.000], loss: 1.215920, mae: 29.274702, mean_q: -34.272060\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6004/10000 [=================>............] - ETA: 1:21 - reward: -0.6696 86006/100000: episode: 143, duration: 2.975s, episode steps: 150, steps per second: 50, episode reward: -89.836, mean reward: -0.599 [-9.119, 98.453], mean action: 1.067 [0.000, 2.000], loss: 0.831219, mae: 28.866816, mean_q: -33.255592\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6154/10000 [=================>............] - ETA: 1:18 - reward: -0.6687 86155/100000: episode: 144, duration: 2.939s, episode steps: 149, steps per second: 51, episode reward: -93.201, mean reward: -0.626 [-9.657, 98.484], mean action: 1.154 [0.000, 2.000], loss: 1.214923, mae: 28.576654, mean_q: -32.326172\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6312/10000 [=================>............] - ETA: 1:15 - reward: -0.6673 86313/100000: episode: 145, duration: 3.128s, episode steps: 158, steps per second: 51, episode reward: -96.633, mean reward: -0.612 [-2.280, 98.542], mean action: 0.994 [0.000, 2.000], loss: 1.197954, mae: 28.635836, mean_q: -32.405308\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6491/10000 [==================>...........] - ETA: 1:11 - reward: -0.6530 86491/100000: episode: 146, duration: 3.498s, episode steps: 178, steps per second: 51, episode reward: -125.515, mean reward: -0.705 [-12.231, 98.452], mean action: 1.096 [0.000, 2.000], loss: 1.631864, mae: 28.039606, mean_q: -31.679245\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6652/10000 [==================>...........] - ETA: 1:08 - reward: -0.6516 86652/100000: episode: 147, duration: 3.337s, episode steps: 161, steps per second: 48, episode reward: -95.592, mean reward: -0.594 [-2.165, 98.605], mean action: 1.081 [0.000, 2.000], loss: 1.086795, mae: 28.338516, mean_q: -31.349825\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6840/10000 [===================>..........] - ETA: 1:04 - reward: -0.6674 86841/100000: episode: 148, duration: 3.735s, episode steps: 189, steps per second: 51, episode reward: -132.445, mean reward: -0.701 [-8.502, 98.412], mean action: 1.037 [0.000, 2.000], loss: 1.064224, mae: 27.918804, mean_q: -31.739651\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6986/10000 [===================>..........] - ETA: 1:01 - reward: -0.6499 86986/100000: episode: 149, duration: 2.858s, episode steps: 145, steps per second: 51, episode reward: -73.507, mean reward: -0.507 [-2.077, 98.746], mean action: 1.172 [0.000, 2.000], loss: 0.829645, mae: 27.516138, mean_q: -30.194895\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 7141/10000 [====================>.........] - ETA: 58s - reward: -0.6488 87141/100000: episode: 150, duration: 3.076s, episode steps: 155, steps per second: 50, episode reward: -92.825, mean reward: -0.599 [-5.821, 98.468], mean action: 1.077 [0.000, 2.000], loss: 0.901592, mae: 28.195372, mean_q: -31.108109\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 7287/10000 [====================>.........] - ETA: 55s - reward: -0.6623 87288/100000: episode: 151, duration: 2.916s, episode steps: 147, steps per second: 50, episode reward: -94.612, mean reward: -0.644 [-11.367, 98.440], mean action: 1.102 [0.000, 2.000], loss: 1.159063, mae: 27.433903, mean_q: -29.372116\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 7469/10000 [=====================>........] - ETA: 51s - reward: -0.6503 87469/100000: episode: 152, duration: 3.565s, episode steps: 181, steps per second: 51, episode reward: -129.321, mean reward: -0.714 [-12.062, 98.419], mean action: 1.105 [0.000, 2.000], loss: 1.770943, mae: 27.132999, mean_q: -28.686966\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 7651/10000 [=====================>........] - ETA: 47s - reward: -0.6642 87653/100000: episode: 153, duration: 3.627s, episode steps: 184, steps per second: 51, episode reward: -127.914, mean reward: -0.695 [-4.879, 98.524], mean action: 1.152 [0.000, 2.000], loss: 1.525429, mae: 27.647902, mean_q: -29.127882\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 7774/10000 [======================>.......] - ETA: 45s - reward: -0.6476 87774/100000: episode: 154, duration: 2.400s, episode steps: 121, steps per second: 50, episode reward: -49.835, mean reward: -0.412 [-2.322, 98.818], mean action: 1.264 [0.000, 2.000], loss: 1.268163, mae: 27.418257, mean_q: -30.212187\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 7922/10000 [======================>.......] - ETA: 42s - reward: -0.6599 87923/100000: episode: 155, duration: 2.983s, episode steps: 149, steps per second: 50, episode reward: -94.828, mean reward: -0.636 [-11.377, 98.440], mean action: 1.101 [0.000, 2.000], loss: 1.065448, mae: 27.457930, mean_q: -29.049137\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8134/10000 [=======================>......] - ETA: 37s - reward: -0.6483 88134/100000: episode: 156, duration: 4.341s, episode steps: 211, steps per second: 49, episode reward: -143.539, mean reward: -0.680 [-2.104, 98.819], mean action: 1.047 [0.000, 2.000], loss: 0.985760, mae: 27.293417, mean_q: -28.601757\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8292/10000 [=======================>......] - ETA: 34s - reward: -0.6474 88292/100000: episode: 157, duration: 3.145s, episode steps: 158, steps per second: 50, episode reward: -95.317, mean reward: -0.603 [-2.332, 98.506], mean action: 1.127 [0.000, 2.000], loss: 0.907041, mae: 27.194918, mean_q: -28.547091\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8474/10000 [========================>.....] - ETA: 30s - reward: -0.6600 88476/100000: episode: 158, duration: 3.628s, episode steps: 184, steps per second: 51, episode reward: -127.078, mean reward: -0.691 [-5.576, 98.524], mean action: 1.168 [0.000, 2.000], loss: 0.893484, mae: 27.157322, mean_q: -28.349089\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8594/10000 [========================>.....] - ETA: 28s - reward: -0.6449 88594/100000: episode: 159, duration: 2.339s, episode steps: 118, steps per second: 50, episode reward: -46.417, mean reward: -0.393 [-2.259, 98.734], mean action: 1.220 [0.000, 2.000], loss: 1.211828, mae: 27.032339, mean_q: -28.616562\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8751/10000 [=========================>....] - ETA: 25s - reward: -0.6432 88751/100000: episode: 160, duration: 3.235s, episode steps: 157, steps per second: 49, episode reward: -86.667, mean reward: -0.552 [-2.031, 98.823], mean action: 1.210 [0.000, 2.000], loss: 0.794338, mae: 26.859596, mean_q: -28.175264\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8921/10000 [=========================>....] - ETA: 21s - reward: -0.6433 88921/100000: episode: 161, duration: 3.353s, episode steps: 170, steps per second: 51, episode reward: -110.096, mean reward: -0.648 [-2.382, 98.538], mean action: 1.082 [0.000, 2.000], loss: 1.159318, mae: 26.861084, mean_q: -28.163740\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9103/10000 [==========================>...] - ETA: 18s - reward: -0.6449 89103/100000: episode: 162, duration: 3.594s, episode steps: 182, steps per second: 51, episode reward: -132.086, mean reward: -0.726 [-12.805, 98.546], mean action: 1.132 [0.000, 2.000], loss: 1.240618, mae: 26.697422, mean_q: -27.941051\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9216/10000 [==========================>...] - ETA: 15s - reward: -0.6414 89216/100000: episode: 163, duration: 2.228s, episode steps: 113, steps per second: 51, episode reward: -40.382, mean reward: -0.357 [-2.272, 98.667], mean action: 1.248 [0.000, 2.000], loss: 1.095223, mae: 26.895061, mean_q: -26.363262\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9337/10000 [===========================>..] - ETA: 13s - reward: -0.6490 89338/100000: episode: 164, duration: 2.532s, episode steps: 122, steps per second: 48, episode reward: -49.393, mean reward: -0.405 [-2.098, 98.715], mean action: 1.262 [0.000, 2.000], loss: 0.883537, mae: 26.220566, mean_q: -26.857441\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9498/10000 [===========================>..] - ETA: 10s - reward: -0.6475 89499/100000: episode: 165, duration: 3.163s, episode steps: 161, steps per second: 51, episode reward: -90.763, mean reward: -0.564 [-2.068, 98.760], mean action: 1.199 [0.000, 2.000], loss: 0.866440, mae: 26.122908, mean_q: -25.282654\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9662/10000 [===========================>..] - ETA: 6s - reward: -0.6474 89663/100000: episode: 166, duration: 3.243s, episode steps: 164, steps per second: 51, episode reward: -105.497, mean reward: -0.643 [-5.239, 98.515], mean action: 1.189 [0.000, 2.000], loss: 1.114976, mae: 25.795929, mean_q: -26.388821\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9781/10000 [============================>.] - ETA: 4s - reward: -0.6442 89783/100000: episode: 167, duration: 2.376s, episode steps: 120, steps per second: 51, episode reward: -46.776, mean reward: -0.390 [-2.229, 98.872], mean action: 1.308 [0.000, 2.000], loss: 1.200413, mae: 26.343456, mean_q: -26.289778\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9946/10000 [============================>.] - ETA: 1s - reward: -0.6336 89946/100000: episode: 168, duration: 3.204s, episode steps: 163, steps per second: 51, episode reward: -98.593, mean reward: -0.605 [-2.736, 98.678], mean action: 1.239 [0.000, 2.000], loss: 1.101843, mae: 25.607229, mean_q: -24.270634\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "10000/10000 [==============================] - 202s 20ms/step - reward: -0.6357\n",
      "60 episodes - episode_reward: -108.019 [-200.467, -17.788] - loss: 1.443 - mae: 31.719 - mean_q: -37.937\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "  226/10000 [..............................] - ETA: 3:11 - reward: -1.2414 90227/100000: episode: 169, duration: 5.679s, episode steps: 281, steps per second: 49, episode reward: -237.125, mean reward: -0.844 [-13.129, 98.546], mean action: 1.089 [0.000, 2.000], loss: 1.147041, mae: 25.638741, mean_q: -24.649809\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "  348/10000 [>.............................] - ETA: 3:10 - reward: -0.9524 90349/100000: episode: 170, duration: 2.422s, episode steps: 122, steps per second: 50, episode reward: -50.654, mean reward: -0.415 [-2.270, 98.745], mean action: 1.270 [0.000, 2.000], loss: 0.960709, mae: 25.739817, mean_q: -24.623226\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "  512/10000 [>.............................] - ETA: 3:07 - reward: -0.6433 90512/100000: episode: 171, duration: 3.224s, episode steps: 163, steps per second: 51, episode reward: -96.704, mean reward: -0.593 [-2.387, 98.521], mean action: 1.141 [0.000, 2.000], loss: 0.941956, mae: 25.324886, mean_q: -23.647339\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "  685/10000 [=>............................] - ETA: 3:03 - reward: -0.7849 90687/100000: episode: 172, duration: 3.454s, episode steps: 175, steps per second: 51, episode reward: -110.513, mean reward: -0.632 [-2.294, 98.911], mean action: 1.246 [0.000, 2.000], loss: 0.924889, mae: 25.163057, mean_q: -23.797056\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "  865/10000 [=>............................] - ETA: 3:00 - reward: -0.7505 90867/100000: episode: 173, duration: 3.572s, episode steps: 180, steps per second: 50, episode reward: -111.335, mean reward: -0.619 [-2.464, 98.846], mean action: 1.294 [0.000, 2.000], loss: 0.956828, mae: 24.816683, mean_q: -23.020294\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "  997/10000 [=>............................] - ETA: 2:57 - reward: -0.7136 90999/100000: episode: 174, duration: 2.621s, episode steps: 132, steps per second: 50, episode reward: -62.495, mean reward: -0.473 [-2.112, 99.013], mean action: 1.311 [0.000, 2.000], loss: 1.325614, mae: 24.912409, mean_q: -23.453215\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 1198/10000 [==>...........................] - ETA: 2:53 - reward: -0.7295 91199/100000: episode: 175, duration: 3.942s, episode steps: 200, steps per second: 51, episode reward: -161.707, mean reward: -0.809 [-14.863, 98.542], mean action: 1.200 [0.000, 2.000], loss: 1.290833, mae: 24.632441, mean_q: -22.260324\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 1331/10000 [==>...........................] - ETA: 2:52 - reward: -0.7030 91333/100000: episode: 176, duration: 2.798s, episode steps: 134, steps per second: 48, episode reward: -62.189, mean reward: -0.464 [-2.238, 99.041], mean action: 1.284 [0.000, 2.000], loss: 0.997168, mae: 24.336639, mean_q: -22.912502\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 1487/10000 [===>..........................] - ETA: 2:48 - reward: -0.7009 91488/100000: episode: 177, duration: 3.051s, episode steps: 155, steps per second: 51, episode reward: -106.043, mean reward: -0.684 [-12.415, 98.515], mean action: 1.142 [0.000, 2.000], loss: 1.016726, mae: 24.145599, mean_q: -21.343611\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 1642/10000 [===>..........................] - ETA: 2:45 - reward: -0.6342 91642/100000: episode: 178, duration: 3.046s, episode steps: 154, steps per second: 51, episode reward: -97.678, mean reward: -0.634 [-9.452, 98.563], mean action: 1.065 [0.000, 2.000], loss: 1.228505, mae: 23.928082, mean_q: -21.356001\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 1797/10000 [====>.........................] - ETA: 2:42 - reward: -0.6862 91798/100000: episode: 179, duration: 3.069s, episode steps: 156, steps per second: 51, episode reward: -93.100, mean reward: -0.597 [-2.497, 98.593], mean action: 1.167 [0.000, 2.000], loss: 0.843983, mae: 23.799532, mean_q: -20.999380\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 1955/10000 [====>.........................] - ETA: 2:39 - reward: -0.6863 91957/100000: episode: 180, duration: 3.138s, episode steps: 159, steps per second: 51, episode reward: -110.143, mean reward: -0.693 [-12.283, 98.563], mean action: 1.107 [0.000, 2.000], loss: 0.739121, mae: 23.996798, mean_q: -21.971554\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 2110/10000 [=====>........................] - ETA: 2:36 - reward: -0.6328 92110/100000: episode: 181, duration: 3.141s, episode steps: 153, steps per second: 49, episode reward: -90.606, mean reward: -0.592 [-2.487, 98.551], mean action: 1.196 [0.000, 2.000], loss: 1.117595, mae: 23.936447, mean_q: -20.568569\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 2256/10000 [=====>........................] - ETA: 2:34 - reward: -0.6775 92257/100000: episode: 182, duration: 3.005s, episode steps: 147, steps per second: 49, episode reward: -94.896, mean reward: -0.646 [-12.912, 98.466], mean action: 1.116 [0.000, 2.000], loss: 1.056241, mae: 23.830446, mean_q: -20.457420\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 2411/10000 [======>.......................] - ETA: 2:31 - reward: -0.6724 92413/100000: episode: 183, duration: 3.089s, episode steps: 156, steps per second: 50, episode reward: -94.428, mean reward: -0.605 [-2.485, 98.203], mean action: 1.083 [0.000, 2.000], loss: 1.148670, mae: 23.545586, mean_q: -21.026646\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 2594/10000 [======>.......................] - ETA: 2:27 - reward: -0.6782 92595/100000: episode: 184, duration: 3.609s, episode steps: 182, steps per second: 50, episode reward: -136.366, mean reward: -0.749 [-12.594, 98.422], mean action: 1.192 [0.000, 2.000], loss: 1.145180, mae: 23.430038, mean_q: -20.423134\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 2789/10000 [=======>......................] - ETA: 2:23 - reward: -0.6461 92789/100000: episode: 185, duration: 3.990s, episode steps: 194, steps per second: 49, episode reward: -141.007, mean reward: -0.727 [-10.357, 98.489], mean action: 1.201 [0.000, 2.000], loss: 0.844369, mae: 23.510355, mean_q: -18.873850\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 2911/10000 [=======>......................] - ETA: 2:21 - reward: -0.6706 92912/100000: episode: 186, duration: 2.442s, episode steps: 123, steps per second: 50, episode reward: -51.345, mean reward: -0.417 [-2.342, 98.841], mean action: 1.309 [0.000, 2.000], loss: 0.966970, mae: 23.088379, mean_q: -19.359474\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 3071/10000 [========>.....................] - ETA: 2:18 - reward: -0.6702 93073/100000: episode: 187, duration: 3.210s, episode steps: 161, steps per second: 50, episode reward: -107.691, mean reward: -0.669 [-5.897, 98.654], mean action: 1.143 [0.000, 2.000], loss: 1.095269, mae: 23.317949, mean_q: -19.248043\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 3185/10000 [========>.....................] - ETA: 2:15 - reward: -0.6595 93186/100000: episode: 188, duration: 2.255s, episode steps: 113, steps per second: 50, episode reward: -40.762, mean reward: -0.361 [-2.140, 98.808], mean action: 1.301 [0.000, 2.000], loss: 1.260405, mae: 23.275272, mean_q: -18.889042\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 3379/10000 [=========>....................] - ETA: 2:12 - reward: -0.6345 93379/100000: episode: 189, duration: 3.961s, episode steps: 193, steps per second: 49, episode reward: -142.232, mean reward: -0.737 [-9.113, 98.544], mean action: 1.031 [0.000, 2.000], loss: 0.980806, mae: 22.788858, mean_q: -17.850067\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 3528/10000 [=========>....................] - ETA: 2:09 - reward: -0.6579 93529/100000: episode: 190, duration: 2.960s, episode steps: 150, steps per second: 51, episode reward: -78.176, mean reward: -0.521 [-2.142, 98.952], mean action: 1.167 [0.000, 2.000], loss: 0.929284, mae: 22.995684, mean_q: -17.873461\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 3686/10000 [==========>...................] - ETA: 2:06 - reward: -0.6297 93686/100000: episode: 191, duration: 3.106s, episode steps: 157, steps per second: 51, episode reward: -98.857, mean reward: -0.630 [-2.416, 98.560], mean action: 1.146 [0.000, 2.000], loss: 0.733339, mae: 22.557110, mean_q: -17.519703\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 3835/10000 [==========>...................] - ETA: 2:03 - reward: -0.6533 93836/100000: episode: 192, duration: 2.961s, episode steps: 150, steps per second: 51, episode reward: -85.856, mean reward: -0.572 [-2.372, 98.724], mean action: 1.200 [0.000, 2.000], loss: 1.316382, mae: 22.811939, mean_q: -17.395321\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 3985/10000 [==========>...................] - ETA: 2:00 - reward: -0.6503 93987/100000: episode: 193, duration: 3.132s, episode steps: 151, steps per second: 48, episode reward: -87.410, mean reward: -0.579 [-2.351, 98.624], mean action: 1.132 [0.000, 2.000], loss: 0.746905, mae: 22.482124, mean_q: -18.152330\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4147/10000 [===========>..................] - ETA: 1:56 - reward: -0.6497 94149/100000: episode: 194, duration: 3.189s, episode steps: 162, steps per second: 51, episode reward: -102.984, mean reward: -0.636 [-2.441, 98.497], mean action: 1.056 [0.000, 2.000], loss: 1.310771, mae: 22.501095, mean_q: -17.977118\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4303/10000 [===========>..................] - ETA: 1:53 - reward: -0.6253 94303/100000: episode: 195, duration: 3.032s, episode steps: 154, steps per second: 51, episode reward: -93.456, mean reward: -0.607 [-2.463, 98.662], mean action: 1.195 [0.000, 2.000], loss: 0.693759, mae: 22.490061, mean_q: -17.077244\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4467/10000 [============>.................] - ETA: 1:50 - reward: -0.6453 94469/100000: episode: 196, duration: 3.289s, episode steps: 166, steps per second: 50, episode reward: -93.978, mean reward: -0.566 [-2.181, 98.903], mean action: 1.078 [0.000, 2.000], loss: 0.763629, mae: 22.490623, mean_q: -17.687330\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4610/10000 [============>.................] - ETA: 1:47 - reward: -0.6414 94611/100000: episode: 197, duration: 2.947s, episode steps: 142, steps per second: 48, episode reward: -73.705, mean reward: -0.519 [-2.357, 98.685], mean action: 1.099 [0.000, 2.000], loss: 1.070624, mae: 22.317593, mean_q: -16.551649\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4753/10000 [=============>................] - ETA: 1:44 - reward: -0.6380 94755/100000: episode: 198, duration: 2.855s, episode steps: 144, steps per second: 50, episode reward: -76.645, mean reward: -0.532 [-2.272, 98.786], mean action: 1.104 [0.000, 2.000], loss: 0.751657, mae: 22.135708, mean_q: -16.681183\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 4912/10000 [=============>................] - ETA: 1:41 - reward: -0.6391 94914/100000: episode: 199, duration: 3.139s, episode steps: 159, steps per second: 51, episode reward: -107.202, mean reward: -0.674 [-6.887, 98.608], mean action: 1.113 [0.000, 2.000], loss: 1.383212, mae: 21.966122, mean_q: -16.749054\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 5083/10000 [==============>...............] - ETA: 1:38 - reward: -0.6389 95084/100000: episode: 200, duration: 3.356s, episode steps: 170, steps per second: 51, episode reward: -106.639, mean reward: -0.627 [-2.403, 98.692], mean action: 1.135 [0.000, 2.000], loss: 0.702079, mae: 21.736301, mean_q: -15.556430\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 5277/10000 [==============>...............] - ETA: 1:34 - reward: -0.6413 95278/100000: episode: 201, duration: 3.977s, episode steps: 194, steps per second: 49, episode reward: -136.903, mean reward: -0.706 [-2.446, 98.597], mean action: 1.113 [0.000, 2.000], loss: 1.104072, mae: 21.510067, mean_q: -16.032980\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 5429/10000 [===============>..............] - ETA: 1:31 - reward: -0.6386 95430/100000: episode: 202, duration: 3.001s, episode steps: 152, steps per second: 51, episode reward: -82.699, mean reward: -0.544 [-2.259, 98.744], mean action: 1.132 [0.000, 2.000], loss: 0.685416, mae: 21.807127, mean_q: -15.698669\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 5581/10000 [===============>..............] - ETA: 1:28 - reward: -0.6367 95582/100000: episode: 203, duration: 3.008s, episode steps: 152, steps per second: 51, episode reward: -85.933, mean reward: -0.565 [-2.260, 98.954], mean action: 1.138 [0.000, 2.000], loss: 0.986170, mae: 21.135950, mean_q: -16.319462\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 5747/10000 [================>.............] - ETA: 1:24 - reward: -0.6363 95748/100000: episode: 204, duration: 3.296s, episode steps: 166, steps per second: 50, episode reward: -103.479, mean reward: -0.623 [-2.358, 98.781], mean action: 1.157 [0.000, 2.000], loss: 0.786650, mae: 21.337585, mean_q: -16.069080\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 5902/10000 [================>.............] - ETA: 1:21 - reward: -0.6351 95904/100000: episode: 205, duration: 3.251s, episode steps: 156, steps per second: 48, episode reward: -93.652, mean reward: -0.600 [-2.324, 98.559], mean action: 1.083 [0.000, 2.000], loss: 0.860982, mae: 21.354689, mean_q: -16.020094\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6085/10000 [=================>............] - ETA: 1:18 - reward: -0.6355 96086/100000: episode: 206, duration: 3.588s, episode steps: 182, steps per second: 51, episode reward: -116.376, mean reward: -0.639 [-2.239, 98.950], mean action: 1.242 [0.000, 2.000], loss: 0.666252, mae: 21.201677, mean_q: -15.668845\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6258/10000 [=================>............] - ETA: 1:14 - reward: -0.6202 96258/100000: episode: 207, duration: 3.387s, episode steps: 172, steps per second: 51, episode reward: -113.328, mean reward: -0.659 [-2.740, 98.620], mean action: 1.029 [0.000, 2.000], loss: 0.836177, mae: 20.947035, mean_q: -15.330541\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6376/10000 [==================>...........] - ETA: 1:12 - reward: -0.6320 96378/100000: episode: 208, duration: 2.512s, episode steps: 120, steps per second: 48, episode reward: -50.711, mean reward: -0.423 [-2.296, 98.745], mean action: 1.208 [0.000, 2.000], loss: 0.700899, mae: 21.102930, mean_q: -14.432032\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6529/10000 [==================>...........] - ETA: 1:09 - reward: -0.6307 96531/100000: episode: 209, duration: 3.023s, episode steps: 153, steps per second: 51, episode reward: -88.917, mean reward: -0.581 [-2.324, 98.675], mean action: 1.190 [0.000, 2.000], loss: 0.806592, mae: 21.116720, mean_q: -15.148457\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6652/10000 [==================>...........] - ETA: 1:06 - reward: -0.6271 96654/100000: episode: 210, duration: 2.454s, episode steps: 123, steps per second: 50, episode reward: -52.696, mean reward: -0.428 [-2.219, 98.935], mean action: 1.366 [0.000, 2.000], loss: 0.602445, mae: 21.175621, mean_q: -15.009881\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 6842/10000 [===================>..........] - ETA: 1:03 - reward: -0.6305 96843/100000: episode: 211, duration: 3.863s, episode steps: 189, steps per second: 49, episode reward: -141.589, mean reward: -0.749 [-12.083, 98.518], mean action: 1.222 [0.000, 2.000], loss: 1.166623, mae: 20.728571, mean_q: -15.902270\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 7042/10000 [====================>.........] - ETA: 59s - reward: -0.6185 97042/100000: episode: 212, duration: 3.955s, episode steps: 199, steps per second: 50, episode reward: -140.084, mean reward: -0.704 [-5.138, 98.573], mean action: 1.080 [0.000, 2.000], loss: 0.597860, mae: 20.981760, mean_q: -14.836838\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 7227/10000 [====================>.........] - ETA: 55s - reward: -0.6344 97228/100000: episode: 213, duration: 3.680s, episode steps: 186, steps per second: 51, episode reward: -130.722, mean reward: -0.703 [-2.442, 98.669], mean action: 1.161 [0.000, 2.000], loss: 0.766860, mae: 20.642017, mean_q: -14.589043\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 7348/10000 [=====================>........] - ETA: 53s - reward: -0.6305 97349/100000: episode: 214, duration: 2.442s, episode steps: 121, steps per second: 50, episode reward: -48.231, mean reward: -0.399 [-2.207, 98.814], mean action: 1.306 [0.000, 2.000], loss: 0.780573, mae: 20.793512, mean_q: -15.337951\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 7553/10000 [=====================>........] - ETA: 48s - reward: -0.6326 97554/100000: episode: 215, duration: 4.167s, episode steps: 205, steps per second: 49, episode reward: -145.582, mean reward: -0.710 [-2.434, 98.617], mean action: 1.180 [0.000, 2.000], loss: 0.679389, mae: 20.924116, mean_q: -15.274919\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 7729/10000 [======================>.......] - ETA: 45s - reward: -0.6340 97731/100000: episode: 216, duration: 3.497s, episode steps: 177, steps per second: 51, episode reward: -123.579, mean reward: -0.698 [-7.410, 98.556], mean action: 1.141 [0.000, 2.000], loss: 0.657442, mae: 20.399511, mean_q: -14.908364\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 7903/10000 [======================>.......] - ETA: 41s - reward: -0.6350 97904/100000: episode: 217, duration: 3.713s, episode steps: 173, steps per second: 47, episode reward: -116.231, mean reward: -0.672 [-2.371, 98.668], mean action: 1.087 [0.000, 2.000], loss: 0.596051, mae: 20.377222, mean_q: -14.382172\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8067/10000 [=======================>......] - ETA: 38s - reward: -0.6355 98068/100000: episode: 218, duration: 3.270s, episode steps: 164, steps per second: 50, episode reward: -108.113, mean reward: -0.659 [-8.730, 98.587], mean action: 1.183 [0.000, 2.000], loss: 0.675564, mae: 20.666918, mean_q: -13.909894\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8234/10000 [=======================>......] - ETA: 35s - reward: -0.6348 98236/100000: episode: 219, duration: 3.467s, episode steps: 168, steps per second: 48, episode reward: -102.172, mean reward: -0.608 [-2.363, 98.401], mean action: 1.232 [0.000, 2.000], loss: 0.566469, mae: 20.623396, mean_q: -14.211525\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8412/10000 [========================>.....] - ETA: 31s - reward: -0.6233 98412/100000: episode: 220, duration: 3.626s, episode steps: 176, steps per second: 49, episode reward: -113.416, mean reward: -0.644 [-2.396, 98.640], mean action: 1.199 [0.000, 2.000], loss: 0.751655, mae: 20.546198, mean_q: -14.496701\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8639/10000 [========================>.....] - ETA: 27s - reward: -0.6407 98640/100000: episode: 221, duration: 4.507s, episode steps: 228, steps per second: 51, episode reward: -193.249, mean reward: -0.848 [-13.330, 98.591], mean action: 1.250 [0.000, 2.000], loss: 0.648164, mae: 20.326019, mean_q: -14.317416\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8836/10000 [=========================>....] - ETA: 23s - reward: -0.6444 98837/100000: episode: 222, duration: 3.917s, episode steps: 197, steps per second: 50, episode reward: -158.804, mean reward: -0.806 [-13.337, 98.575], mean action: 1.264 [0.000, 2.000], loss: 0.831711, mae: 20.416552, mean_q: -14.555822\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 8959/10000 [=========================>....] - ETA: 20s - reward: -0.6413 98961/100000: episode: 223, duration: 2.591s, episode steps: 124, steps per second: 48, episode reward: -52.150, mean reward: -0.421 [-2.164, 98.883], mean action: 1.411 [0.000, 2.000], loss: 0.690743, mae: 20.501568, mean_q: -14.078890\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9121/10000 [==========================>...] - ETA: 17s - reward: -0.6299 99121/100000: episode: 224, duration: 3.167s, episode steps: 160, steps per second: 51, episode reward: -98.268, mean reward: -0.614 [-2.385, 98.694], mean action: 1.031 [0.000, 2.000], loss: 1.068773, mae: 20.289997, mean_q: -14.305069\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9288/10000 [==========================>...] - ETA: 14s - reward: -0.6303 99288/100000: episode: 225, duration: 3.318s, episode steps: 167, steps per second: 50, episode reward: -108.465, mean reward: -0.649 [-2.389, 98.590], mean action: 1.060 [0.000, 2.000], loss: 0.607854, mae: 20.348446, mean_q: -13.156289\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9440/10000 [===========================>..] - ETA: 11s - reward: -0.6304 99440/100000: episode: 226, duration: 3.124s, episode steps: 152, steps per second: 49, episode reward: -96.446, mean reward: -0.635 [-9.028, 98.447], mean action: 1.184 [0.000, 2.000], loss: 0.711408, mae: 20.236536, mean_q: -13.132506\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9595/10000 [===========================>..] - ETA: 8s - reward: -0.6403 99597/100000: episode: 227, duration: 3.086s, episode steps: 157, steps per second: 51, episode reward: -96.193, mean reward: -0.613 [-2.352, 98.774], mean action: 1.115 [0.000, 2.000], loss: 0.777757, mae: 20.626419, mean_q: -13.970834\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9750/10000 [============================>.] - ETA: 5s - reward: -0.6406 99751/100000: episode: 228, duration: 3.104s, episode steps: 154, steps per second: 50, episode reward: -100.457, mean reward: -0.652 [-8.603, 98.300], mean action: 1.013 [0.000, 2.000], loss: 0.728922, mae: 20.830853, mean_q: -14.001589\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 9910/10000 [============================>.] - ETA: 1s - reward: -0.6401 99911/100000: episode: 229, duration: 3.198s, episode steps: 160, steps per second: 50, episode reward: -97.863, mean reward: -0.612 [-2.404, 98.687], mean action: 1.144 [0.000, 2.000], loss: 0.680339, mae: 20.584442, mean_q: -13.809141\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "10000/10000 [==============================] - 201s 20ms/step - reward: -0.6343\n",
      "done, took 1989.735 seconds\n",
      "done, took 1989.735 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1561cc16eb0>"
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env,nb_steps=100000,visualize=False,verbose = 1, callbacks=callbacks, nb_max_episode_steps=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [41]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m scores \u001B[38;5;241m=\u001B[39m \u001B[43mdqn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnb_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvisualize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\rl\\core.py:341\u001B[0m, in \u001B[0;36mAgent.test\u001B[1;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001B[0m\n\u001B[0;32m    338\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[0;32m    339\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_step_begin(episode_step)\n\u001B[1;32m--> 341\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    342\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    343\u001B[0m         action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessor\u001B[38;5;241m.\u001B[39mprocess_action(action)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\rl\\agents\\dqn.py:224\u001B[0m, in \u001B[0;36mDQNAgent.forward\u001B[1;34m(self, observation)\u001B[0m\n\u001B[0;32m    221\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, observation):\n\u001B[0;32m    222\u001B[0m     \u001B[38;5;66;03m# Select an action.\u001B[39;00m\n\u001B[0;32m    223\u001B[0m     state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmemory\u001B[38;5;241m.\u001B[39mget_recent_state(observation)\n\u001B[1;32m--> 224\u001B[0m     q_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_q_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    225\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[0;32m    226\u001B[0m         action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39mselect_action(q_values\u001B[38;5;241m=\u001B[39mq_values)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\rl\\agents\\dqn.py:68\u001B[0m, in \u001B[0;36mAbstractDQNAgent.compute_q_values\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_q_values\u001B[39m(\u001B[38;5;28mself\u001B[39m, state):\n\u001B[1;32m---> 68\u001B[0m     q_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_batch_q_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m q_values\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m==\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnb_actions,)\n\u001B[0;32m     70\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m q_values\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\rl\\agents\\dqn.py:63\u001B[0m, in \u001B[0;36mAbstractDQNAgent.compute_batch_q_values\u001B[1;34m(self, state_batch)\u001B[0m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_batch_q_values\u001B[39m(\u001B[38;5;28mself\u001B[39m, state_batch):\n\u001B[0;32m     62\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_state_batch(state_batch)\n\u001B[1;32m---> 63\u001B[0m     q_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_on_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     64\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m q_values\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m==\u001B[39m (\u001B[38;5;28mlen\u001B[39m(state_batch), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnb_actions)\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m q_values\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training_v1.py:1200\u001B[0m, in \u001B[0;36mModel.predict_on_batch\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m   1197\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(inputs)  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[0;32m   1199\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_predict_function()\n\u001B[1;32m-> 1200\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1202\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(outputs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   1203\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m outputs[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend.py:4284\u001B[0m, in \u001B[0;36mGraphExecutionFunction.__call__\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m   4278\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callable_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m feed_arrays \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_feed_arrays \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m   4279\u001B[0m     symbol_vals \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_symbol_vals \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m   4280\u001B[0m     feed_symbols \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_feed_symbols \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfetches \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetches \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m   4281\u001B[0m     session \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session):\n\u001B[0;32m   4282\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001B[1;32m-> 4284\u001B[0m fetched \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_callable_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43marray_vals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4285\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mrun_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_metadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4286\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_fetch_callbacks(fetched[\u001B[38;5;241m-\u001B[39m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetches):])\n\u001B[0;32m   4287\u001B[0m output_structure \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mnest\u001B[38;5;241m.\u001B[39mpack_sequence_as(\n\u001B[0;32m   4288\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_outputs_structure,\n\u001B[0;32m   4289\u001B[0m     fetched[:\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutputs)],\n\u001B[0;32m   4290\u001B[0m     expand_composites\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1480\u001B[0m, in \u001B[0;36mBaseSession._Callable.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1478\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1479\u001B[0m   run_metadata_ptr \u001B[38;5;241m=\u001B[39m tf_session\u001B[38;5;241m.\u001B[39mTF_NewBuffer() \u001B[38;5;28;01mif\u001B[39;00m run_metadata \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1480\u001B[0m   ret \u001B[38;5;241m=\u001B[39m \u001B[43mtf_session\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTF_SessionRunCallable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_session\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_session\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1481\u001B[0m \u001B[43m                                         \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1482\u001B[0m \u001B[43m                                         \u001B[49m\u001B[43mrun_metadata_ptr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1483\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m run_metadata:\n\u001B[0;32m   1484\u001B[0m     proto_data \u001B[38;5;241m=\u001B[39m tf_session\u001B[38;5;241m.\u001B[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=2, visualize=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dqn.save_weights(\"./model/model.h5f\", overwrite=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "dqn.load_weights(\"./model/checkpoint_reward_-17.788243832614583.h5f\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = dqn.test(env, nb_episodes=1, visualize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}