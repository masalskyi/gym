{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "from gym.envs.box2d.lunar_lander import LunarLander\n",
    "\n",
    "class LunarLanderModifiedReward(LunarLander):\n",
    "    def step(self, action: int):\n",
    "        new_state, reward, done, info = super().step(action)\n",
    "        if np.abs(new_state[0]) >= 1:\n",
    "            reward -= 1000\n",
    "        if reward == 100:\n",
    "            reward *= 10\n",
    "        if reward == -100:\n",
    "            reward *= 10\n",
    "\n",
    "        return new_state, reward-10, done, info\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# env = LunarLanderModifiedReward()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2' )\n",
    "# env = gym.wrappers.Monitor(env, 'recordings1')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    inputs = layers.Input(shape=(1, states))\n",
    "    x = layers.Dense(256, activation=\"relu\") (inputs)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(128, activation=\"relu\") (x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(128, activation=\"relu\") (x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(64, activation=\"relu\") (x)\n",
    "    x = layers.Flatten()(x)\n",
    "    outputs = layers.Dense(actions, activation=\"linear\")(x)\n",
    "    return Model(inputs, outputs, name=\"LunarLander\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model = build_model(8,4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LunarLander\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1, 8)]            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1, 256)            2304      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1, 256)            0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1, 128)            32896     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1, 128)            0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1, 128)            16512     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1, 128)            0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1, 64)             8256      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,228\n",
      "Trainable params: 60,228\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, policy=policy, memory=memory, nb_actions=actions, nb_steps_warmup=1000, target_model_update=0.1, gamma=0.99)\n",
    "    return dqn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "dqn = build_agent(model, 4)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=[\"mae\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from callbacks import TrainEpisodeLogger\n",
    "callbacks = [TrainEpisodeLogger(\"./model/\", episodes_for_averaging=20)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000000 steps ...\n",
      "Number of episodes for accumulate = 20\n",
      " 128873/1000000: episode: 1, duration: 0.997s, episode steps: 175, steps per second: 176, episode reward: 1.268, mean reward: 0.007 [-100.000, 17.991], mean action: 1.851 [0.000, 3.000], loss: --, mae: --, mean_q: --\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 128999/1000000: episode: 2, duration: 0.639s, episode steps: 126, steps per second: 197, episode reward: -1.015, mean reward: -0.008 [-100.000, 18.545], mean action: 1.849 [0.000, 3.000], loss: --, mae: --, mean_q: --\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 129999/1000000: episode: 3, duration: 12.569s, episode steps: 1000, steps per second: 80, episode reward: 137.911, mean reward: 0.138 [-18.920, 12.880], mean action: 1.332 [0.000, 3.000], loss: 11.605567, mae: 34.588356, mean_q: 46.187516\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 130284/1000000: episode: 4, duration: 7.733s, episode steps: 285, steps per second: 37, episode reward: -80.003, mean reward: -0.281 [-100.000, 10.251], mean action: 1.674 [0.000, 3.000], loss: 15.404477, mae: 35.156059, mean_q: 46.778137\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 131284/1000000: episode: 5, duration: 28.688s, episode steps: 1000, steps per second: 35, episode reward: 118.538, mean reward: 0.119 [-20.100, 22.915], mean action: 1.409 [0.000, 3.000], loss: 14.385666, mae: 37.153461, mean_q: 49.676079\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 132284/1000000: episode: 6, duration: 28.427s, episode steps: 1000, steps per second: 35, episode reward: 99.035, mean reward: 0.099 [-20.142, 22.644], mean action: 1.426 [0.000, 3.000], loss: 17.455465, mae: 36.732841, mean_q: 49.042187\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 132453/1000000: episode: 7, duration: 4.606s, episode steps: 169, steps per second: 37, episode reward: -59.078, mean reward: -0.350 [-100.000, 11.436], mean action: 1.722 [0.000, 3.000], loss: 13.595827, mae: 36.145554, mean_q: 48.212002\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 132628/1000000: episode: 8, duration: 4.749s, episode steps: 175, steps per second: 37, episode reward: -34.795, mean reward: -0.199 [-100.000, 12.000], mean action: 1.714 [0.000, 3.000], loss: 19.264469, mae: 35.550228, mean_q: 47.262478\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 132966/1000000: episode: 9, duration: 9.183s, episode steps: 338, steps per second: 37, episode reward: -232.005, mean reward: -0.686 [-100.000, 11.462], mean action: 1.716 [0.000, 3.000], loss: 11.929384, mae: 35.504009, mean_q: 47.048519\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 133966/1000000: episode: 10, duration: 28.475s, episode steps: 1000, steps per second: 35, episode reward: 54.299, mean reward: 0.054 [-22.850, 36.816], mean action: 1.509 [0.000, 3.000], loss: 14.189075, mae: 35.782478, mean_q: 47.660027\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 134203/1000000: episode: 11, duration: 6.454s, episode steps: 237, steps per second: 37, episode reward: -19.884, mean reward: -0.084 [-100.000, 10.914], mean action: 1.869 [0.000, 3.000], loss: 18.413237, mae: 35.914032, mean_q: 47.740227\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 134486/1000000: episode: 12, duration: 7.688s, episode steps: 283, steps per second: 37, episode reward: -180.188, mean reward: -0.637 [-100.000, 4.022], mean action: 1.887 [0.000, 3.000], loss: 17.524717, mae: 35.929871, mean_q: 47.804092\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 134983/1000000: episode: 13, duration: 13.618s, episode steps: 497, steps per second: 36, episode reward: -153.862, mean reward: -0.310 [-100.000, 49.691], mean action: 1.594 [0.000, 3.000], loss: 12.690217, mae: 34.788269, mean_q: 46.158829\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 135983/1000000: episode: 14, duration: 27.464s, episode steps: 1000, steps per second: 36, episode reward: 137.432, mean reward: 0.137 [-21.365, 34.943], mean action: 1.312 [0.000, 3.000], loss: 14.672559, mae: 34.397839, mean_q: 45.801720\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 136220/1000000: episode: 15, duration: 6.411s, episode steps: 237, steps per second: 37, episode reward: -67.194, mean reward: -0.284 [-100.000, 10.266], mean action: 1.565 [0.000, 3.000], loss: 15.010833, mae: 34.213398, mean_q: 45.659470\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 137220/1000000: episode: 16, duration: 27.821s, episode steps: 1000, steps per second: 36, episode reward: 128.297, mean reward: 0.128 [-19.857, 14.369], mean action: 1.376 [0.000, 3.000], loss: 17.546223, mae: 32.984592, mean_q: 43.847984\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 137409/1000000: episode: 17, duration: 5.116s, episode steps: 189, steps per second: 37, episode reward: -60.381, mean reward: -0.319 [-100.000, 10.177], mean action: 1.778 [0.000, 3.000], loss: 10.719092, mae: 31.995247, mean_q: 42.420769\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 138409/1000000: episode: 18, duration: 28.072s, episode steps: 1000, steps per second: 36, episode reward: 50.185, mean reward: 0.050 [-21.991, 32.735], mean action: 1.296 [0.000, 3.000], loss: 14.287765, mae: 30.944899, mean_q: 41.148453\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 139409/1000000: episode: 19, duration: 27.619s, episode steps: 1000, steps per second: 36, episode reward: 145.900, mean reward: 0.146 [-19.629, 23.097], mean action: 1.452 [0.000, 3.000], loss: 15.275551, mae: 31.122107, mean_q: 41.196617\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 140409/1000000: episode: 20, duration: 27.892s, episode steps: 1000, steps per second: 36, episode reward: 126.858, mean reward: 0.127 [-23.715, 22.911], mean action: 1.350 [0.000, 3.000], loss: 10.130656, mae: 30.062361, mean_q: 40.172501\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 140783/1000000: episode: 21, duration: 10.240s, episode steps: 374, steps per second: 37, episode reward: -234.069, mean reward: -0.626 [-100.000, 16.374], mean action: 1.537 [0.000, 3.000], loss: 10.510289, mae: 30.033939, mean_q: 40.249706\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 140960/1000000: episode: 22, duration: 4.777s, episode steps: 177, steps per second: 37, episode reward: -43.482, mean reward: -0.246 [-100.000, 21.759], mean action: 1.593 [0.000, 3.000], loss: 13.410953, mae: 30.835497, mean_q: 41.320904\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 141960/1000000: episode: 23, duration: 27.848s, episode steps: 1000, steps per second: 36, episode reward: 109.941, mean reward: 0.110 [-21.351, 23.218], mean action: 1.365 [0.000, 3.000], loss: 12.734785, mae: 31.271353, mean_q: 41.767673\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 142960/1000000: episode: 24, duration: 27.794s, episode steps: 1000, steps per second: 36, episode reward: 89.935, mean reward: 0.090 [-21.762, 24.973], mean action: 1.392 [0.000, 3.000], loss: 12.051570, mae: 29.401966, mean_q: 39.134964\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 143136/1000000: episode: 25, duration: 4.732s, episode steps: 176, steps per second: 37, episode reward: -76.310, mean reward: -0.434 [-100.000, 16.323], mean action: 1.699 [0.000, 3.000], loss: 9.575835, mae: 28.370533, mean_q: 37.704746\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 143289/1000000: episode: 26, duration: 4.145s, episode steps: 153, steps per second: 37, episode reward: -259.349, mean reward: -1.695 [-100.000, 6.574], mean action: 2.065 [0.000, 3.000], loss: 13.162802, mae: 28.280022, mean_q: 37.589558\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 143463/1000000: episode: 27, duration: 4.715s, episode steps: 174, steps per second: 37, episode reward: -257.959, mean reward: -1.483 [-100.000, 5.771], mean action: 1.879 [0.000, 3.000], loss: 12.879714, mae: 28.280554, mean_q: 37.393970\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 143655/1000000: episode: 28, duration: 5.221s, episode steps: 192, steps per second: 37, episode reward: 32.343, mean reward: 0.168 [-100.000, 13.977], mean action: 1.682 [0.000, 3.000], loss: 9.778427, mae: 28.089560, mean_q: 37.407536\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 143800/1000000: episode: 29, duration: 3.916s, episode steps: 145, steps per second: 37, episode reward: -90.846, mean reward: -0.627 [-100.000, 20.640], mean action: 1.703 [0.000, 3.000], loss: 14.875539, mae: 28.042503, mean_q: 36.987549\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 144800/1000000: episode: 30, duration: 27.848s, episode steps: 1000, steps per second: 36, episode reward: 80.639, mean reward: 0.081 [-21.206, 23.124], mean action: 1.281 [0.000, 3.000], loss: 10.833345, mae: 28.910221, mean_q: 38.437733\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 145800/1000000: episode: 31, duration: 27.395s, episode steps: 1000, steps per second: 37, episode reward: 169.985, mean reward: 0.170 [-20.790, 23.477], mean action: 1.311 [0.000, 3.000], loss: 12.044614, mae: 27.684746, mean_q: 36.842010\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 145949/1000000: episode: 32, duration: 4.027s, episode steps: 149, steps per second: 37, episode reward: -4.044, mean reward: -0.027 [-100.000, 12.914], mean action: 1.711 [0.000, 3.000], loss: 12.420730, mae: 27.248878, mean_q: 36.166832\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 146106/1000000: episode: 33, duration: 4.254s, episode steps: 157, steps per second: 37, episode reward: -73.598, mean reward: -0.469 [-100.000, 12.008], mean action: 1.732 [0.000, 3.000], loss: 6.795862, mae: 27.409565, mean_q: 36.124344\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 147106/1000000: episode: 34, duration: 27.909s, episode steps: 1000, steps per second: 36, episode reward: 172.313, mean reward: 0.172 [-21.049, 22.487], mean action: 1.268 [0.000, 3.000], loss: 11.323113, mae: 27.581661, mean_q: 36.745319\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 147569/1000000: episode: 35, duration: 12.546s, episode steps: 463, steps per second: 37, episode reward: -173.487, mean reward: -0.375 [-100.000, 13.221], mean action: 1.454 [0.000, 3.000], loss: 12.501113, mae: 27.731457, mean_q: 36.929840\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 147829/1000000: episode: 36, duration: 7.037s, episode steps: 260, steps per second: 37, episode reward: -214.318, mean reward: -0.824 [-100.000, 24.766], mean action: 1.731 [0.000, 3.000], loss: 12.140963, mae: 27.657974, mean_q: 36.984734\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 147983/1000000: episode: 37, duration: 4.180s, episode steps: 154, steps per second: 37, episode reward: -162.021, mean reward: -1.052 [-100.000, 8.597], mean action: 1.792 [0.000, 3.000], loss: 21.030628, mae: 27.625076, mean_q: 36.760357\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 148212/1000000: episode: 38, duration: 6.176s, episode steps: 229, steps per second: 37, episode reward: -25.914, mean reward: -0.113 [-100.000, 17.399], mean action: 1.668 [0.000, 3.000], loss: 15.601147, mae: 27.703650, mean_q: 36.801273\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 148419/1000000: episode: 39, duration: 5.601s, episode steps: 207, steps per second: 37, episode reward: -42.845, mean reward: -0.207 [-100.000, 48.064], mean action: 1.623 [0.000, 3.000], loss: 12.301567, mae: 26.947147, mean_q: 35.951763\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 148612/1000000: episode: 40, duration: 5.236s, episode steps: 193, steps per second: 37, episode reward: -16.620, mean reward: -0.086 [-100.000, 17.270], mean action: 1.845 [0.000, 3.000], loss: 14.006322, mae: 26.919113, mean_q: 35.979420\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 148768/1000000: episode: 41, duration: 4.214s, episode steps: 156, steps per second: 37, episode reward: -79.088, mean reward: -0.507 [-100.000, 21.546], mean action: 1.564 [0.000, 3.000], loss: 12.241941, mae: 26.744453, mean_q: 35.539543\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 149015/1000000: episode: 42, duration: 6.683s, episode steps: 247, steps per second: 37, episode reward: -14.053, mean reward: -0.057 [-100.000, 19.608], mean action: 1.737 [0.000, 3.000], loss: 11.660819, mae: 26.983177, mean_q: 36.006943\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 149209/1000000: episode: 43, duration: 5.248s, episode steps: 194, steps per second: 37, episode reward: -153.495, mean reward: -0.791 [-100.000, 23.249], mean action: 1.789 [0.000, 3.000], loss: 6.609955, mae: 27.204321, mean_q: 36.215668\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 150209/1000000: episode: 44, duration: 28.509s, episode steps: 1000, steps per second: 35, episode reward: -20.459, mean reward: -0.020 [-24.799, 32.265], mean action: 1.659 [0.000, 3.000], loss: 13.610186, mae: 27.962477, mean_q: 37.414711\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 150512/1000000: episode: 45, duration: 8.170s, episode steps: 303, steps per second: 37, episode reward: -48.502, mean reward: -0.160 [-100.000, 15.305], mean action: 1.667 [0.000, 3.000], loss: 11.634423, mae: 29.050398, mean_q: 38.950142\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 150687/1000000: episode: 46, duration: 4.736s, episode steps: 175, steps per second: 37, episode reward: -1.231, mean reward: -0.007 [-100.000, 11.189], mean action: 1.737 [0.000, 3.000], loss: 13.259835, mae: 29.001255, mean_q: 38.950157\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 150893/1000000: episode: 47, duration: 5.571s, episode steps: 206, steps per second: 37, episode reward: -379.310, mean reward: -1.841 [-100.000, 64.633], mean action: 1.621 [0.000, 3.000], loss: 10.723374, mae: 28.924129, mean_q: 39.024059\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 151106/1000000: episode: 48, duration: 5.737s, episode steps: 213, steps per second: 37, episode reward: 65.414, mean reward: 0.307 [-100.000, 19.466], mean action: 1.756 [0.000, 3.000], loss: 12.891055, mae: 28.850737, mean_q: 38.634407\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 151416/1000000: episode: 49, duration: 8.416s, episode steps: 310, steps per second: 37, episode reward: -79.680, mean reward: -0.257 [-100.000, 14.197], mean action: 1.713 [0.000, 3.000], loss: 11.764006, mae: 29.208555, mean_q: 39.090233\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 151589/1000000: episode: 50, duration: 4.663s, episode steps: 173, steps per second: 37, episode reward: 14.580, mean reward: 0.084 [-100.000, 16.138], mean action: 1.815 [0.000, 3.000], loss: 13.487794, mae: 29.157650, mean_q: 39.048576\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 151776/1000000: episode: 51, duration: 5.042s, episode steps: 187, steps per second: 37, episode reward: -37.597, mean reward: -0.201 [-100.000, 15.140], mean action: 1.791 [0.000, 3.000], loss: 16.847700, mae: 28.875788, mean_q: 38.625427\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 151944/1000000: episode: 52, duration: 4.515s, episode steps: 168, steps per second: 37, episode reward: 29.241, mean reward: 0.174 [-100.000, 17.101], mean action: 1.738 [0.000, 3.000], loss: 11.964064, mae: 28.437063, mean_q: 37.803719\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 152127/1000000: episode: 53, duration: 4.961s, episode steps: 183, steps per second: 37, episode reward: -81.367, mean reward: -0.445 [-100.000, 14.269], mean action: 1.858 [0.000, 3.000], loss: 15.749465, mae: 28.150988, mean_q: 37.717697\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 152348/1000000: episode: 54, duration: 5.988s, episode steps: 221, steps per second: 37, episode reward: -181.581, mean reward: -0.822 [-100.000, 14.707], mean action: 1.706 [0.000, 3.000], loss: 14.341241, mae: 27.607714, mean_q: 37.005676\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 152554/1000000: episode: 55, duration: 5.536s, episode steps: 206, steps per second: 37, episode reward: -41.132, mean reward: -0.200 [-100.000, 20.036], mean action: 1.917 [0.000, 3.000], loss: 13.084789, mae: 27.079275, mean_q: 35.983139\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 153554/1000000: episode: 56, duration: 28.180s, episode steps: 1000, steps per second: 35, episode reward: 119.678, mean reward: 0.120 [-19.496, 23.407], mean action: 1.306 [0.000, 3.000], loss: 14.300549, mae: 27.305769, mean_q: 36.437595\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 153789/1000000: episode: 57, duration: 6.306s, episode steps: 235, steps per second: 37, episode reward: 25.846, mean reward: 0.110 [-100.000, 54.245], mean action: 1.838 [0.000, 3.000], loss: 10.333731, mae: 26.045767, mean_q: 34.752090\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 154030/1000000: episode: 58, duration: 6.531s, episode steps: 241, steps per second: 37, episode reward: -74.519, mean reward: -0.309 [-100.000, 9.665], mean action: 1.784 [0.000, 3.000], loss: 12.259141, mae: 25.853592, mean_q: 34.560707\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 154213/1000000: episode: 59, duration: 4.944s, episode steps: 183, steps per second: 37, episode reward: -21.905, mean reward: -0.120 [-100.000, 19.488], mean action: 1.781 [0.000, 3.000], loss: 9.917727, mae: 26.089828, mean_q: 34.746449\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 154418/1000000: episode: 60, duration: 5.528s, episode steps: 205, steps per second: 37, episode reward: -102.232, mean reward: -0.499 [-100.000, 12.400], mean action: 1.932 [0.000, 3.000], loss: 13.261636, mae: 26.313234, mean_q: 35.134872\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 154609/1000000: episode: 61, duration: 5.132s, episode steps: 191, steps per second: 37, episode reward: 27.643, mean reward: 0.145 [-100.000, 14.448], mean action: 1.613 [0.000, 3.000], loss: 11.552959, mae: 27.075354, mean_q: 35.959312\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 155019/1000000: episode: 62, duration: 11.129s, episode steps: 410, steps per second: 37, episode reward: -198.666, mean reward: -0.485 [-100.000, 18.067], mean action: 1.798 [0.000, 3.000], loss: 14.284449, mae: 26.720530, mean_q: 35.601887\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 156019/1000000: episode: 63, duration: 27.281s, episode steps: 1000, steps per second: 37, episode reward: 149.018, mean reward: 0.149 [-19.157, 25.183], mean action: 1.351 [0.000, 3.000], loss: 12.377187, mae: 26.311541, mean_q: 34.879047\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 157019/1000000: episode: 64, duration: 28.116s, episode steps: 1000, steps per second: 36, episode reward: 128.107, mean reward: 0.128 [-18.684, 16.063], mean action: 1.349 [0.000, 3.000], loss: 14.613375, mae: 25.641590, mean_q: 34.004025\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 157191/1000000: episode: 65, duration: 4.655s, episode steps: 172, steps per second: 37, episode reward: -64.338, mean reward: -0.374 [-100.000, 15.381], mean action: 1.494 [0.000, 3.000], loss: 8.857225, mae: 25.979706, mean_q: 34.359592\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 157310/1000000: episode: 66, duration: 3.233s, episode steps: 119, steps per second: 37, episode reward: -13.881, mean reward: -0.117 [-100.000, 22.460], mean action: 1.689 [0.000, 3.000], loss: 9.305545, mae: 26.384361, mean_q: 34.891308\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 157585/1000000: episode: 67, duration: 7.497s, episode steps: 275, steps per second: 37, episode reward: -109.073, mean reward: -0.397 [-100.000, 12.182], mean action: 1.833 [0.000, 3.000], loss: 13.311784, mae: 26.527117, mean_q: 35.309280\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 157809/1000000: episode: 68, duration: 6.015s, episode steps: 224, steps per second: 37, episode reward: 7.502, mean reward: 0.033 [-100.000, 16.528], mean action: 1.714 [0.000, 3.000], loss: 11.684784, mae: 26.350382, mean_q: 34.939556\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 158059/1000000: episode: 69, duration: 6.758s, episode steps: 250, steps per second: 37, episode reward: -86.665, mean reward: -0.347 [-100.000, 14.848], mean action: 1.676 [0.000, 3.000], loss: 11.816164, mae: 26.082933, mean_q: 34.623886\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 159059/1000000: episode: 70, duration: 27.429s, episode steps: 1000, steps per second: 36, episode reward: 101.029, mean reward: 0.101 [-20.746, 22.347], mean action: 1.392 [0.000, 3.000], loss: 12.969482, mae: 25.164331, mean_q: 33.440842\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 159281/1000000: episode: 71, duration: 5.995s, episode steps: 222, steps per second: 37, episode reward: -0.159, mean reward: -0.001 [-100.000, 21.582], mean action: 1.869 [0.000, 3.000], loss: 17.700108, mae: 24.937433, mean_q: 33.093086\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 159460/1000000: episode: 72, duration: 4.871s, episode steps: 179, steps per second: 37, episode reward: -75.184, mean reward: -0.420 [-100.000, 7.371], mean action: 1.777 [0.000, 3.000], loss: 13.151359, mae: 24.984110, mean_q: 33.298321\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 160460/1000000: episode: 73, duration: 28.090s, episode steps: 1000, steps per second: 36, episode reward: 30.755, mean reward: 0.031 [-19.993, 23.260], mean action: 1.421 [0.000, 3.000], loss: 13.384629, mae: 25.088484, mean_q: 33.613613\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 160599/1000000: episode: 74, duration: 3.766s, episode steps: 139, steps per second: 37, episode reward: -57.196, mean reward: -0.411 [-100.000, 11.601], mean action: 1.655 [0.000, 3.000], loss: 11.102481, mae: 25.466013, mean_q: 34.391846\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 160857/1000000: episode: 75, duration: 6.962s, episode steps: 258, steps per second: 37, episode reward: -12.540, mean reward: -0.049 [-100.000, 14.383], mean action: 1.798 [0.000, 3.000], loss: 9.903660, mae: 25.757048, mean_q: 34.589798\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 161051/1000000: episode: 76, duration: 5.223s, episode steps: 194, steps per second: 37, episode reward: -23.496, mean reward: -0.121 [-100.000, 21.525], mean action: 1.727 [0.000, 3.000], loss: 9.472255, mae: 26.140892, mean_q: 35.052059\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 162051/1000000: episode: 77, duration: 27.850s, episode steps: 1000, steps per second: 36, episode reward: 121.461, mean reward: 0.121 [-20.032, 22.701], mean action: 1.411 [0.000, 3.000], loss: 12.274997, mae: 26.154274, mean_q: 35.124481\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 162281/1000000: episode: 78, duration: 6.186s, episode steps: 230, steps per second: 37, episode reward: -130.225, mean reward: -0.566 [-100.000, 34.760], mean action: 1.617 [0.000, 3.000], loss: 13.861414, mae: 26.368196, mean_q: 35.252785\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 163281/1000000: episode: 79, duration: 28.282s, episode steps: 1000, steps per second: 35, episode reward: 101.209, mean reward: 0.101 [-20.600, 23.627], mean action: 1.364 [0.000, 3.000], loss: 11.064306, mae: 26.678020, mean_q: 35.721962\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 163601/1000000: episode: 80, duration: 8.698s, episode steps: 320, steps per second: 37, episode reward: -62.639, mean reward: -0.196 [-100.000, 11.143], mean action: 1.753 [0.000, 3.000], loss: 14.915182, mae: 26.887280, mean_q: 36.044102\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 163892/1000000: episode: 81, duration: 7.961s, episode steps: 291, steps per second: 37, episode reward: -252.932, mean reward: -0.869 [-100.000, 39.449], mean action: 1.629 [0.000, 3.000], loss: 18.289865, mae: 26.853115, mean_q: 35.961395\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 164154/1000000: episode: 82, duration: 7.151s, episode steps: 262, steps per second: 37, episode reward: -6.933, mean reward: -0.026 [-100.000, 20.475], mean action: 1.672 [0.000, 3.000], loss: 12.920654, mae: 26.486126, mean_q: 35.258923\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 164449/1000000: episode: 83, duration: 8.047s, episode steps: 295, steps per second: 37, episode reward: -25.305, mean reward: -0.086 [-100.000, 9.957], mean action: 1.817 [0.000, 3.000], loss: 11.336271, mae: 26.515886, mean_q: 35.551399\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 165449/1000000: episode: 84, duration: 27.471s, episode steps: 1000, steps per second: 36, episode reward: 57.610, mean reward: 0.058 [-20.592, 23.526], mean action: 1.286 [0.000, 3.000], loss: 10.941259, mae: 26.612196, mean_q: 35.749722\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 165833/1000000: episode: 85, duration: 10.365s, episode steps: 384, steps per second: 37, episode reward: 53.372, mean reward: 0.139 [-100.000, 9.043], mean action: 1.677 [0.000, 3.000], loss: 10.424610, mae: 27.174784, mean_q: 36.380428\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 165924/1000000: episode: 86, duration: 2.443s, episode steps: 91, steps per second: 37, episode reward: -76.579, mean reward: -0.842 [-100.000, 9.469], mean action: 1.571 [0.000, 3.000], loss: 13.599074, mae: 26.424778, mean_q: 35.519325\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 166257/1000000: episode: 87, duration: 9.042s, episode steps: 333, steps per second: 37, episode reward: -92.903, mean reward: -0.279 [-100.000, 8.474], mean action: 1.697 [0.000, 3.000], loss: 11.200153, mae: 26.848764, mean_q: 35.900364\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 166469/1000000: episode: 88, duration: 5.719s, episode steps: 212, steps per second: 37, episode reward: -193.834, mean reward: -0.914 [-100.000, 51.135], mean action: 1.604 [0.000, 3.000], loss: 12.902699, mae: 26.828709, mean_q: 35.964737\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 166619/1000000: episode: 89, duration: 4.039s, episode steps: 150, steps per second: 37, episode reward: -115.642, mean reward: -0.771 [-100.000, 7.188], mean action: 1.653 [0.000, 3.000], loss: 12.680566, mae: 26.546093, mean_q: 35.451572\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 166913/1000000: episode: 90, duration: 7.977s, episode steps: 294, steps per second: 37, episode reward: -28.825, mean reward: -0.098 [-100.000, 20.121], mean action: 1.707 [0.000, 3.000], loss: 12.445988, mae: 26.308453, mean_q: 35.064159\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 167114/1000000: episode: 91, duration: 5.457s, episode steps: 201, steps per second: 37, episode reward: -56.448, mean reward: -0.281 [-100.000, 13.887], mean action: 1.736 [0.000, 3.000], loss: 7.615860, mae: 26.218170, mean_q: 34.960453\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 168114/1000000: episode: 92, duration: 27.522s, episode steps: 1000, steps per second: 36, episode reward: 47.695, mean reward: 0.048 [-20.133, 13.701], mean action: 1.404 [0.000, 3.000], loss: 12.037016, mae: 26.658176, mean_q: 35.556614\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 168278/1000000: episode: 93, duration: 4.419s, episode steps: 164, steps per second: 37, episode reward: 20.203, mean reward: 0.123 [-100.000, 12.107], mean action: 1.768 [0.000, 3.000], loss: 9.761211, mae: 26.866072, mean_q: 36.120003\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 169278/1000000: episode: 94, duration: 27.410s, episode steps: 1000, steps per second: 36, episode reward: 82.008, mean reward: 0.082 [-18.784, 21.550], mean action: 1.266 [0.000, 3.000], loss: 11.815368, mae: 26.583309, mean_q: 35.656876\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 170278/1000000: episode: 95, duration: 28.296s, episode steps: 1000, steps per second: 35, episode reward: 124.532, mean reward: 0.125 [-19.857, 22.631], mean action: 1.413 [0.000, 3.000], loss: 10.947244, mae: 26.631613, mean_q: 35.873425\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 170482/1000000: episode: 96, duration: 5.515s, episode steps: 204, steps per second: 37, episode reward: -49.103, mean reward: -0.241 [-100.000, 19.196], mean action: 1.809 [0.000, 3.000], loss: 13.364034, mae: 26.720291, mean_q: 35.927460\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 170643/1000000: episode: 97, duration: 4.377s, episode steps: 161, steps per second: 37, episode reward: -22.365, mean reward: -0.139 [-100.000, 13.277], mean action: 1.745 [0.000, 3.000], loss: 14.871970, mae: 26.212248, mean_q: 35.118965\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 170850/1000000: episode: 98, duration: 5.587s, episode steps: 207, steps per second: 37, episode reward: -194.492, mean reward: -0.940 [-100.000, 5.788], mean action: 1.700 [0.000, 3.000], loss: 8.905554, mae: 25.987919, mean_q: 34.773365\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 171068/1000000: episode: 99, duration: 5.892s, episode steps: 218, steps per second: 37, episode reward: -11.950, mean reward: -0.055 [-100.000, 20.170], mean action: 1.697 [0.000, 3.000], loss: 12.527761, mae: 25.671879, mean_q: 34.314583\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 172068/1000000: episode: 100, duration: 27.612s, episode steps: 1000, steps per second: 36, episode reward: 140.037, mean reward: 0.140 [-19.415, 47.050], mean action: 1.315 [0.000, 3.000], loss: 12.375248, mae: 25.866083, mean_q: 34.545570\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 173068/1000000: episode: 101, duration: 27.627s, episode steps: 1000, steps per second: 36, episode reward: 25.505, mean reward: 0.026 [-21.813, 26.011], mean action: 1.392 [0.000, 3.000], loss: 11.618909, mae: 26.724226, mean_q: 35.996651\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 174068/1000000: episode: 102, duration: 27.816s, episode steps: 1000, steps per second: 36, episode reward: 174.409, mean reward: 0.174 [-19.416, 23.105], mean action: 1.295 [0.000, 3.000], loss: 11.766178, mae: 25.945906, mean_q: 34.751438\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 174411/1000000: episode: 103, duration: 9.324s, episode steps: 343, steps per second: 37, episode reward: -28.281, mean reward: -0.082 [-100.000, 13.295], mean action: 1.697 [0.000, 3.000], loss: 9.755774, mae: 25.076780, mean_q: 33.583282\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 174651/1000000: episode: 104, duration: 6.519s, episode steps: 240, steps per second: 37, episode reward: -107.893, mean reward: -0.450 [-100.000, 21.262], mean action: 1.800 [0.000, 3.000], loss: 8.676733, mae: 25.450142, mean_q: 34.008480\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 174968/1000000: episode: 105, duration: 8.641s, episode steps: 317, steps per second: 37, episode reward: 8.183, mean reward: 0.026 [-100.000, 12.122], mean action: 1.779 [0.000, 3.000], loss: 9.672854, mae: 25.360308, mean_q: 34.167198\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 175270/1000000: episode: 106, duration: 8.168s, episode steps: 302, steps per second: 37, episode reward: -200.759, mean reward: -0.665 [-100.000, 27.235], mean action: 1.652 [0.000, 3.000], loss: 9.371991, mae: 25.389057, mean_q: 34.258465\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 175415/1000000: episode: 107, duration: 3.923s, episode steps: 145, steps per second: 37, episode reward: 1.372, mean reward: 0.009 [-100.000, 17.209], mean action: 1.641 [0.000, 3.000], loss: 12.738873, mae: 25.244951, mean_q: 34.000145\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 176415/1000000: episode: 108, duration: 27.755s, episode steps: 1000, steps per second: 36, episode reward: 158.138, mean reward: 0.158 [-19.602, 20.987], mean action: 1.341 [0.000, 3.000], loss: 12.830613, mae: 25.757534, mean_q: 34.704910\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 177415/1000000: episode: 109, duration: 27.577s, episode steps: 1000, steps per second: 36, episode reward: 170.948, mean reward: 0.171 [-19.184, 14.426], mean action: 1.314 [0.000, 3.000], loss: 11.482763, mae: 25.982141, mean_q: 34.968735\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 178415/1000000: episode: 110, duration: 27.955s, episode steps: 1000, steps per second: 36, episode reward: 127.143, mean reward: 0.127 [-18.726, 22.723], mean action: 1.392 [0.000, 3.000], loss: 12.711681, mae: 25.288582, mean_q: 33.948795\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 178742/1000000: episode: 111, duration: 8.849s, episode steps: 327, steps per second: 37, episode reward: -43.549, mean reward: -0.133 [-100.000, 11.206], mean action: 1.618 [0.000, 3.000], loss: 15.894144, mae: 24.887632, mean_q: 33.503963\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 178918/1000000: episode: 112, duration: 4.738s, episode steps: 176, steps per second: 37, episode reward: -230.429, mean reward: -1.309 [-100.000, 7.483], mean action: 1.722 [0.000, 3.000], loss: 17.439127, mae: 24.575142, mean_q: 33.156445\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 179146/1000000: episode: 113, duration: 6.154s, episode steps: 228, steps per second: 37, episode reward: -217.491, mean reward: -0.954 [-100.000, 13.836], mean action: 1.741 [0.000, 3.000], loss: 10.675513, mae: 23.927139, mean_q: 32.295837\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 179321/1000000: episode: 114, duration: 4.724s, episode steps: 175, steps per second: 37, episode reward: -173.886, mean reward: -0.994 [-100.000, 8.610], mean action: 1.703 [0.000, 3.000], loss: 11.333591, mae: 24.475737, mean_q: 32.796814\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 180321/1000000: episode: 115, duration: 27.665s, episode steps: 1000, steps per second: 36, episode reward: 172.381, mean reward: 0.172 [-20.092, 35.683], mean action: 1.407 [0.000, 3.000], loss: 10.579020, mae: 24.053398, mean_q: 32.297104\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 181321/1000000: episode: 116, duration: 27.990s, episode steps: 1000, steps per second: 36, episode reward: 123.772, mean reward: 0.124 [-19.553, 22.341], mean action: 1.361 [0.000, 3.000], loss: 11.426977, mae: 24.011559, mean_q: 32.415012\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 181572/1000000: episode: 117, duration: 6.818s, episode steps: 251, steps per second: 37, episode reward: -90.633, mean reward: -0.361 [-100.000, 10.953], mean action: 1.809 [0.000, 3.000], loss: 9.860097, mae: 24.436235, mean_q: 32.828178\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 182572/1000000: episode: 118, duration: 27.495s, episode steps: 1000, steps per second: 36, episode reward: 120.460, mean reward: 0.120 [-18.404, 21.521], mean action: 1.420 [0.000, 3.000], loss: 11.305190, mae: 23.966700, mean_q: 32.232334\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 183572/1000000: episode: 119, duration: 28.080s, episode steps: 1000, steps per second: 36, episode reward: 98.519, mean reward: 0.099 [-18.032, 17.325], mean action: 1.386 [0.000, 3.000], loss: 11.967253, mae: 22.691080, mean_q: 30.411333\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 183850/1000000: episode: 120, duration: 7.512s, episode steps: 278, steps per second: 37, episode reward: -160.377, mean reward: -0.577 [-100.000, 16.143], mean action: 1.799 [0.000, 3.000], loss: 9.940010, mae: 23.007704, mean_q: 31.249199\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 184012/1000000: episode: 121, duration: 4.367s, episode steps: 162, steps per second: 37, episode reward: -41.613, mean reward: -0.257 [-100.000, 16.223], mean action: 1.648 [0.000, 3.000], loss: 12.935347, mae: 23.588581, mean_q: 31.945795\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 185012/1000000: episode: 122, duration: 27.938s, episode steps: 1000, steps per second: 36, episode reward: 103.422, mean reward: 0.103 [-19.359, 21.073], mean action: 1.430 [0.000, 3.000], loss: 10.702043, mae: 23.472324, mean_q: 31.576845\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 185384/1000000: episode: 123, duration: 10.149s, episode steps: 372, steps per second: 37, episode reward: -247.396, mean reward: -0.665 [-100.000, 21.944], mean action: 1.866 [0.000, 3.000], loss: 9.760728, mae: 23.673082, mean_q: 31.719316\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 185624/1000000: episode: 124, duration: 6.514s, episode steps: 240, steps per second: 37, episode reward: -79.031, mean reward: -0.329 [-100.000, 6.006], mean action: 1.850 [0.000, 3.000], loss: 8.773346, mae: 23.604580, mean_q: 31.746910\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 185867/1000000: episode: 125, duration: 6.639s, episode steps: 243, steps per second: 37, episode reward: 52.210, mean reward: 0.215 [-100.000, 14.149], mean action: 1.807 [0.000, 3.000], loss: 10.912507, mae: 23.655752, mean_q: 31.745937\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 186213/1000000: episode: 126, duration: 9.409s, episode steps: 346, steps per second: 37, episode reward: -22.684, mean reward: -0.066 [-100.000, 12.181], mean action: 1.665 [0.000, 3.000], loss: 14.446328, mae: 23.458477, mean_q: 31.653067\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 187213/1000000: episode: 127, duration: 27.746s, episode steps: 1000, steps per second: 36, episode reward: 102.591, mean reward: 0.103 [-23.160, 23.895], mean action: 1.331 [0.000, 3.000], loss: 8.944754, mae: 23.012808, mean_q: 31.043095\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 187337/1000000: episode: 128, duration: 3.350s, episode steps: 124, steps per second: 37, episode reward: 4.877, mean reward: 0.039 [-100.000, 15.663], mean action: 1.823 [0.000, 3.000], loss: 12.614251, mae: 24.467541, mean_q: 33.008278\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 188337/1000000: episode: 129, duration: 27.887s, episode steps: 1000, steps per second: 36, episode reward: 123.095, mean reward: 0.123 [-20.484, 23.139], mean action: 1.417 [0.000, 3.000], loss: 10.100533, mae: 24.030724, mean_q: 32.305115\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 189337/1000000: episode: 130, duration: 27.981s, episode steps: 1000, steps per second: 36, episode reward: 146.407, mean reward: 0.146 [-19.928, 20.780], mean action: 1.423 [0.000, 3.000], loss: 10.032121, mae: 23.911724, mean_q: 32.174870\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 190337/1000000: episode: 131, duration: 28.108s, episode steps: 1000, steps per second: 36, episode reward: 99.547, mean reward: 0.100 [-20.237, 22.632], mean action: 1.399 [0.000, 3.000], loss: 8.583921, mae: 24.422773, mean_q: 32.807030\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 191337/1000000: episode: 132, duration: 27.283s, episode steps: 1000, steps per second: 37, episode reward: 171.046, mean reward: 0.171 [-21.875, 23.203], mean action: 1.377 [0.000, 3.000], loss: 11.663857, mae: 24.496683, mean_q: 32.892040\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 191551/1000000: episode: 133, duration: 5.775s, episode steps: 214, steps per second: 37, episode reward: -158.633, mean reward: -0.741 [-100.000, 41.032], mean action: 1.692 [0.000, 3.000], loss: 8.396682, mae: 23.626841, mean_q: 31.769308\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 192551/1000000: episode: 134, duration: 28.477s, episode steps: 1000, steps per second: 35, episode reward: 114.774, mean reward: 0.115 [-19.983, 22.759], mean action: 1.337 [0.000, 3.000], loss: 10.043310, mae: 23.713390, mean_q: 31.994242\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 193551/1000000: episode: 135, duration: 27.584s, episode steps: 1000, steps per second: 36, episode reward: 170.327, mean reward: 0.170 [-17.694, 15.364], mean action: 1.345 [0.000, 3.000], loss: 8.752301, mae: 23.672094, mean_q: 31.898008\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 194551/1000000: episode: 136, duration: 27.252s, episode steps: 1000, steps per second: 37, episode reward: 102.425, mean reward: 0.102 [-18.388, 23.178], mean action: 1.302 [0.000, 3.000], loss: 8.270215, mae: 24.116922, mean_q: 32.529568\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 195551/1000000: episode: 137, duration: 27.839s, episode steps: 1000, steps per second: 36, episode reward: 73.340, mean reward: 0.073 [-23.666, 30.512], mean action: 1.481 [0.000, 3.000], loss: 9.418948, mae: 25.814453, mean_q: 34.923218\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 196551/1000000: episode: 138, duration: 27.827s, episode steps: 1000, steps per second: 36, episode reward: 40.996, mean reward: 0.041 [-17.739, 22.871], mean action: 1.437 [0.000, 3.000], loss: 9.490611, mae: 25.601770, mean_q: 34.593037\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 197551/1000000: episode: 139, duration: 27.358s, episode steps: 1000, steps per second: 37, episode reward: 190.393, mean reward: 0.190 [-19.128, 23.732], mean action: 1.301 [0.000, 3.000], loss: 10.570939, mae: 24.171993, mean_q: 32.382530\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 197816/1000000: episode: 140, duration: 7.191s, episode steps: 265, steps per second: 37, episode reward: -197.346, mean reward: -0.745 [-100.000, 15.963], mean action: 1.717 [0.000, 3.000], loss: 8.547918, mae: 24.298717, mean_q: 32.745365\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 197990/1000000: episode: 141, duration: 4.689s, episode steps: 174, steps per second: 37, episode reward: -35.451, mean reward: -0.204 [-100.000, 12.887], mean action: 1.897 [0.000, 3.000], loss: 10.655913, mae: 24.372301, mean_q: 32.931797\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 198139/1000000: episode: 142, duration: 4.022s, episode steps: 149, steps per second: 37, episode reward: -17.110, mean reward: -0.115 [-100.000, 15.960], mean action: 1.886 [0.000, 3.000], loss: 8.843627, mae: 24.542185, mean_q: 33.214252\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 199139/1000000: episode: 143, duration: 28.624s, episode steps: 1000, steps per second: 35, episode reward: 99.337, mean reward: 0.099 [-20.447, 25.012], mean action: 1.468 [0.000, 3.000], loss: 9.959688, mae: 24.292540, mean_q: 32.739334\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 200139/1000000: episode: 144, duration: 27.820s, episode steps: 1000, steps per second: 36, episode reward: 87.529, mean reward: 0.088 [-20.140, 22.611], mean action: 1.383 [0.000, 3.000], loss: 10.465641, mae: 23.822161, mean_q: 32.105492\n",
      "The reward is higher than the best one, saving checkpoint weights\n",
      " 200584/1000000: episode: 145, duration: 12.195s, episode steps: 445, steps per second: 36, episode reward: -127.576, mean reward: -0.287 [-100.000, 13.841], mean action: 1.658 [0.000, 3.000], loss: 6.580205, mae: 24.670856, mean_q: 33.253239\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 201584/1000000: episode: 146, duration: 28.156s, episode steps: 1000, steps per second: 36, episode reward: 83.372, mean reward: 0.083 [-24.183, 12.727], mean action: 1.405 [0.000, 3.000], loss: 7.880418, mae: 24.885859, mean_q: 33.571892\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 201788/1000000: episode: 147, duration: 5.500s, episode steps: 204, steps per second: 37, episode reward: -1.411, mean reward: -0.007 [-100.000, 24.159], mean action: 1.750 [0.000, 3.000], loss: 7.282543, mae: 24.794079, mean_q: 33.502926\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 202788/1000000: episode: 148, duration: 27.325s, episode steps: 1000, steps per second: 37, episode reward: 132.843, mean reward: 0.133 [-18.046, 13.245], mean action: 1.419 [0.000, 3.000], loss: 7.580013, mae: 24.842770, mean_q: 33.406212\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 203036/1000000: episode: 149, duration: 6.665s, episode steps: 248, steps per second: 37, episode reward: 29.973, mean reward: 0.121 [-100.000, 52.288], mean action: 1.681 [0.000, 3.000], loss: 10.475473, mae: 25.541166, mean_q: 34.287178\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 203270/1000000: episode: 150, duration: 6.333s, episode steps: 234, steps per second: 37, episode reward: 26.470, mean reward: 0.113 [-100.000, 8.499], mean action: 1.744 [0.000, 3.000], loss: 5.821763, mae: 25.665419, mean_q: 34.495796\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 204270/1000000: episode: 151, duration: 27.832s, episode steps: 1000, steps per second: 36, episode reward: 125.623, mean reward: 0.126 [-19.059, 14.654], mean action: 1.410 [0.000, 3.000], loss: 8.741182, mae: 24.103024, mean_q: 32.575706\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 204520/1000000: episode: 152, duration: 6.755s, episode steps: 250, steps per second: 37, episode reward: -150.673, mean reward: -0.603 [-100.000, 53.314], mean action: 1.884 [0.000, 3.000], loss: 5.397115, mae: 23.237297, mean_q: 31.577320\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 205520/1000000: episode: 153, duration: 27.804s, episode steps: 1000, steps per second: 36, episode reward: 112.407, mean reward: 0.112 [-20.357, 18.036], mean action: 1.420 [0.000, 3.000], loss: 7.783627, mae: 23.561413, mean_q: 31.841032\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 205632/1000000: episode: 154, duration: 3.015s, episode steps: 112, steps per second: 37, episode reward: -47.694, mean reward: -0.426 [-100.000, 12.024], mean action: 1.848 [0.000, 3.000], loss: 4.699124, mae: 23.659607, mean_q: 32.026531\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 206632/1000000: episode: 155, duration: 27.833s, episode steps: 1000, steps per second: 36, episode reward: 56.601, mean reward: 0.057 [-21.545, 23.988], mean action: 1.451 [0.000, 3.000], loss: 8.670488, mae: 23.768631, mean_q: 32.095295\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 207632/1000000: episode: 156, duration: 28.092s, episode steps: 1000, steps per second: 36, episode reward: 99.129, mean reward: 0.099 [-18.959, 23.211], mean action: 1.397 [0.000, 3.000], loss: 7.844826, mae: 23.237942, mean_q: 31.388359\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 207832/1000000: episode: 157, duration: 5.379s, episode steps: 200, steps per second: 37, episode reward: -14.380, mean reward: -0.072 [-100.000, 34.088], mean action: 1.750 [0.000, 3.000], loss: 6.491627, mae: 23.228489, mean_q: 31.400087\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 208067/1000000: episode: 158, duration: 6.359s, episode steps: 235, steps per second: 37, episode reward: -27.780, mean reward: -0.118 [-100.000, 19.724], mean action: 1.821 [0.000, 3.000], loss: 4.673795, mae: 23.411747, mean_q: 31.646852\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 209067/1000000: episode: 159, duration: 28.040s, episode steps: 1000, steps per second: 36, episode reward: 129.819, mean reward: 0.130 [-19.325, 24.349], mean action: 1.365 [0.000, 3.000], loss: 7.056779, mae: 24.150276, mean_q: 32.459377\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 209296/1000000: episode: 160, duration: 6.180s, episode steps: 229, steps per second: 37, episode reward: -73.073, mean reward: -0.319 [-100.000, 8.531], mean action: 1.852 [0.000, 3.000], loss: 7.370689, mae: 24.048471, mean_q: 32.248859\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 210296/1000000: episode: 161, duration: 28.330s, episode steps: 1000, steps per second: 35, episode reward: 128.333, mean reward: 0.128 [-20.899, 22.867], mean action: 1.372 [0.000, 3.000], loss: 6.765071, mae: 23.622395, mean_q: 31.788868\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 210621/1000000: episode: 162, duration: 8.823s, episode steps: 325, steps per second: 37, episode reward: 8.853, mean reward: 0.027 [-100.000, 17.939], mean action: 1.723 [0.000, 3.000], loss: 3.967040, mae: 23.291550, mean_q: 31.528627\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 211621/1000000: episode: 163, duration: 27.691s, episode steps: 1000, steps per second: 36, episode reward: 83.319, mean reward: 0.083 [-21.068, 22.572], mean action: 1.412 [0.000, 3.000], loss: 7.659079, mae: 23.776077, mean_q: 32.229141\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 212621/1000000: episode: 164, duration: 27.711s, episode steps: 1000, steps per second: 36, episode reward: 122.066, mean reward: 0.122 [-19.484, 21.683], mean action: 1.328 [0.000, 3.000], loss: 6.603788, mae: 24.577961, mean_q: 33.304909\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 213621/1000000: episode: 165, duration: 27.277s, episode steps: 1000, steps per second: 37, episode reward: 169.854, mean reward: 0.170 [-18.392, 21.157], mean action: 1.286 [0.000, 3.000], loss: 6.728909, mae: 23.871071, mean_q: 32.230022\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 214621/1000000: episode: 166, duration: 27.780s, episode steps: 1000, steps per second: 36, episode reward: 85.543, mean reward: 0.086 [-19.189, 14.781], mean action: 1.290 [0.000, 3.000], loss: 7.575162, mae: 23.784348, mean_q: 32.214432\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 214739/1000000: episode: 167, duration: 3.187s, episode steps: 118, steps per second: 37, episode reward: -108.647, mean reward: -0.921 [-100.000, 4.577], mean action: 1.602 [0.000, 3.000], loss: 6.528478, mae: 23.729803, mean_q: 32.157780\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 215078/1000000: episode: 168, duration: 9.229s, episode steps: 339, steps per second: 37, episode reward: -188.482, mean reward: -0.556 [-100.000, 21.479], mean action: 1.460 [0.000, 3.000], loss: 3.707910, mae: 23.455122, mean_q: 31.781359\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 215243/1000000: episode: 169, duration: 4.436s, episode steps: 165, steps per second: 37, episode reward: -72.327, mean reward: -0.438 [-100.000, 12.467], mean action: 1.794 [0.000, 3.000], loss: 4.343059, mae: 23.243288, mean_q: 31.366791\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 215475/1000000: episode: 170, duration: 6.285s, episode steps: 232, steps per second: 37, episode reward: -85.483, mean reward: -0.368 [-100.000, 11.082], mean action: 1.810 [0.000, 3.000], loss: 7.545856, mae: 22.989130, mean_q: 30.865948\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 215694/1000000: episode: 171, duration: 5.937s, episode steps: 219, steps per second: 37, episode reward: -52.700, mean reward: -0.241 [-100.000, 20.230], mean action: 1.721 [0.000, 3.000], loss: 8.288068, mae: 22.665701, mean_q: 30.513205\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 215910/1000000: episode: 172, duration: 5.867s, episode steps: 216, steps per second: 37, episode reward: -80.657, mean reward: -0.373 [-100.000, 10.764], mean action: 1.806 [0.000, 3.000], loss: 7.940310, mae: 22.983244, mean_q: 31.208557\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 216113/1000000: episode: 173, duration: 5.450s, episode steps: 203, steps per second: 37, episode reward: -1.770, mean reward: -0.009 [-100.000, 12.353], mean action: 1.773 [0.000, 3.000], loss: 2.946121, mae: 23.392929, mean_q: 31.487089\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 217113/1000000: episode: 174, duration: 27.579s, episode steps: 1000, steps per second: 36, episode reward: 16.682, mean reward: 0.017 [-25.430, 24.196], mean action: 1.443 [0.000, 3.000], loss: 6.030880, mae: 23.856354, mean_q: 32.272938\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 218113/1000000: episode: 175, duration: 27.378s, episode steps: 1000, steps per second: 37, episode reward: 15.991, mean reward: 0.016 [-22.177, 22.842], mean action: 1.421 [0.000, 3.000], loss: 6.633013, mae: 23.112179, mean_q: 31.308792\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 219113/1000000: episode: 176, duration: 28.038s, episode steps: 1000, steps per second: 36, episode reward: 128.573, mean reward: 0.129 [-23.319, 23.750], mean action: 1.402 [0.000, 3.000], loss: 5.865739, mae: 23.081499, mean_q: 31.519081\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 219213/1000000: episode: 177, duration: 2.701s, episode steps: 100, steps per second: 37, episode reward: -74.455, mean reward: -0.745 [-100.000, 8.048], mean action: 1.550 [0.000, 3.000], loss: 6.972635, mae: 23.823303, mean_q: 32.707340\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 219401/1000000: episode: 178, duration: 5.065s, episode steps: 188, steps per second: 37, episode reward: -226.933, mean reward: -1.207 [-100.000, 70.042], mean action: 1.729 [0.000, 3.000], loss: 6.044562, mae: 23.702200, mean_q: 32.468506\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 219854/1000000: episode: 179, duration: 12.353s, episode steps: 453, steps per second: 37, episode reward: 4.075, mean reward: 0.009 [-100.000, 26.203], mean action: 1.775 [0.000, 3.000], loss: 6.942589, mae: 24.726248, mean_q: 33.959061\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 220038/1000000: episode: 180, duration: 4.946s, episode steps: 184, steps per second: 37, episode reward: -22.624, mean reward: -0.123 [-100.000, 45.554], mean action: 1.804 [0.000, 3.000], loss: 4.923196, mae: 24.871025, mean_q: 33.744259\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 220463/1000000: episode: 181, duration: 11.715s, episode steps: 425, steps per second: 36, episode reward: -11.170, mean reward: -0.026 [-100.000, 13.173], mean action: 1.722 [0.000, 3.000], loss: 7.634639, mae: 24.723843, mean_q: 33.562584\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 220680/1000000: episode: 182, duration: 5.922s, episode steps: 217, steps per second: 37, episode reward: -94.615, mean reward: -0.436 [-100.000, 9.512], mean action: 1.747 [0.000, 3.000], loss: 5.377015, mae: 23.982206, mean_q: 32.616741\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 221680/1000000: episode: 183, duration: 27.693s, episode steps: 1000, steps per second: 36, episode reward: 33.692, mean reward: 0.034 [-19.960, 20.027], mean action: 1.372 [0.000, 3.000], loss: 5.308554, mae: 23.718679, mean_q: 32.223358\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 221908/1000000: episode: 184, duration: 6.122s, episode steps: 228, steps per second: 37, episode reward: 11.619, mean reward: 0.051 [-100.000, 16.259], mean action: 1.711 [0.000, 3.000], loss: 3.392504, mae: 24.283312, mean_q: 32.975632\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 222908/1000000: episode: 185, duration: 27.538s, episode steps: 1000, steps per second: 36, episode reward: 128.972, mean reward: 0.129 [-18.305, 24.681], mean action: 1.322 [0.000, 3.000], loss: 7.188408, mae: 23.761887, mean_q: 32.086956\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 223908/1000000: episode: 186, duration: 28.149s, episode steps: 1000, steps per second: 36, episode reward: 91.051, mean reward: 0.091 [-20.545, 20.449], mean action: 1.465 [0.000, 3.000], loss: 7.923886, mae: 22.886383, mean_q: 31.000603\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 224203/1000000: episode: 187, duration: 7.987s, episode steps: 295, steps per second: 37, episode reward: -212.348, mean reward: -0.720 [-100.000, 25.650], mean action: 1.620 [0.000, 3.000], loss: 8.717330, mae: 22.685141, mean_q: 30.510130\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 225203/1000000: episode: 188, duration: 28.268s, episode steps: 1000, steps per second: 35, episode reward: 66.609, mean reward: 0.067 [-21.646, 23.120], mean action: 1.272 [0.000, 3.000], loss: 5.403751, mae: 22.422661, mean_q: 30.255239\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 225415/1000000: episode: 189, duration: 5.747s, episode steps: 212, steps per second: 37, episode reward: -5.435, mean reward: -0.026 [-100.000, 11.071], mean action: 1.689 [0.000, 3.000], loss: 6.940025, mae: 22.923798, mean_q: 31.000257\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 225702/1000000: episode: 190, duration: 7.815s, episode steps: 287, steps per second: 37, episode reward: -54.743, mean reward: -0.191 [-100.000, 17.705], mean action: 1.679 [0.000, 3.000], loss: 6.809651, mae: 23.323757, mean_q: 31.817244\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 226702/1000000: episode: 191, duration: 27.885s, episode steps: 1000, steps per second: 36, episode reward: 104.356, mean reward: 0.104 [-20.580, 17.184], mean action: 1.348 [0.000, 3.000], loss: 5.119971, mae: 23.590689, mean_q: 32.000641\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 226865/1000000: episode: 192, duration: 4.416s, episode steps: 163, steps per second: 37, episode reward: -61.764, mean reward: -0.379 [-100.000, 18.477], mean action: 1.896 [0.000, 3.000], loss: 5.664836, mae: 23.347521, mean_q: 31.594995\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 227004/1000000: episode: 193, duration: 3.837s, episode steps: 139, steps per second: 36, episode reward: -3.778, mean reward: -0.027 [-100.000, 11.104], mean action: 1.734 [0.000, 3.000], loss: 6.620962, mae: 23.456203, mean_q: 31.643078\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 227526/1000000: episode: 194, duration: 14.648s, episode steps: 522, steps per second: 36, episode reward: -250.052, mean reward: -0.479 [-100.000, 20.085], mean action: 1.615 [0.000, 3.000], loss: 5.227866, mae: 23.741199, mean_q: 32.148731\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 227707/1000000: episode: 195, duration: 4.928s, episode steps: 181, steps per second: 37, episode reward: -61.365, mean reward: -0.339 [-100.000, 23.217], mean action: 1.735 [0.000, 3.000], loss: 10.402794, mae: 23.198446, mean_q: 31.379425\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 227933/1000000: episode: 196, duration: 6.019s, episode steps: 226, steps per second: 38, episode reward: -60.631, mean reward: -0.268 [-100.000, 24.457], mean action: 1.597 [0.000, 3.000], loss: 8.662639, mae: 22.938900, mean_q: 30.972834\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 228161/1000000: episode: 197, duration: 6.109s, episode steps: 228, steps per second: 37, episode reward: -183.332, mean reward: -0.804 [-100.000, 30.704], mean action: 1.715 [0.000, 3.000], loss: 8.242662, mae: 22.739738, mean_q: 30.484030\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 229161/1000000: episode: 198, duration: 27.579s, episode steps: 1000, steps per second: 36, episode reward: 45.799, mean reward: 0.046 [-20.472, 23.258], mean action: 1.381 [0.000, 3.000], loss: 7.348906, mae: 22.400927, mean_q: 30.216852\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 229387/1000000: episode: 199, duration: 6.123s, episode steps: 226, steps per second: 37, episode reward: -19.712, mean reward: -0.087 [-100.000, 75.520], mean action: 1.624 [0.000, 3.000], loss: 7.140626, mae: 21.689892, mean_q: 29.238146\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 229539/1000000: episode: 200, duration: 4.077s, episode steps: 152, steps per second: 37, episode reward: -39.091, mean reward: -0.257 [-100.000, 13.418], mean action: 1.849 [0.000, 3.000], loss: 7.221488, mae: 21.781475, mean_q: 29.469318\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 230539/1000000: episode: 201, duration: 27.666s, episode steps: 1000, steps per second: 36, episode reward: 138.171, mean reward: 0.138 [-20.408, 21.169], mean action: 1.350 [0.000, 3.000], loss: 7.891539, mae: 22.107264, mean_q: 29.906244\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 230725/1000000: episode: 202, duration: 4.982s, episode steps: 186, steps per second: 37, episode reward: -31.996, mean reward: -0.172 [-100.000, 23.925], mean action: 1.656 [0.000, 3.000], loss: 8.838040, mae: 22.408413, mean_q: 30.250120\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 230957/1000000: episode: 203, duration: 6.217s, episode steps: 232, steps per second: 37, episode reward: 1.334, mean reward: 0.006 [-100.000, 14.373], mean action: 1.737 [0.000, 3.000], loss: 4.708635, mae: 22.109356, mean_q: 29.953659\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 231128/1000000: episode: 204, duration: 4.591s, episode steps: 171, steps per second: 37, episode reward: -171.549, mean reward: -1.003 [-100.000, 58.566], mean action: 1.912 [0.000, 3.000], loss: 10.151585, mae: 22.366842, mean_q: 30.347656\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 232128/1000000: episode: 205, duration: 27.562s, episode steps: 1000, steps per second: 36, episode reward: 122.495, mean reward: 0.122 [-20.688, 23.882], mean action: 1.371 [0.000, 3.000], loss: 7.503439, mae: 21.870296, mean_q: 29.612909\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 232428/1000000: episode: 206, duration: 8.096s, episode steps: 300, steps per second: 37, episode reward: -31.246, mean reward: -0.104 [-100.000, 19.323], mean action: 1.837 [0.000, 3.000], loss: 8.853271, mae: 22.350626, mean_q: 30.190632\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 233428/1000000: episode: 207, duration: 27.139s, episode steps: 1000, steps per second: 37, episode reward: 84.219, mean reward: 0.084 [-20.656, 27.766], mean action: 1.374 [0.000, 3.000], loss: 6.421403, mae: 21.764694, mean_q: 29.431602\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 233695/1000000: episode: 208, duration: 7.203s, episode steps: 267, steps per second: 37, episode reward: -100.195, mean reward: -0.375 [-100.000, 18.630], mean action: 1.719 [0.000, 3.000], loss: 6.729204, mae: 21.582127, mean_q: 28.976719\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 234091/1000000: episode: 209, duration: 10.637s, episode steps: 396, steps per second: 37, episode reward: -196.992, mean reward: -0.497 [-100.000, 12.162], mean action: 1.518 [0.000, 3.000], loss: 6.568256, mae: 20.996227, mean_q: 28.211605\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 234199/1000000: episode: 210, duration: 2.911s, episode steps: 108, steps per second: 37, episode reward: -33.187, mean reward: -0.307 [-100.000, 11.520], mean action: 1.907 [0.000, 3.000], loss: 3.791483, mae: 21.005516, mean_q: 28.240255\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 234399/1000000: episode: 211, duration: 5.341s, episode steps: 200, steps per second: 37, episode reward: -207.283, mean reward: -1.036 [-100.000, 35.846], mean action: 1.845 [0.000, 3.000], loss: 6.273097, mae: 21.240015, mean_q: 28.804583\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 234711/1000000: episode: 212, duration: 8.398s, episode steps: 312, steps per second: 37, episode reward: -103.462, mean reward: -0.332 [-100.000, 12.165], mean action: 1.692 [0.000, 3.000], loss: 6.602874, mae: 21.194714, mean_q: 28.793570\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 235711/1000000: episode: 213, duration: 27.567s, episode steps: 1000, steps per second: 36, episode reward: 64.390, mean reward: 0.064 [-21.829, 24.068], mean action: 1.359 [0.000, 3.000], loss: 7.621132, mae: 21.134535, mean_q: 28.528564\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 236711/1000000: episode: 214, duration: 27.300s, episode steps: 1000, steps per second: 37, episode reward: 167.047, mean reward: 0.167 [-20.195, 22.368], mean action: 1.301 [0.000, 3.000], loss: 6.780197, mae: 21.009125, mean_q: 28.353868\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 237711/1000000: episode: 215, duration: 27.506s, episode steps: 1000, steps per second: 36, episode reward: 104.543, mean reward: 0.105 [-19.304, 22.979], mean action: 1.420 [0.000, 3.000], loss: 6.504616, mae: 20.778288, mean_q: 28.009218\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 237916/1000000: episode: 216, duration: 5.442s, episode steps: 205, steps per second: 38, episode reward: -37.672, mean reward: -0.184 [-100.000, 21.238], mean action: 1.693 [0.000, 3.000], loss: 6.642544, mae: 20.485632, mean_q: 27.726454\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 238055/1000000: episode: 217, duration: 3.718s, episode steps: 139, steps per second: 37, episode reward: -126.910, mean reward: -0.913 [-100.000, 8.526], mean action: 1.827 [0.000, 3.000], loss: 3.812271, mae: 20.702841, mean_q: 28.050785\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 238333/1000000: episode: 218, duration: 7.473s, episode steps: 278, steps per second: 37, episode reward: -65.393, mean reward: -0.235 [-100.000, 9.728], mean action: 1.665 [0.000, 3.000], loss: 6.065491, mae: 20.775331, mean_q: 28.130005\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 238456/1000000: episode: 219, duration: 3.323s, episode steps: 123, steps per second: 37, episode reward: 11.337, mean reward: 0.092 [-100.000, 16.731], mean action: 1.667 [0.000, 3.000], loss: 5.287640, mae: 21.340292, mean_q: 28.774658\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 238669/1000000: episode: 220, duration: 5.693s, episode steps: 213, steps per second: 37, episode reward: -401.596, mean reward: -1.885 [-100.000, 15.302], mean action: 1.878 [0.000, 3.000], loss: 9.041812, mae: 20.929764, mean_q: 28.180742\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 238890/1000000: episode: 221, duration: 5.913s, episode steps: 221, steps per second: 37, episode reward: -147.373, mean reward: -0.667 [-100.000, 9.882], mean action: 1.719 [0.000, 3.000], loss: 7.110305, mae: 20.323580, mean_q: 27.362808\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 239094/1000000: episode: 222, duration: 5.462s, episode steps: 204, steps per second: 37, episode reward: -262.802, mean reward: -1.288 [-100.000, 63.165], mean action: 1.789 [0.000, 3.000], loss: 6.062288, mae: 20.343002, mean_q: 27.539307\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 239280/1000000: episode: 223, duration: 4.972s, episode steps: 186, steps per second: 37, episode reward: 36.275, mean reward: 0.195 [-100.000, 20.616], mean action: 1.667 [0.000, 3.000], loss: 7.137483, mae: 20.320534, mean_q: 27.345356\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 239436/1000000: episode: 224, duration: 4.168s, episode steps: 156, steps per second: 37, episode reward: -32.686, mean reward: -0.210 [-100.000, 18.165], mean action: 1.763 [0.000, 3.000], loss: 10.858425, mae: 19.967319, mean_q: 26.668125\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 239638/1000000: episode: 225, duration: 5.396s, episode steps: 202, steps per second: 37, episode reward: -60.859, mean reward: -0.301 [-100.000, 13.486], mean action: 1.752 [0.000, 3.000], loss: 7.418752, mae: 20.312374, mean_q: 27.236580\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 239943/1000000: episode: 226, duration: 8.242s, episode steps: 305, steps per second: 37, episode reward: -94.381, mean reward: -0.309 [-100.000, 15.131], mean action: 1.718 [0.000, 3.000], loss: 10.031528, mae: 20.459969, mean_q: 27.364523\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 240086/1000000: episode: 227, duration: 3.825s, episode steps: 143, steps per second: 37, episode reward: -37.350, mean reward: -0.261 [-100.000, 16.782], mean action: 1.762 [0.000, 3.000], loss: 7.062403, mae: 20.458765, mean_q: 27.365480\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 240236/1000000: episode: 228, duration: 3.998s, episode steps: 150, steps per second: 38, episode reward: -67.621, mean reward: -0.451 [-100.000, 10.614], mean action: 1.687 [0.000, 3.000], loss: 11.145763, mae: 21.336884, mean_q: 28.505976\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 240418/1000000: episode: 229, duration: 4.884s, episode steps: 182, steps per second: 37, episode reward: 2.764, mean reward: 0.015 [-100.000, 17.517], mean action: 1.703 [0.000, 3.000], loss: 4.995772, mae: 21.102949, mean_q: 28.332664\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 241418/1000000: episode: 230, duration: 27.126s, episode steps: 1000, steps per second: 37, episode reward: 160.913, mean reward: 0.161 [-19.496, 23.074], mean action: 1.295 [0.000, 3.000], loss: 8.319177, mae: 21.675426, mean_q: 29.196579\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 241614/1000000: episode: 231, duration: 5.259s, episode steps: 196, steps per second: 37, episode reward: -237.617, mean reward: -1.212 [-100.000, 15.760], mean action: 1.811 [0.000, 3.000], loss: 12.595340, mae: 22.477484, mean_q: 30.207674\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 241726/1000000: episode: 232, duration: 3.006s, episode steps: 112, steps per second: 37, episode reward: -99.487, mean reward: -0.888 [-100.000, 10.465], mean action: 2.062 [0.000, 3.000], loss: 14.447305, mae: 22.568830, mean_q: 30.195736\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 241925/1000000: episode: 233, duration: 5.330s, episode steps: 199, steps per second: 37, episode reward: 15.270, mean reward: 0.077 [-100.000, 16.514], mean action: 1.734 [0.000, 3.000], loss: 11.003006, mae: 22.040663, mean_q: 29.601997\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 242116/1000000: episode: 234, duration: 5.128s, episode steps: 191, steps per second: 37, episode reward: 59.427, mean reward: 0.311 [-100.000, 19.059], mean action: 1.764 [0.000, 3.000], loss: 12.618348, mae: 21.989025, mean_q: 29.371042\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 242291/1000000: episode: 235, duration: 4.723s, episode steps: 175, steps per second: 37, episode reward: 5.456, mean reward: 0.031 [-100.000, 10.115], mean action: 1.669 [0.000, 3.000], loss: 6.131874, mae: 22.328199, mean_q: 29.784447\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 242475/1000000: episode: 236, duration: 4.927s, episode steps: 184, steps per second: 37, episode reward: -69.312, mean reward: -0.377 [-100.000, 15.236], mean action: 1.728 [0.000, 3.000], loss: 10.397341, mae: 21.893291, mean_q: 29.022705\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 243475/1000000: episode: 237, duration: 27.601s, episode steps: 1000, steps per second: 36, episode reward: 62.692, mean reward: 0.063 [-19.666, 22.956], mean action: 1.368 [0.000, 3.000], loss: 9.452850, mae: 21.100039, mean_q: 28.273067\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 243615/1000000: episode: 238, duration: 3.754s, episode steps: 140, steps per second: 37, episode reward: -166.716, mean reward: -1.191 [-100.000, 9.582], mean action: 1.893 [0.000, 3.000], loss: 10.395643, mae: 20.836119, mean_q: 27.735214\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 244615/1000000: episode: 239, duration: 27.482s, episode steps: 1000, steps per second: 36, episode reward: 171.570, mean reward: 0.172 [-20.285, 22.524], mean action: 1.332 [0.000, 3.000], loss: 7.144628, mae: 21.759081, mean_q: 29.164021\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 244805/1000000: episode: 240, duration: 5.088s, episode steps: 190, steps per second: 37, episode reward: -50.749, mean reward: -0.267 [-100.000, 64.889], mean action: 1.642 [0.000, 3.000], loss: 5.794363, mae: 22.029240, mean_q: 29.485491\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 245805/1000000: episode: 241, duration: 28.458s, episode steps: 1000, steps per second: 35, episode reward: 101.486, mean reward: 0.101 [-22.447, 22.947], mean action: 1.450 [0.000, 3.000], loss: 8.747072, mae: 21.092297, mean_q: 28.140207\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 245971/1000000: episode: 242, duration: 4.440s, episode steps: 166, steps per second: 37, episode reward: -3.087, mean reward: -0.019 [-100.000, 21.888], mean action: 1.777 [0.000, 3.000], loss: 12.007229, mae: 20.994223, mean_q: 28.040760\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 246218/1000000: episode: 243, duration: 6.600s, episode steps: 247, steps per second: 37, episode reward: -20.942, mean reward: -0.085 [-100.000, 12.552], mean action: 1.830 [0.000, 3.000], loss: 9.199609, mae: 21.423353, mean_q: 28.674583\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 246351/1000000: episode: 244, duration: 3.550s, episode steps: 133, steps per second: 37, episode reward: -42.631, mean reward: -0.321 [-100.000, 14.016], mean action: 1.782 [0.000, 3.000], loss: 10.813540, mae: 21.332968, mean_q: 28.490017\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 247351/1000000: episode: 245, duration: 27.663s, episode steps: 1000, steps per second: 36, episode reward: 87.563, mean reward: 0.088 [-19.657, 22.585], mean action: 1.460 [0.000, 3.000], loss: 10.425107, mae: 21.639851, mean_q: 28.953794\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 247517/1000000: episode: 246, duration: 4.466s, episode steps: 166, steps per second: 37, episode reward: -186.113, mean reward: -1.121 [-100.000, 25.959], mean action: 1.819 [0.000, 3.000], loss: 9.050812, mae: 21.546885, mean_q: 28.996702\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 247725/1000000: episode: 247, duration: 5.563s, episode steps: 208, steps per second: 37, episode reward: -64.825, mean reward: -0.312 [-100.000, 15.460], mean action: 1.899 [0.000, 3.000], loss: 7.376328, mae: 21.832438, mean_q: 29.156881\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 247926/1000000: episode: 248, duration: 5.347s, episode steps: 201, steps per second: 38, episode reward: -176.477, mean reward: -0.878 [-100.000, 26.610], mean action: 1.821 [0.000, 3.000], loss: 6.312654, mae: 22.074955, mean_q: 29.765524\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 248926/1000000: episode: 249, duration: 28.155s, episode steps: 1000, steps per second: 36, episode reward: 120.342, mean reward: 0.120 [-20.432, 23.181], mean action: 1.362 [0.000, 3.000], loss: 8.182899, mae: 22.225309, mean_q: 29.813810\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 249228/1000000: episode: 250, duration: 8.163s, episode steps: 302, steps per second: 37, episode reward: -182.350, mean reward: -0.604 [-100.000, 15.097], mean action: 1.937 [0.000, 3.000], loss: 9.040935, mae: 22.096151, mean_q: 29.693485\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 249394/1000000: episode: 251, duration: 4.455s, episode steps: 166, steps per second: 37, episode reward: -55.096, mean reward: -0.332 [-100.000, 8.266], mean action: 1.699 [0.000, 3.000], loss: 7.829959, mae: 22.342968, mean_q: 29.952261\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 250394/1000000: episode: 252, duration: 28.315s, episode steps: 1000, steps per second: 35, episode reward: 70.565, mean reward: 0.071 [-18.880, 24.193], mean action: 1.437 [0.000, 3.000], loss: 7.437229, mae: 22.641308, mean_q: 30.549749\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 250619/1000000: episode: 253, duration: 6.040s, episode steps: 225, steps per second: 37, episode reward: 34.270, mean reward: 0.152 [-100.000, 12.037], mean action: 1.662 [0.000, 3.000], loss: 10.077605, mae: 22.274261, mean_q: 29.965313\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 251619/1000000: episode: 254, duration: 28.599s, episode steps: 1000, steps per second: 35, episode reward: 94.792, mean reward: 0.095 [-21.767, 23.278], mean action: 1.382 [0.000, 3.000], loss: 11.562649, mae: 22.282518, mean_q: 29.893938\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 252619/1000000: episode: 255, duration: 27.899s, episode steps: 1000, steps per second: 36, episode reward: 101.159, mean reward: 0.101 [-22.372, 22.562], mean action: 1.419 [0.000, 3.000], loss: 8.078860, mae: 22.203640, mean_q: 29.758562\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 253619/1000000: episode: 256, duration: 27.827s, episode steps: 1000, steps per second: 36, episode reward: 130.922, mean reward: 0.131 [-18.103, 22.589], mean action: 1.421 [0.000, 3.000], loss: 10.581949, mae: 22.633364, mean_q: 30.308765\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 254619/1000000: episode: 257, duration: 28.182s, episode steps: 1000, steps per second: 35, episode reward: 94.749, mean reward: 0.095 [-22.858, 22.630], mean action: 1.368 [0.000, 3.000], loss: 9.233691, mae: 22.982758, mean_q: 30.864882\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 255619/1000000: episode: 258, duration: 27.469s, episode steps: 1000, steps per second: 36, episode reward: 94.387, mean reward: 0.094 [-18.639, 23.657], mean action: 1.449 [0.000, 3.000], loss: 10.057412, mae: 23.126528, mean_q: 31.027382\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 256619/1000000: episode: 259, duration: 27.447s, episode steps: 1000, steps per second: 36, episode reward: 111.656, mean reward: 0.112 [-20.749, 25.615], mean action: 1.392 [0.000, 3.000], loss: 8.830048, mae: 22.867250, mean_q: 30.626972\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 256923/1000000: episode: 260, duration: 8.235s, episode steps: 304, steps per second: 37, episode reward: -15.539, mean reward: -0.051 [-100.000, 11.368], mean action: 1.766 [0.000, 3.000], loss: 6.326300, mae: 22.780703, mean_q: 30.620731\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 257923/1000000: episode: 261, duration: 27.308s, episode steps: 1000, steps per second: 37, episode reward: 141.310, mean reward: 0.141 [-23.275, 23.378], mean action: 1.262 [0.000, 3.000], loss: 9.020891, mae: 22.546669, mean_q: 30.223177\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 258125/1000000: episode: 262, duration: 5.438s, episode steps: 202, steps per second: 37, episode reward: -8.997, mean reward: -0.045 [-100.000, 12.315], mean action: 1.738 [0.000, 3.000], loss: 9.021878, mae: 22.906723, mean_q: 30.568094\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 258359/1000000: episode: 263, duration: 6.339s, episode steps: 234, steps per second: 37, episode reward: 1.578, mean reward: 0.007 [-100.000, 17.261], mean action: 1.774 [0.000, 3.000], loss: 8.739611, mae: 23.254076, mean_q: 30.834944\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 258532/1000000: episode: 264, duration: 4.677s, episode steps: 173, steps per second: 37, episode reward: -13.305, mean reward: -0.077 [-100.000, 17.413], mean action: 1.769 [0.000, 3.000], loss: 11.369623, mae: 22.745979, mean_q: 30.217354\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 258873/1000000: episode: 265, duration: 9.207s, episode steps: 341, steps per second: 37, episode reward: -280.151, mean reward: -0.822 [-100.000, 25.496], mean action: 1.710 [0.000, 3.000], loss: 16.226692, mae: 22.279678, mean_q: 29.904583\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 259873/1000000: episode: 266, duration: 27.305s, episode steps: 1000, steps per second: 37, episode reward: 104.251, mean reward: 0.104 [-18.903, 23.152], mean action: 1.324 [0.000, 3.000], loss: 8.272491, mae: 23.251917, mean_q: 31.207006\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 260096/1000000: episode: 267, duration: 5.991s, episode steps: 223, steps per second: 37, episode reward: -64.241, mean reward: -0.288 [-100.000, 8.711], mean action: 1.749 [0.000, 3.000], loss: 8.268348, mae: 23.432722, mean_q: 31.485680\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 261096/1000000: episode: 268, duration: 28.515s, episode steps: 1000, steps per second: 35, episode reward: 88.248, mean reward: 0.088 [-19.260, 22.994], mean action: 1.508 [0.000, 3.000], loss: 8.705873, mae: 23.649738, mean_q: 31.687536\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 261240/1000000: episode: 269, duration: 3.872s, episode steps: 144, steps per second: 37, episode reward: 26.451, mean reward: 0.184 [-100.000, 13.124], mean action: 1.708 [0.000, 3.000], loss: 9.461390, mae: 22.956871, mean_q: 30.680210\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 262240/1000000: episode: 270, duration: 27.552s, episode steps: 1000, steps per second: 36, episode reward: 148.334, mean reward: 0.148 [-19.698, 26.608], mean action: 1.310 [0.000, 3.000], loss: 9.412967, mae: 23.205832, mean_q: 31.059538\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 262398/1000000: episode: 271, duration: 4.257s, episode steps: 158, steps per second: 37, episode reward: -32.243, mean reward: -0.204 [-100.000, 20.272], mean action: 1.930 [0.000, 3.000], loss: 10.247577, mae: 23.044338, mean_q: 30.981451\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 263398/1000000: episode: 272, duration: 27.953s, episode steps: 1000, steps per second: 36, episode reward: 86.740, mean reward: 0.087 [-24.187, 18.921], mean action: 1.357 [0.000, 3.000], loss: 10.167207, mae: 23.345169, mean_q: 31.295990\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 263675/1000000: episode: 273, duration: 7.503s, episode steps: 277, steps per second: 37, episode reward: -78.239, mean reward: -0.282 [-100.000, 20.804], mean action: 1.722 [0.000, 3.000], loss: 10.155257, mae: 23.462669, mean_q: 31.241411\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 263863/1000000: episode: 274, duration: 5.104s, episode steps: 188, steps per second: 37, episode reward: -145.205, mean reward: -0.772 [-100.000, 21.107], mean action: 1.894 [0.000, 3.000], loss: 9.192393, mae: 23.143385, mean_q: 30.819014\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 264101/1000000: episode: 275, duration: 6.440s, episode steps: 238, steps per second: 37, episode reward: -5.750, mean reward: -0.024 [-100.000, 24.250], mean action: 1.685 [0.000, 3.000], loss: 7.827092, mae: 22.898060, mean_q: 30.763601\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 265101/1000000: episode: 276, duration: 27.599s, episode steps: 1000, steps per second: 36, episode reward: 112.906, mean reward: 0.113 [-23.711, 22.749], mean action: 1.289 [0.000, 3.000], loss: 9.014273, mae: 22.687302, mean_q: 30.407660\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 265446/1000000: episode: 277, duration: 9.404s, episode steps: 345, steps per second: 37, episode reward: -58.547, mean reward: -0.170 [-100.000, 12.200], mean action: 1.771 [0.000, 3.000], loss: 10.557561, mae: 23.105207, mean_q: 30.829103\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 265664/1000000: episode: 278, duration: 5.906s, episode steps: 218, steps per second: 37, episode reward: -61.542, mean reward: -0.282 [-100.000, 19.067], mean action: 1.812 [0.000, 3.000], loss: 5.559452, mae: 22.476841, mean_q: 30.339687\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 265848/1000000: episode: 279, duration: 4.979s, episode steps: 184, steps per second: 37, episode reward: -25.049, mean reward: -0.136 [-100.000, 20.003], mean action: 1.750 [0.000, 3.000], loss: 16.278271, mae: 22.755169, mean_q: 30.272890\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 266848/1000000: episode: 280, duration: 27.865s, episode steps: 1000, steps per second: 36, episode reward: 138.280, mean reward: 0.138 [-20.444, 23.170], mean action: 1.281 [0.000, 3.000], loss: 8.744490, mae: 22.811314, mean_q: 30.558582\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 267370/1000000: episode: 281, duration: 14.292s, episode steps: 522, steps per second: 37, episode reward: -167.151, mean reward: -0.320 [-100.000, 21.186], mean action: 1.467 [0.000, 3.000], loss: 8.132718, mae: 23.550911, mean_q: 31.580120\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 268370/1000000: episode: 282, duration: 27.224s, episode steps: 1000, steps per second: 37, episode reward: 109.625, mean reward: 0.110 [-21.231, 22.729], mean action: 1.329 [0.000, 3.000], loss: 7.923760, mae: 23.628868, mean_q: 31.658497\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 269370/1000000: episode: 283, duration: 27.385s, episode steps: 1000, steps per second: 37, episode reward: 172.284, mean reward: 0.172 [-20.173, 22.793], mean action: 1.351 [0.000, 3.000], loss: 10.650832, mae: 23.635382, mean_q: 31.663364\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 270370/1000000: episode: 284, duration: 27.603s, episode steps: 1000, steps per second: 36, episode reward: 122.853, mean reward: 0.123 [-19.732, 23.214], mean action: 1.344 [0.000, 3.000], loss: 6.744815, mae: 23.111008, mean_q: 30.998621\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 271370/1000000: episode: 285, duration: 27.802s, episode steps: 1000, steps per second: 36, episode reward: 113.278, mean reward: 0.113 [-17.547, 22.766], mean action: 1.379 [0.000, 3.000], loss: 6.508897, mae: 24.386116, mean_q: 32.873493\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 271578/1000000: episode: 286, duration: 5.607s, episode steps: 208, steps per second: 37, episode reward: -31.313, mean reward: -0.151 [-100.000, 8.653], mean action: 1.904 [0.000, 3.000], loss: 4.965981, mae: 24.380802, mean_q: 32.884995\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 272578/1000000: episode: 287, duration: 27.861s, episode steps: 1000, steps per second: 36, episode reward: 118.252, mean reward: 0.118 [-18.939, 28.759], mean action: 1.434 [0.000, 3.000], loss: 7.453810, mae: 24.314762, mean_q: 32.719959\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 272732/1000000: episode: 288, duration: 4.133s, episode steps: 154, steps per second: 37, episode reward: -29.781, mean reward: -0.193 [-100.000, 21.717], mean action: 1.649 [0.000, 3.000], loss: 3.469857, mae: 24.495964, mean_q: 32.913261\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 272890/1000000: episode: 289, duration: 4.246s, episode steps: 158, steps per second: 37, episode reward: -81.401, mean reward: -0.515 [-100.000, 6.608], mean action: 1.690 [0.000, 3.000], loss: 7.060185, mae: 24.449093, mean_q: 32.787178\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 273147/1000000: episode: 290, duration: 6.999s, episode steps: 257, steps per second: 37, episode reward: -214.982, mean reward: -0.837 [-100.000, 50.567], mean action: 1.856 [0.000, 3.000], loss: 11.630745, mae: 23.856707, mean_q: 31.721451\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 274147/1000000: episode: 291, duration: 27.856s, episode steps: 1000, steps per second: 36, episode reward: 117.105, mean reward: 0.117 [-18.857, 15.230], mean action: 1.333 [0.000, 3.000], loss: 8.430674, mae: 23.613762, mean_q: 31.559502\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 274290/1000000: episode: 292, duration: 3.842s, episode steps: 143, steps per second: 37, episode reward: -18.490, mean reward: -0.129 [-100.000, 21.540], mean action: 1.860 [0.000, 3.000], loss: 13.433837, mae: 24.317375, mean_q: 32.578793\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 274500/1000000: episode: 293, duration: 5.648s, episode steps: 210, steps per second: 37, episode reward: -44.874, mean reward: -0.214 [-100.000, 61.791], mean action: 1.867 [0.000, 3.000], loss: 6.294821, mae: 23.815290, mean_q: 31.966681\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 274829/1000000: episode: 294, duration: 8.957s, episode steps: 329, steps per second: 37, episode reward: -61.651, mean reward: -0.187 [-100.000, 15.149], mean action: 1.708 [0.000, 3.000], loss: 10.351268, mae: 24.083239, mean_q: 32.231174\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 275829/1000000: episode: 295, duration: 27.784s, episode steps: 1000, steps per second: 36, episode reward: 145.368, mean reward: 0.145 [-19.285, 22.987], mean action: 1.356 [0.000, 3.000], loss: 8.268610, mae: 24.598106, mean_q: 33.066193\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 276829/1000000: episode: 296, duration: 27.582s, episode steps: 1000, steps per second: 36, episode reward: 84.206, mean reward: 0.084 [-19.777, 23.857], mean action: 1.309 [0.000, 3.000], loss: 8.460066, mae: 24.311100, mean_q: 32.741234\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 277829/1000000: episode: 297, duration: 28.091s, episode steps: 1000, steps per second: 36, episode reward: 91.991, mean reward: 0.092 [-20.807, 22.004], mean action: 1.311 [0.000, 3.000], loss: 9.309067, mae: 23.966743, mean_q: 32.242180\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 278829/1000000: episode: 298, duration: 28.378s, episode steps: 1000, steps per second: 35, episode reward: 71.977, mean reward: 0.072 [-20.365, 23.040], mean action: 1.426 [0.000, 3.000], loss: 6.780925, mae: 24.072023, mean_q: 32.505173\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 278961/1000000: episode: 299, duration: 3.558s, episode steps: 132, steps per second: 37, episode reward: -55.055, mean reward: -0.417 [-100.000, 14.704], mean action: 1.682 [0.000, 3.000], loss: 10.005909, mae: 23.842537, mean_q: 31.960123\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 279174/1000000: episode: 300, duration: 5.758s, episode steps: 213, steps per second: 37, episode reward: -126.081, mean reward: -0.592 [-100.000, 12.561], mean action: 1.854 [0.000, 3.000], loss: 5.655617, mae: 24.652115, mean_q: 33.327019\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 280174/1000000: episode: 301, duration: 27.959s, episode steps: 1000, steps per second: 36, episode reward: 128.435, mean reward: 0.128 [-20.323, 23.123], mean action: 1.333 [0.000, 3.000], loss: 7.933294, mae: 25.221941, mean_q: 33.863842\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 281174/1000000: episode: 302, duration: 27.786s, episode steps: 1000, steps per second: 36, episode reward: 98.518, mean reward: 0.099 [-23.749, 25.493], mean action: 1.341 [0.000, 3.000], loss: 9.612252, mae: 24.506598, mean_q: 32.821640\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 282174/1000000: episode: 303, duration: 27.654s, episode steps: 1000, steps per second: 36, episode reward: 95.550, mean reward: 0.096 [-20.272, 22.792], mean action: 1.253 [0.000, 3.000], loss: 7.372261, mae: 24.453587, mean_q: 32.945438\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 282568/1000000: episode: 304, duration: 10.660s, episode steps: 394, steps per second: 37, episode reward: -111.807, mean reward: -0.284 [-100.000, 18.945], mean action: 1.652 [0.000, 3.000], loss: 6.250865, mae: 24.637438, mean_q: 33.302429\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 282773/1000000: episode: 305, duration: 5.466s, episode steps: 205, steps per second: 38, episode reward: -26.633, mean reward: -0.130 [-100.000, 17.503], mean action: 1.712 [0.000, 3.000], loss: 8.393310, mae: 24.766350, mean_q: 33.467510\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 283773/1000000: episode: 306, duration: 27.720s, episode steps: 1000, steps per second: 36, episode reward: 86.528, mean reward: 0.087 [-19.554, 22.639], mean action: 1.437 [0.000, 3.000], loss: 7.118066, mae: 24.577820, mean_q: 33.085953\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 284773/1000000: episode: 307, duration: 27.692s, episode steps: 1000, steps per second: 36, episode reward: 22.331, mean reward: 0.022 [-22.032, 22.929], mean action: 1.496 [0.000, 3.000], loss: 8.061481, mae: 24.191416, mean_q: 32.727737\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 285773/1000000: episode: 308, duration: 27.441s, episode steps: 1000, steps per second: 36, episode reward: 99.476, mean reward: 0.099 [-19.408, 21.328], mean action: 1.376 [0.000, 3.000], loss: 6.390829, mae: 24.906221, mean_q: 33.777546\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 286773/1000000: episode: 309, duration: 27.897s, episode steps: 1000, steps per second: 36, episode reward: 151.928, mean reward: 0.152 [-19.533, 23.002], mean action: 1.345 [0.000, 3.000], loss: 8.805291, mae: 25.694157, mean_q: 34.875336\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 287036/1000000: episode: 310, duration: 7.174s, episode steps: 263, steps per second: 37, episode reward: -32.733, mean reward: -0.124 [-100.000, 35.283], mean action: 1.886 [0.000, 3.000], loss: 3.762819, mae: 25.813227, mean_q: 34.964626\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 287554/1000000: episode: 311, duration: 14.375s, episode steps: 518, steps per second: 36, episode reward: -302.584, mean reward: -0.584 [-100.000, 22.191], mean action: 1.585 [0.000, 3.000], loss: 7.168108, mae: 25.836493, mean_q: 34.889759\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 288554/1000000: episode: 312, duration: 27.726s, episode steps: 1000, steps per second: 36, episode reward: 121.955, mean reward: 0.122 [-18.730, 17.959], mean action: 1.403 [0.000, 3.000], loss: 8.042878, mae: 25.314535, mean_q: 34.138275\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 289554/1000000: episode: 313, duration: 27.827s, episode steps: 1000, steps per second: 36, episode reward: 135.477, mean reward: 0.135 [-19.147, 23.956], mean action: 1.393 [0.000, 3.000], loss: 6.882030, mae: 24.113668, mean_q: 32.484116\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 290554/1000000: episode: 314, duration: 27.804s, episode steps: 1000, steps per second: 36, episode reward: 105.612, mean reward: 0.106 [-24.443, 18.879], mean action: 1.420 [0.000, 3.000], loss: 6.913943, mae: 23.360775, mean_q: 31.601824\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 291554/1000000: episode: 315, duration: 27.618s, episode steps: 1000, steps per second: 36, episode reward: 95.789, mean reward: 0.096 [-19.990, 22.185], mean action: 1.362 [0.000, 3.000], loss: 5.538678, mae: 23.459539, mean_q: 31.805344\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 292554/1000000: episode: 316, duration: 27.895s, episode steps: 1000, steps per second: 36, episode reward: 107.840, mean reward: 0.108 [-19.185, 22.985], mean action: 1.344 [0.000, 3.000], loss: 3.886367, mae: 24.216360, mean_q: 32.820873\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 292783/1000000: episode: 317, duration: 6.186s, episode steps: 229, steps per second: 37, episode reward: -57.102, mean reward: -0.249 [-100.000, 11.219], mean action: 1.725 [0.000, 3.000], loss: 3.337170, mae: 23.706160, mean_q: 32.145866\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 293783/1000000: episode: 318, duration: 27.644s, episode steps: 1000, steps per second: 36, episode reward: 131.539, mean reward: 0.132 [-19.765, 23.234], mean action: 1.334 [0.000, 3.000], loss: 4.610382, mae: 24.392569, mean_q: 33.091972\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 294083/1000000: episode: 319, duration: 8.119s, episode steps: 300, steps per second: 37, episode reward: -36.279, mean reward: -0.121 [-100.000, 17.907], mean action: 1.607 [0.000, 3.000], loss: 3.988116, mae: 23.873602, mean_q: 32.590202\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 294322/1000000: episode: 320, duration: 6.466s, episode steps: 239, steps per second: 37, episode reward: -87.555, mean reward: -0.366 [-100.000, 10.120], mean action: 1.791 [0.000, 3.000], loss: 6.319528, mae: 23.413723, mean_q: 31.699598\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 295322/1000000: episode: 321, duration: 27.906s, episode steps: 1000, steps per second: 36, episode reward: 36.409, mean reward: 0.036 [-20.158, 16.405], mean action: 1.432 [0.000, 3.000], loss: 5.949245, mae: 23.467062, mean_q: 31.883488\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 295610/1000000: episode: 322, duration: 7.820s, episode steps: 288, steps per second: 37, episode reward: -33.425, mean reward: -0.116 [-100.000, 11.728], mean action: 1.760 [0.000, 3.000], loss: 6.170133, mae: 24.119493, mean_q: 32.792217\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 296610/1000000: episode: 323, duration: 27.589s, episode steps: 1000, steps per second: 36, episode reward: 100.480, mean reward: 0.100 [-21.492, 25.482], mean action: 1.457 [0.000, 3.000], loss: 6.225989, mae: 24.102745, mean_q: 32.807377\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 296819/1000000: episode: 324, duration: 5.635s, episode steps: 209, steps per second: 37, episode reward: -16.255, mean reward: -0.078 [-100.000, 18.658], mean action: 1.856 [0.000, 3.000], loss: 3.252079, mae: 22.925224, mean_q: 31.235641\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 297203/1000000: episode: 325, duration: 10.444s, episode steps: 384, steps per second: 37, episode reward: -107.599, mean reward: -0.280 [-100.000, 13.220], mean action: 1.609 [0.000, 3.000], loss: 4.937766, mae: 23.249405, mean_q: 31.518946\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 298203/1000000: episode: 326, duration: 27.831s, episode steps: 1000, steps per second: 36, episode reward: 151.943, mean reward: 0.152 [-19.074, 12.966], mean action: 1.321 [0.000, 3.000], loss: 4.986611, mae: 23.061031, mean_q: 31.270229\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 298442/1000000: episode: 327, duration: 6.444s, episode steps: 239, steps per second: 37, episode reward: -8.001, mean reward: -0.033 [-100.000, 11.364], mean action: 1.695 [0.000, 3.000], loss: 4.379023, mae: 23.615171, mean_q: 32.018143\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 298627/1000000: episode: 328, duration: 4.999s, episode steps: 185, steps per second: 37, episode reward: -79.509, mean reward: -0.430 [-100.000, 15.693], mean action: 1.724 [0.000, 3.000], loss: 3.651172, mae: 23.736591, mean_q: 32.257847\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 298822/1000000: episode: 329, duration: 5.256s, episode steps: 195, steps per second: 37, episode reward: 7.012, mean reward: 0.036 [-100.000, 25.815], mean action: 1.749 [0.000, 3.000], loss: 3.290765, mae: 23.858103, mean_q: 32.467594\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 299822/1000000: episode: 330, duration: 27.596s, episode steps: 1000, steps per second: 36, episode reward: 180.339, mean reward: 0.180 [-19.207, 21.868], mean action: 1.306 [0.000, 3.000], loss: 5.789366, mae: 24.171652, mean_q: 33.000797\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 300822/1000000: episode: 331, duration: 27.963s, episode steps: 1000, steps per second: 36, episode reward: 143.456, mean reward: 0.143 [-18.847, 22.286], mean action: 1.373 [0.000, 3.000], loss: 5.458614, mae: 24.036406, mean_q: 32.761925\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 301822/1000000: episode: 332, duration: 27.704s, episode steps: 1000, steps per second: 36, episode reward: 126.314, mean reward: 0.126 [-19.419, 22.666], mean action: 1.345 [0.000, 3.000], loss: 5.832412, mae: 22.955446, mean_q: 31.210012\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 302822/1000000: episode: 333, duration: 27.329s, episode steps: 1000, steps per second: 37, episode reward: 104.235, mean reward: 0.104 [-21.804, 23.155], mean action: 1.366 [0.000, 3.000], loss: 4.972624, mae: 22.539553, mean_q: 30.779573\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 302968/1000000: episode: 334, duration: 3.929s, episode steps: 146, steps per second: 37, episode reward: 25.469, mean reward: 0.174 [-100.000, 19.494], mean action: 1.781 [0.000, 3.000], loss: 5.719405, mae: 22.669937, mean_q: 30.853285\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 303209/1000000: episode: 335, duration: 6.509s, episode steps: 241, steps per second: 37, episode reward: -412.295, mean reward: -1.711 [-100.000, 50.534], mean action: 1.660 [0.000, 3.000], loss: 4.429070, mae: 22.927168, mean_q: 31.244827\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 304209/1000000: episode: 336, duration: 28.109s, episode steps: 1000, steps per second: 36, episode reward: 95.928, mean reward: 0.096 [-22.524, 23.063], mean action: 1.336 [0.000, 3.000], loss: 6.551837, mae: 23.093258, mean_q: 31.375347\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 304374/1000000: episode: 337, duration: 4.452s, episode steps: 165, steps per second: 37, episode reward: -26.590, mean reward: -0.161 [-100.000, 12.829], mean action: 1.691 [0.000, 3.000], loss: 5.523144, mae: 23.361530, mean_q: 31.675674\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 305374/1000000: episode: 338, duration: 28.013s, episode steps: 1000, steps per second: 36, episode reward: 120.011, mean reward: 0.120 [-23.106, 24.533], mean action: 1.475 [0.000, 3.000], loss: 6.220722, mae: 23.083506, mean_q: 31.349415\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 305853/1000000: episode: 339, duration: 13.191s, episode steps: 479, steps per second: 36, episode reward: -340.319, mean reward: -0.710 [-100.000, 18.033], mean action: 1.833 [0.000, 3.000], loss: 5.371032, mae: 23.201006, mean_q: 31.542034\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 306853/1000000: episode: 340, duration: 27.501s, episode steps: 1000, steps per second: 36, episode reward: 109.659, mean reward: 0.110 [-20.740, 21.480], mean action: 1.371 [0.000, 3.000], loss: 6.703636, mae: 23.159492, mean_q: 31.448872\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 307853/1000000: episode: 341, duration: 27.535s, episode steps: 1000, steps per second: 36, episode reward: 27.390, mean reward: 0.027 [-22.170, 24.070], mean action: 1.372 [0.000, 3.000], loss: 6.145954, mae: 23.107229, mean_q: 31.338808\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 308853/1000000: episode: 342, duration: 27.388s, episode steps: 1000, steps per second: 37, episode reward: 150.129, mean reward: 0.150 [-19.208, 22.720], mean action: 1.304 [0.000, 3.000], loss: 5.636116, mae: 22.515644, mean_q: 30.622532\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 308988/1000000: episode: 343, duration: 3.654s, episode steps: 135, steps per second: 37, episode reward: 4.397, mean reward: 0.033 [-100.000, 10.007], mean action: 1.741 [0.000, 3.000], loss: 6.486525, mae: 22.854256, mean_q: 31.146292\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 309292/1000000: episode: 344, duration: 8.286s, episode steps: 304, steps per second: 37, episode reward: -9.671, mean reward: -0.032 [-100.000, 11.026], mean action: 1.724 [0.000, 3.000], loss: 4.230146, mae: 22.878174, mean_q: 31.150143\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 309416/1000000: episode: 345, duration: 3.338s, episode steps: 124, steps per second: 37, episode reward: -16.353, mean reward: -0.132 [-100.000, 14.861], mean action: 1.702 [0.000, 3.000], loss: 4.735569, mae: 22.748844, mean_q: 30.906204\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 310416/1000000: episode: 346, duration: 28.437s, episode steps: 1000, steps per second: 35, episode reward: 86.445, mean reward: 0.086 [-20.787, 22.697], mean action: 1.327 [0.000, 3.000], loss: 5.374936, mae: 23.231121, mean_q: 31.572536\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 311416/1000000: episode: 347, duration: 27.661s, episode steps: 1000, steps per second: 36, episode reward: 92.672, mean reward: 0.093 [-19.520, 23.510], mean action: 1.408 [0.000, 3.000], loss: 6.016088, mae: 23.113277, mean_q: 31.454105\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 311701/1000000: episode: 348, duration: 7.769s, episode steps: 285, steps per second: 37, episode reward: -551.282, mean reward: -1.934 [-100.000, 4.912], mean action: 1.789 [0.000, 3.000], loss: 5.873997, mae: 23.501360, mean_q: 32.095348\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 311920/1000000: episode: 349, duration: 5.920s, episode steps: 219, steps per second: 37, episode reward: -36.495, mean reward: -0.167 [-100.000, 37.137], mean action: 1.685 [0.000, 3.000], loss: 6.127985, mae: 23.635120, mean_q: 32.376102\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 312018/1000000: episode: 350, duration: 2.627s, episode steps: 98, steps per second: 37, episode reward: -483.107, mean reward: -4.930 [-100.000, 1.100], mean action: 1.847 [0.000, 3.000], loss: 2.983164, mae: 23.736944, mean_q: 32.440651\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 312458/1000000: episode: 351, duration: 12.022s, episode steps: 440, steps per second: 37, episode reward: -270.299, mean reward: -0.614 [-100.000, 17.582], mean action: 1.691 [0.000, 3.000], loss: 6.452096, mae: 24.533123, mean_q: 33.334869\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 312713/1000000: episode: 352, duration: 6.914s, episode steps: 255, steps per second: 37, episode reward: 23.594, mean reward: 0.093 [-100.000, 17.442], mean action: 1.682 [0.000, 3.000], loss: 7.995257, mae: 23.927763, mean_q: 32.589554\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 312886/1000000: episode: 353, duration: 4.667s, episode steps: 173, steps per second: 37, episode reward: 9.234, mean reward: 0.053 [-100.000, 18.058], mean action: 1.676 [0.000, 3.000], loss: 4.956707, mae: 23.962461, mean_q: 32.792248\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 313074/1000000: episode: 354, duration: 5.099s, episode steps: 188, steps per second: 37, episode reward: -44.551, mean reward: -0.237 [-100.000, 13.510], mean action: 1.824 [0.000, 3.000], loss: 9.214956, mae: 24.025816, mean_q: 32.517651\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 313388/1000000: episode: 355, duration: 8.596s, episode steps: 314, steps per second: 37, episode reward: -63.260, mean reward: -0.201 [-100.000, 13.450], mean action: 1.697 [0.000, 3.000], loss: 4.075799, mae: 24.260748, mean_q: 33.014393\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 313570/1000000: episode: 356, duration: 4.932s, episode steps: 182, steps per second: 37, episode reward: -212.823, mean reward: -1.169 [-100.000, 17.961], mean action: 1.687 [0.000, 3.000], loss: 6.081139, mae: 24.613375, mean_q: 33.509342\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 314570/1000000: episode: 357, duration: 27.639s, episode steps: 1000, steps per second: 36, episode reward: 73.017, mean reward: 0.073 [-21.038, 59.226], mean action: 1.371 [0.000, 3.000], loss: 6.224031, mae: 24.040087, mean_q: 32.539162\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 314768/1000000: episode: 358, duration: 5.399s, episode steps: 198, steps per second: 37, episode reward: -214.960, mean reward: -1.086 [-100.000, 21.929], mean action: 1.798 [0.000, 3.000], loss: 5.072344, mae: 23.610029, mean_q: 31.809156\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 314954/1000000: episode: 359, duration: 5.043s, episode steps: 186, steps per second: 37, episode reward: -6.066, mean reward: -0.033 [-100.000, 19.192], mean action: 1.737 [0.000, 3.000], loss: 6.834014, mae: 23.502907, mean_q: 31.680729\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 315186/1000000: episode: 360, duration: 6.254s, episode steps: 232, steps per second: 37, episode reward: -46.126, mean reward: -0.199 [-100.000, 16.276], mean action: 1.647 [0.000, 3.000], loss: 7.242524, mae: 23.630611, mean_q: 32.084404\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 316186/1000000: episode: 361, duration: 27.517s, episode steps: 1000, steps per second: 36, episode reward: 47.012, mean reward: 0.047 [-17.304, 22.904], mean action: 1.361 [0.000, 3.000], loss: 5.505154, mae: 24.307049, mean_q: 33.013149\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 317186/1000000: episode: 362, duration: 27.726s, episode steps: 1000, steps per second: 36, episode reward: 152.695, mean reward: 0.153 [-19.482, 21.730], mean action: 1.294 [0.000, 3.000], loss: 8.086907, mae: 24.048204, mean_q: 32.378048\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 318186/1000000: episode: 363, duration: 27.549s, episode steps: 1000, steps per second: 36, episode reward: 91.969, mean reward: 0.092 [-18.413, 13.549], mean action: 1.268 [0.000, 3.000], loss: 6.159733, mae: 24.898819, mean_q: 33.674095\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 318332/1000000: episode: 364, duration: 3.971s, episode steps: 146, steps per second: 37, episode reward: -9.750, mean reward: -0.067 [-100.000, 14.702], mean action: 1.753 [0.000, 3.000], loss: 4.876711, mae: 25.475494, mean_q: 34.379929\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 318580/1000000: episode: 365, duration: 6.707s, episode steps: 248, steps per second: 37, episode reward: -51.974, mean reward: -0.210 [-100.000, 14.007], mean action: 1.802 [0.000, 3.000], loss: 8.484401, mae: 25.249701, mean_q: 34.228771\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 318681/1000000: episode: 366, duration: 2.741s, episode steps: 101, steps per second: 37, episode reward: -429.399, mean reward: -4.251 [-100.000, 1.337], mean action: 1.842 [0.000, 3.000], loss: 7.176426, mae: 25.071939, mean_q: 33.846302\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 319681/1000000: episode: 367, duration: 27.961s, episode steps: 1000, steps per second: 36, episode reward: 78.188, mean reward: 0.078 [-18.866, 22.417], mean action: 1.384 [0.000, 3.000], loss: 6.161516, mae: 25.099758, mean_q: 33.788574\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 319953/1000000: episode: 368, duration: 7.357s, episode steps: 272, steps per second: 37, episode reward: -84.827, mean reward: -0.312 [-100.000, 19.817], mean action: 1.676 [0.000, 3.000], loss: 5.167034, mae: 24.985653, mean_q: 33.677387\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 320169/1000000: episode: 369, duration: 5.840s, episode steps: 216, steps per second: 37, episode reward: -191.009, mean reward: -0.884 [-100.000, 49.049], mean action: 1.750 [0.000, 3.000], loss: 8.691814, mae: 24.798193, mean_q: 33.267990\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 320376/1000000: episode: 370, duration: 5.554s, episode steps: 207, steps per second: 37, episode reward: -42.145, mean reward: -0.204 [-100.000, 16.322], mean action: 1.792 [0.000, 3.000], loss: 7.795059, mae: 25.152626, mean_q: 33.933491\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 320625/1000000: episode: 371, duration: 6.735s, episode steps: 249, steps per second: 37, episode reward: -11.090, mean reward: -0.045 [-100.000, 18.217], mean action: 1.675 [0.000, 3.000], loss: 9.013021, mae: 24.901339, mean_q: 33.554070\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 321625/1000000: episode: 372, duration: 28.231s, episode steps: 1000, steps per second: 35, episode reward: 93.224, mean reward: 0.093 [-19.028, 22.427], mean action: 1.348 [0.000, 3.000], loss: 7.842590, mae: 24.977961, mean_q: 33.749836\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 321787/1000000: episode: 373, duration: 4.402s, episode steps: 162, steps per second: 37, episode reward: 65.129, mean reward: 0.402 [-100.000, 27.808], mean action: 1.679 [0.000, 3.000], loss: 2.833644, mae: 24.611971, mean_q: 33.211964\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 322043/1000000: episode: 374, duration: 6.978s, episode steps: 256, steps per second: 37, episode reward: -186.895, mean reward: -0.730 [-100.000, 12.161], mean action: 1.789 [0.000, 3.000], loss: 6.595126, mae: 24.943474, mean_q: 33.850105\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 323043/1000000: episode: 375, duration: 27.436s, episode steps: 1000, steps per second: 36, episode reward: 166.639, mean reward: 0.167 [-19.694, 22.506], mean action: 1.336 [0.000, 3.000], loss: 7.356524, mae: 25.391207, mean_q: 34.253651\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 324043/1000000: episode: 376, duration: 27.819s, episode steps: 1000, steps per second: 36, episode reward: 121.937, mean reward: 0.122 [-19.891, 22.773], mean action: 1.361 [0.000, 3.000], loss: 5.829235, mae: 25.221924, mean_q: 33.975815\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 325043/1000000: episode: 377, duration: 27.828s, episode steps: 1000, steps per second: 36, episode reward: 93.850, mean reward: 0.094 [-19.128, 23.151], mean action: 1.451 [0.000, 3.000], loss: 7.062569, mae: 24.771034, mean_q: 33.361126\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 325280/1000000: episode: 378, duration: 6.432s, episode steps: 237, steps per second: 37, episode reward: -276.045, mean reward: -1.165 [-100.000, 67.891], mean action: 1.810 [0.000, 3.000], loss: 10.332791, mae: 24.923050, mean_q: 33.686188\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 325442/1000000: episode: 379, duration: 4.390s, episode steps: 162, steps per second: 37, episode reward: -50.078, mean reward: -0.309 [-100.000, 14.773], mean action: 1.710 [0.000, 3.000], loss: 4.416982, mae: 25.131971, mean_q: 33.859570\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 325843/1000000: episode: 380, duration: 10.944s, episode steps: 401, steps per second: 37, episode reward: -253.393, mean reward: -0.632 [-100.000, 21.103], mean action: 1.603 [0.000, 3.000], loss: 5.017611, mae: 25.334970, mean_q: 33.989807\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 325980/1000000: episode: 381, duration: 3.681s, episode steps: 137, steps per second: 37, episode reward: -86.850, mean reward: -0.634 [-100.000, 13.386], mean action: 1.474 [0.000, 3.000], loss: 11.389641, mae: 25.398359, mean_q: 34.278961\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 326103/1000000: episode: 382, duration: 3.310s, episode steps: 123, steps per second: 37, episode reward: -77.472, mean reward: -0.630 [-100.000, 12.971], mean action: 1.699 [0.000, 3.000], loss: 4.551465, mae: 25.839325, mean_q: 35.037300\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 326337/1000000: episode: 383, duration: 6.365s, episode steps: 234, steps per second: 37, episode reward: -39.135, mean reward: -0.167 [-100.000, 16.528], mean action: 1.778 [0.000, 3.000], loss: 7.322541, mae: 25.555862, mean_q: 34.436218\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 326740/1000000: episode: 384, duration: 10.989s, episode steps: 403, steps per second: 37, episode reward: -49.730, mean reward: -0.123 [-100.000, 14.677], mean action: 1.730 [0.000, 3.000], loss: 5.060961, mae: 25.450333, mean_q: 34.227726\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 327035/1000000: episode: 385, duration: 8.009s, episode steps: 295, steps per second: 37, episode reward: -16.804, mean reward: -0.057 [-100.000, 17.915], mean action: 1.664 [0.000, 3.000], loss: 6.851749, mae: 25.556789, mean_q: 34.408615\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 328035/1000000: episode: 386, duration: 27.524s, episode steps: 1000, steps per second: 36, episode reward: 81.581, mean reward: 0.082 [-19.336, 13.187], mean action: 1.366 [0.000, 3.000], loss: 7.734803, mae: 25.621891, mean_q: 34.524960\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 329035/1000000: episode: 387, duration: 27.598s, episode steps: 1000, steps per second: 36, episode reward: 122.965, mean reward: 0.123 [-20.889, 23.233], mean action: 1.439 [0.000, 3.000], loss: 7.644927, mae: 25.477512, mean_q: 34.355999\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 329118/1000000: episode: 388, duration: 2.268s, episode steps: 83, steps per second: 37, episode reward: -431.452, mean reward: -5.198 [-100.000, 0.607], mean action: 1.940 [0.000, 3.000], loss: 3.478128, mae: 24.790293, mean_q: 33.478653\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 329251/1000000: episode: 389, duration: 3.595s, episode steps: 133, steps per second: 37, episode reward: -16.564, mean reward: -0.125 [-100.000, 19.173], mean action: 1.887 [0.000, 3.000], loss: 4.885646, mae: 25.320059, mean_q: 34.189095\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 329383/1000000: episode: 390, duration: 3.562s, episode steps: 132, steps per second: 37, episode reward: -35.342, mean reward: -0.268 [-100.000, 13.232], mean action: 1.712 [0.000, 3.000], loss: 5.641286, mae: 24.904537, mean_q: 33.670403\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 329637/1000000: episode: 391, duration: 6.864s, episode steps: 254, steps per second: 37, episode reward: 3.859, mean reward: 0.015 [-100.000, 10.160], mean action: 1.654 [0.000, 3.000], loss: 7.016157, mae: 24.930487, mean_q: 33.467613\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 329889/1000000: episode: 392, duration: 6.808s, episode steps: 252, steps per second: 37, episode reward: -64.621, mean reward: -0.256 [-100.000, 18.558], mean action: 1.718 [0.000, 3.000], loss: 8.452511, mae: 24.648872, mean_q: 33.098991\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 330008/1000000: episode: 393, duration: 3.228s, episode steps: 119, steps per second: 37, episode reward: -206.028, mean reward: -1.731 [-100.000, 81.583], mean action: 1.630 [0.000, 3.000], loss: 5.381043, mae: 24.292406, mean_q: 32.746601\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 330162/1000000: episode: 394, duration: 4.149s, episode steps: 154, steps per second: 37, episode reward: -87.455, mean reward: -0.568 [-100.000, 12.007], mean action: 1.669 [0.000, 3.000], loss: 7.461414, mae: 24.431204, mean_q: 32.830181\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 330410/1000000: episode: 395, duration: 6.704s, episode steps: 248, steps per second: 37, episode reward: -30.874, mean reward: -0.124 [-100.000, 21.455], mean action: 1.702 [0.000, 3.000], loss: 7.354258, mae: 24.930679, mean_q: 33.613705\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 330659/1000000: episode: 396, duration: 6.704s, episode steps: 249, steps per second: 37, episode reward: -15.705, mean reward: -0.063 [-100.000, 21.100], mean action: 1.639 [0.000, 3.000], loss: 6.557687, mae: 24.890047, mean_q: 33.535973\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 330811/1000000: episode: 397, duration: 4.128s, episode steps: 152, steps per second: 37, episode reward: -119.643, mean reward: -0.787 [-100.000, 10.587], mean action: 1.599 [0.000, 3.000], loss: 10.785917, mae: 24.634085, mean_q: 33.029694\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 330925/1000000: episode: 398, duration: 3.113s, episode steps: 114, steps per second: 37, episode reward: -10.429, mean reward: -0.091 [-100.000, 10.953], mean action: 1.816 [0.000, 3.000], loss: 6.959754, mae: 24.728006, mean_q: 33.602577\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 331925/1000000: episode: 399, duration: 27.661s, episode steps: 1000, steps per second: 36, episode reward: 163.454, mean reward: 0.163 [-17.521, 22.735], mean action: 1.320 [0.000, 3.000], loss: 7.812006, mae: 25.120224, mean_q: 33.933765\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 332148/1000000: episode: 400, duration: 6.046s, episode steps: 223, steps per second: 37, episode reward: -108.709, mean reward: -0.487 [-100.000, 17.326], mean action: 1.758 [0.000, 3.000], loss: 7.559155, mae: 25.318069, mean_q: 34.333565\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 333148/1000000: episode: 401, duration: 27.582s, episode steps: 1000, steps per second: 36, episode reward: 94.299, mean reward: 0.094 [-18.880, 22.589], mean action: 1.356 [0.000, 3.000], loss: 8.264004, mae: 25.111860, mean_q: 33.705139\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 334148/1000000: episode: 402, duration: 28.174s, episode steps: 1000, steps per second: 35, episode reward: 86.600, mean reward: 0.087 [-22.516, 21.332], mean action: 1.397 [0.000, 3.000], loss: 7.015998, mae: 26.523144, mean_q: 35.562275\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 335148/1000000: episode: 403, duration: 27.911s, episode steps: 1000, steps per second: 36, episode reward: 123.625, mean reward: 0.124 [-19.832, 21.036], mean action: 1.331 [0.000, 3.000], loss: 6.807711, mae: 26.416836, mean_q: 35.582825\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 335393/1000000: episode: 404, duration: 6.628s, episode steps: 245, steps per second: 37, episode reward: -57.769, mean reward: -0.236 [-100.000, 12.552], mean action: 1.743 [0.000, 3.000], loss: 16.283850, mae: 26.415812, mean_q: 35.441422\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 335594/1000000: episode: 405, duration: 5.422s, episode steps: 201, steps per second: 37, episode reward: -54.839, mean reward: -0.273 [-100.000, 30.185], mean action: 1.801 [0.000, 3.000], loss: 9.570009, mae: 25.775061, mean_q: 34.118401\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 336594/1000000: episode: 406, duration: 28.211s, episode steps: 1000, steps per second: 35, episode reward: 78.980, mean reward: 0.079 [-20.132, 21.981], mean action: 1.330 [0.000, 3.000], loss: 7.393035, mae: 25.031628, mean_q: 33.679047\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 336819/1000000: episode: 407, duration: 6.092s, episode steps: 225, steps per second: 37, episode reward: -67.595, mean reward: -0.300 [-100.000, 18.005], mean action: 1.809 [0.000, 3.000], loss: 14.362922, mae: 24.911879, mean_q: 33.415329\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 337819/1000000: episode: 408, duration: 27.562s, episode steps: 1000, steps per second: 36, episode reward: 163.897, mean reward: 0.164 [-18.494, 22.441], mean action: 1.336 [0.000, 3.000], loss: 8.006580, mae: 24.817652, mean_q: 33.339813\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 338030/1000000: episode: 409, duration: 5.744s, episode steps: 211, steps per second: 37, episode reward: -1.083, mean reward: -0.005 [-100.000, 12.218], mean action: 1.777 [0.000, 3.000], loss: 6.659991, mae: 24.838617, mean_q: 33.279152\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 339030/1000000: episode: 410, duration: 27.551s, episode steps: 1000, steps per second: 36, episode reward: 92.774, mean reward: 0.093 [-19.917, 13.114], mean action: 1.370 [0.000, 3.000], loss: 8.162521, mae: 25.067219, mean_q: 33.638794\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 339203/1000000: episode: 411, duration: 4.666s, episode steps: 173, steps per second: 37, episode reward: 15.757, mean reward: 0.091 [-100.000, 19.333], mean action: 1.728 [0.000, 3.000], loss: 10.582638, mae: 25.185518, mean_q: 33.724648\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 340203/1000000: episode: 412, duration: 27.785s, episode steps: 1000, steps per second: 36, episode reward: 63.791, mean reward: 0.064 [-22.583, 21.380], mean action: 1.426 [0.000, 3.000], loss: 7.816158, mae: 25.134739, mean_q: 33.824123\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 340362/1000000: episode: 413, duration: 4.289s, episode steps: 159, steps per second: 37, episode reward: -39.939, mean reward: -0.251 [-100.000, 93.689], mean action: 1.686 [0.000, 3.000], loss: 3.712051, mae: 24.838160, mean_q: 33.086956\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 340632/1000000: episode: 414, duration: 7.289s, episode steps: 270, steps per second: 37, episode reward: -2.484, mean reward: -0.009 [-100.000, 14.770], mean action: 1.781 [0.000, 3.000], loss: 8.300551, mae: 24.803589, mean_q: 33.176277\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 340823/1000000: episode: 415, duration: 5.160s, episode steps: 191, steps per second: 37, episode reward: -9.022, mean reward: -0.047 [-100.000, 9.656], mean action: 1.723 [0.000, 3.000], loss: 7.655841, mae: 24.816345, mean_q: 33.276966\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 341823/1000000: episode: 416, duration: 28.234s, episode steps: 1000, steps per second: 35, episode reward: 88.732, mean reward: 0.089 [-9.763, 15.470], mean action: 1.334 [0.000, 3.000], loss: 10.181461, mae: 25.699532, mean_q: 34.721836\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 342823/1000000: episode: 417, duration: 28.183s, episode steps: 1000, steps per second: 35, episode reward: 116.051, mean reward: 0.116 [-18.390, 19.536], mean action: 1.392 [0.000, 3.000], loss: 10.465607, mae: 25.818886, mean_q: 34.696613\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 343108/1000000: episode: 418, duration: 7.713s, episode steps: 285, steps per second: 37, episode reward: -23.927, mean reward: -0.084 [-100.000, 10.303], mean action: 1.719 [0.000, 3.000], loss: 6.839146, mae: 26.021795, mean_q: 35.007915\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 343308/1000000: episode: 419, duration: 5.410s, episode steps: 200, steps per second: 37, episode reward: -32.160, mean reward: -0.161 [-100.000, 20.174], mean action: 1.815 [0.000, 3.000], loss: 9.297268, mae: 26.308247, mean_q: 35.338165\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 343632/1000000: episode: 420, duration: 8.810s, episode steps: 324, steps per second: 37, episode reward: -20.745, mean reward: -0.064 [-100.000, 16.681], mean action: 1.688 [0.000, 3.000], loss: 11.578714, mae: 26.395210, mean_q: 35.404438\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 343933/1000000: episode: 421, duration: 8.157s, episode steps: 301, steps per second: 37, episode reward: -4.749, mean reward: -0.016 [-100.000, 12.475], mean action: 1.751 [0.000, 3.000], loss: 14.624551, mae: 25.590424, mean_q: 34.121910\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 344933/1000000: episode: 422, duration: 28.518s, episode steps: 1000, steps per second: 35, episode reward: 119.965, mean reward: 0.120 [-24.031, 22.772], mean action: 1.336 [0.000, 3.000], loss: 8.063793, mae: 25.149551, mean_q: 33.749306\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 345331/1000000: episode: 423, duration: 10.885s, episode steps: 398, steps per second: 37, episode reward: -77.297, mean reward: -0.194 [-100.000, 11.217], mean action: 1.583 [0.000, 3.000], loss: 11.152982, mae: 26.603596, mean_q: 35.768379\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 345588/1000000: episode: 424, duration: 6.949s, episode steps: 257, steps per second: 37, episode reward: -10.858, mean reward: -0.042 [-100.000, 15.389], mean action: 1.802 [0.000, 3.000], loss: 7.976620, mae: 26.863174, mean_q: 36.123714\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 345762/1000000: episode: 425, duration: 4.752s, episode steps: 174, steps per second: 37, episode reward: -326.137, mean reward: -1.874 [-100.000, 3.919], mean action: 1.799 [0.000, 3.000], loss: 9.056929, mae: 26.895926, mean_q: 36.091949\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 346007/1000000: episode: 426, duration: 6.619s, episode steps: 245, steps per second: 37, episode reward: 24.533, mean reward: 0.100 [-100.000, 19.883], mean action: 1.788 [0.000, 3.000], loss: 8.167770, mae: 26.224621, mean_q: 35.380611\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 346287/1000000: episode: 427, duration: 7.617s, episode steps: 280, steps per second: 37, episode reward: 25.043, mean reward: 0.089 [-100.000, 14.583], mean action: 1.750 [0.000, 3.000], loss: 8.881083, mae: 26.381073, mean_q: 35.411842\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 347287/1000000: episode: 428, duration: 28.124s, episode steps: 1000, steps per second: 36, episode reward: 92.195, mean reward: 0.092 [-20.191, 22.278], mean action: 1.312 [0.000, 3.000], loss: 8.759166, mae: 25.762543, mean_q: 34.708355\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 348287/1000000: episode: 429, duration: 30.649s, episode steps: 1000, steps per second: 33, episode reward: 99.903, mean reward: 0.100 [-19.537, 23.922], mean action: 1.345 [0.000, 3.000], loss: 8.934934, mae: 26.004126, mean_q: 35.096207\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 349287/1000000: episode: 430, duration: 30.083s, episode steps: 1000, steps per second: 33, episode reward: 120.153, mean reward: 0.120 [-17.406, 16.266], mean action: 1.318 [0.000, 3.000], loss: 9.508534, mae: 26.065819, mean_q: 35.114220\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 349516/1000000: episode: 431, duration: 6.272s, episode steps: 229, steps per second: 37, episode reward: -24.811, mean reward: -0.108 [-100.000, 18.837], mean action: 1.790 [0.000, 3.000], loss: 10.905792, mae: 25.775574, mean_q: 34.550606\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 349694/1000000: episode: 432, duration: 4.929s, episode steps: 178, steps per second: 36, episode reward: -6.276, mean reward: -0.035 [-100.000, 16.198], mean action: 1.736 [0.000, 3.000], loss: 5.811660, mae: 25.853226, mean_q: 34.869694\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 349923/1000000: episode: 433, duration: 6.384s, episode steps: 229, steps per second: 36, episode reward: -42.040, mean reward: -0.184 [-100.000, 13.902], mean action: 1.729 [0.000, 3.000], loss: 8.738820, mae: 26.267168, mean_q: 35.412838\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      " 350119/1000000: episode: 434, duration: 5.426s, episode steps: 196, steps per second: 36, episode reward: -477.832, mean reward: -2.438 [-100.000, 4.162], mean action: 1.745 [0.000, 3.000], loss: 8.894211, mae: 25.385418, mean_q: 34.101822\n",
      "The reward is lower than the best one, checkpoint weights not updated\n",
      "done, took 6097.891 seconds\n"
     ]
    }
   ],
   "source": [
    "history = dqn.fit(env,nb_steps=1000000,visualize=False,verbose = 0, callbacks=callbacks, nb_max_episode_steps=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 4 episodes ...\n",
      "Episode 1: reward: -243.006, steps: 383\n",
      "Episode 2: reward: -80.729, steps: 1000\n",
      "Episode 3: reward: -69.349, steps: 1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [18]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m _ \u001B[38;5;241m=\u001B[39m \u001B[43mdqn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnb_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvisualize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\rl\\core.py:352\u001B[0m, in \u001B[0;36mAgent.test\u001B[1;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001B[0m\n\u001B[0;32m    350\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    351\u001B[0m     observation, r, d, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessor\u001B[38;5;241m.\u001B[39mprocess_step(observation, r, d, info)\n\u001B[1;32m--> 352\u001B[0m \u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_action_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    353\u001B[0m reward \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m r\n\u001B[0;32m    354\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m info\u001B[38;5;241m.\u001B[39mitems():\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\rl\\callbacks.py:98\u001B[0m, in \u001B[0;36mCallbackList.on_action_end\u001B[1;34m(self, action, logs)\u001B[0m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks:\n\u001B[0;32m     97\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callable(\u001B[38;5;28mgetattr\u001B[39m(callback, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mon_action_end\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)):\n\u001B[1;32m---> 98\u001B[0m         \u001B[43mcallback\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_action_end\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\rl\\callbacks.py:360\u001B[0m, in \u001B[0;36mVisualizer.on_action_end\u001B[1;34m(self, action, logs)\u001B[0m\n\u001B[0;32m    358\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_action_end\u001B[39m(\u001B[38;5;28mself\u001B[39m, action, logs):\n\u001B[0;32m    359\u001B[0m     \u001B[38;5;124;03m\"\"\" Render environment at the end of each action \"\"\"\u001B[39;00m\n\u001B[1;32m--> 360\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhuman\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\gym\\core.py:295\u001B[0m, in \u001B[0;36mWrapper.render\u001B[1;34m(self, mode, **kwargs)\u001B[0m\n\u001B[0;32m    294\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrender\u001B[39m(\u001B[38;5;28mself\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 295\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\gym\\envs\\box2d\\lunar_lander.py:442\u001B[0m, in \u001B[0;36mLunarLander.render\u001B[1;34m(self, mode)\u001B[0m\n\u001B[0;32m    432\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mviewer\u001B[38;5;241m.\u001B[39mdraw_polyline([(x, flagy1), (x, flagy2)], color\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m    433\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mviewer\u001B[38;5;241m.\u001B[39mdraw_polygon(\n\u001B[0;32m    434\u001B[0m         [\n\u001B[0;32m    435\u001B[0m             (x, flagy2),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    439\u001B[0m         color\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m0.8\u001B[39m, \u001B[38;5;241m0.8\u001B[39m, \u001B[38;5;241m0\u001B[39m),\n\u001B[0;32m    440\u001B[0m     )\n\u001B[1;32m--> 442\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mviewer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreturn_rgb_array\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrgb_array\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py:145\u001B[0m, in \u001B[0;36mViewer.render\u001B[1;34m(self, return_rgb_array)\u001B[0m\n\u001B[0;32m    143\u001B[0m     arr \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mreshape(buffer\u001B[38;5;241m.\u001B[39mheight, buffer\u001B[38;5;241m.\u001B[39mwidth, \u001B[38;5;241m4\u001B[39m)\n\u001B[0;32m    144\u001B[0m     arr \u001B[38;5;241m=\u001B[39m arr[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :, \u001B[38;5;241m0\u001B[39m:\u001B[38;5;241m3\u001B[39m]\n\u001B[1;32m--> 145\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflip\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    146\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39monetime_geoms \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    147\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m arr \u001B[38;5;28;01mif\u001B[39;00m return_rgb_array \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39misopen\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py:356\u001B[0m, in \u001B[0;36mWin32Window.flip\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_always_dwm \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dwm_composition_enabled():\n\u001B[0;32m    355\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_interval:\n\u001B[1;32m--> 356\u001B[0m             \u001B[43m_dwmapi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDwmFlush\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    358\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext\u001B[38;5;241m.\u001B[39mflip()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "_ = dqn.test(env, nb_episodes=4, visualize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import  matplotlib.pyplot as plt\n",
    "plt.plot(history.history[\"episode_reward\"])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "model1 = build_model(8,4)\n",
    "dqn1 = build_agent(model1, 4)\n",
    "dqn1.compile(Adam(learning_rate=1e-3), metrics=[\"mae\"])\n",
    "dqn1.load_weights(\"model/checkpoint_reward_216.07929728701765.h5f\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 4 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leff0\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: -196.370, steps: 1000\n",
      "Episode 2: reward: -174.996, steps: 1000\n",
      "Episode 3: reward: -170.461, steps: 1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [16]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m _ \u001B[38;5;241m=\u001B[39m \u001B[43mdqn1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnb_episodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvisualize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\rl\\core.py:341\u001B[0m, in \u001B[0;36mAgent.test\u001B[1;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001B[0m\n\u001B[0;32m    338\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[0;32m    339\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_step_begin(episode_step)\n\u001B[1;32m--> 341\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    342\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    343\u001B[0m         action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessor\u001B[38;5;241m.\u001B[39mprocess_action(action)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\rl\\agents\\dqn.py:224\u001B[0m, in \u001B[0;36mDQNAgent.forward\u001B[1;34m(self, observation)\u001B[0m\n\u001B[0;32m    221\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, observation):\n\u001B[0;32m    222\u001B[0m     \u001B[38;5;66;03m# Select an action.\u001B[39;00m\n\u001B[0;32m    223\u001B[0m     state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmemory\u001B[38;5;241m.\u001B[39mget_recent_state(observation)\n\u001B[1;32m--> 224\u001B[0m     q_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_q_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    225\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[0;32m    226\u001B[0m         action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39mselect_action(q_values\u001B[38;5;241m=\u001B[39mq_values)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\rl\\agents\\dqn.py:68\u001B[0m, in \u001B[0;36mAbstractDQNAgent.compute_q_values\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_q_values\u001B[39m(\u001B[38;5;28mself\u001B[39m, state):\n\u001B[1;32m---> 68\u001B[0m     q_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_batch_q_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m q_values\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m==\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnb_actions,)\n\u001B[0;32m     70\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m q_values\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\rl\\agents\\dqn.py:63\u001B[0m, in \u001B[0;36mAbstractDQNAgent.compute_batch_q_values\u001B[1;34m(self, state_batch)\u001B[0m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_batch_q_values\u001B[39m(\u001B[38;5;28mself\u001B[39m, state_batch):\n\u001B[0;32m     62\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_state_batch(state_batch)\n\u001B[1;32m---> 63\u001B[0m     q_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_on_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     64\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m q_values\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m==\u001B[39m (\u001B[38;5;28mlen\u001B[39m(state_batch), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnb_actions)\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m q_values\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\engine\\training_v1.py:1200\u001B[0m, in \u001B[0;36mModel.predict_on_batch\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m   1197\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(inputs)  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[0;32m   1199\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_predict_function()\n\u001B[1;32m-> 1200\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1202\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(outputs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   1203\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m outputs[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\keras\\backend.py:4284\u001B[0m, in \u001B[0;36mGraphExecutionFunction.__call__\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m   4278\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callable_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m feed_arrays \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_feed_arrays \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m   4279\u001B[0m     symbol_vals \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_symbol_vals \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m   4280\u001B[0m     feed_symbols \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_feed_symbols \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfetches \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetches \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m   4281\u001B[0m     session \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session):\n\u001B[0;32m   4282\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001B[1;32m-> 4284\u001B[0m fetched \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_callable_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43marray_vals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4285\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mrun_metadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_metadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4286\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_fetch_callbacks(fetched[\u001B[38;5;241m-\u001B[39m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetches):])\n\u001B[0;32m   4287\u001B[0m output_structure \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mnest\u001B[38;5;241m.\u001B[39mpack_sequence_as(\n\u001B[0;32m   4288\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_outputs_structure,\n\u001B[0;32m   4289\u001B[0m     fetched[:\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutputs)],\n\u001B[0;32m   4290\u001B[0m     expand_composites\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1480\u001B[0m, in \u001B[0;36mBaseSession._Callable.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1478\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1479\u001B[0m   run_metadata_ptr \u001B[38;5;241m=\u001B[39m tf_session\u001B[38;5;241m.\u001B[39mTF_NewBuffer() \u001B[38;5;28;01mif\u001B[39;00m run_metadata \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1480\u001B[0m   ret \u001B[38;5;241m=\u001B[39m \u001B[43mtf_session\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTF_SessionRunCallable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_session\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_session\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1481\u001B[0m \u001B[43m                                         \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1482\u001B[0m \u001B[43m                                         \u001B[49m\u001B[43mrun_metadata_ptr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1483\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m run_metadata:\n\u001B[0;32m   1484\u001B[0m     proto_data \u001B[38;5;241m=\u001B[39m tf_session\u001B[38;5;241m.\u001B[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "_ = dqn1.test(env, nb_episodes=4, visualize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}